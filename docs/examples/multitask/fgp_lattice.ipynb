{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fast Multitask Lattice GP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastgp\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## True Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape = (128, 1)\n",
      "y.shape = (3, 128)\n",
      "z.shape = (256, 1)\n"
     ]
    }
   ],
   "source": [
    "def f_ackley(x, a=20, b=0.2, c=2*np.pi, scaling=32.768):\n",
    "    # https://www.sfu.ca/~ssurjano/ackley.html\n",
    "    assert x.ndim==2\n",
    "    x = 2*scaling*x-scaling\n",
    "    t1 = a*torch.exp(-b*torch.sqrt(torch.mean(x**2,1)))\n",
    "    t2 = torch.exp(torch.mean(torch.cos(c*x),1))\n",
    "    t3 = a+np.exp(1)\n",
    "    y = -t1-t2+t3\n",
    "    return y\n",
    "f_low_fidelity = lambda x: f_ackley(x,c=0)\n",
    "f_high_fidelity = lambda x: f_ackley(x)\n",
    "f_cos = lambda x: torch.cos(2*np.pi*x).sum(1)\n",
    "fs = [f_low_fidelity,f_high_fidelity,f_cos]\n",
    "d = 1 # dimension\n",
    "rng = torch.Generator().manual_seed(17)\n",
    "x = torch.rand((2**7,d),generator=rng) # random testing locations\n",
    "y = torch.vstack([f(x) for f in fs]) # true values at random testing locations\n",
    "z = torch.rand((2**8,d),generator=rng) # other random locations at which to evaluate covariance\n",
    "print(\"x.shape = %s\"%str(tuple(x.shape)))\n",
    "print(\"y.shape = %s\"%str(tuple(y.shape)))\n",
    "print(\"z.shape = %s\"%str(tuple(z.shape)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct Fast GP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i = 0\n",
      "\tx_next[0].shape = (64, 1)\n",
      "\ty_next[0].shape = (64,)\n",
      "i = 1\n",
      "\tx_next[1].shape = (8, 1)\n",
      "\ty_next[1].shape = (8,)\n",
      "i = 2\n",
      "\tx_next[2].shape = (256, 1)\n",
      "\ty_next[2].shape = (256,)\n"
     ]
    }
   ],
   "source": [
    "fgp = fastgp.FastGPLattice(d,seed_for_seq=7,num_tasks=len(fs))\n",
    "x_next = fgp.get_x_next(n=[2**6,2**3,2**8])\n",
    "y_next = [fs[i](x_next[i]) for i in range(fgp.num_tasks)]\n",
    "fgp.add_y_next(y_next)\n",
    "assert len(x_next)==len(y_next)\n",
    "for i in range(len(x_next)):\n",
    "    print(\"i = %d\"%i)\n",
    "    print(\"\\tx_next[%d].shape = %s\"%(i,str(tuple(x_next[i].shape))))\n",
    "    print(\"\\ty_next[%d].shape = %s\"%(i,str(tuple(y_next[i].shape))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pmean.shape = (3, 128)\n",
      "l2 relative error = tensor([0.0046, 0.0692, 0.0003])\n"
     ]
    }
   ],
   "source": [
    "pmean = fgp.post_mean(x)\n",
    "print(\"pmean.shape = %s\"%str(tuple(pmean.shape)))\n",
    "print(\"l2 relative error =\",(torch.linalg.norm(y-pmean,dim=1)/torch.linalg.norm(y,dim=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     iter of 5.0e+03 | NMLL       | noise      | scale      | lengthscales         | task_kernel \n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "            0.00e+00 | 9.65e+03   | 1.00e-08   | 1.00e+00   | [1.0e+00]            | [[2.0e+00 1.0e+00 1.0e+00] [1.0e+00 2.0e+00 1.0e+00] [1.0e+00 1.0e+00 2.0e+00]]\n",
      "            5.00e+00 | -7.24e+02  | 1.00e-08   | 2.10e+00   | [2.1e+00]            | [[5.1e+00 3.0e+00 4.5e-01] [3.0e+00 5.1e+00 4.5e-01] [4.5e-01 4.5e-01 1.8e+00]]\n",
      "            1.00e+01 | -1.34e+03  | 1.00e-08   | 2.62e+00   | [2.6e+00]            | [[2.6e+01 1.3e+01 1.3e-01] [1.3e+01 1.6e+01 1.3e-01] [1.3e-01 1.3e-01 9.0e-01]]\n",
      "            1.50e+01 | -2.52e+03  | 1.00e-08   | 5.29e-01   | [5.3e-01]            | [[4.4e+02 6.7e+01 3.8e-01] [6.7e+01 6.9e+01 3.8e-01] [3.8e-01 3.8e-01 1.8e-01]]\n",
      "            2.00e+01 | -3.11e+03  | 1.00e-08   | 2.71e-01   | [2.7e-01]            | [[6.8e+03 3.2e+02 4.7e-02] [3.2e+02 4.0e+02 5.6e-02] [4.7e-02 5.6e-02 3.8e-02]]\n",
      "            2.50e+01 | -3.14e+03  | 1.00e-08   | 2.73e-01   | [2.7e-01]            | [[4.7e+03 4.5e+02 1.6e-02] [4.5e+02 7.3e+02 2.3e-02] [1.6e-02 2.3e-02 2.8e-02]]\n",
      "            3.00e+01 | -3.14e+03  | 1.00e-08   | 2.73e-01   | [2.7e-01]            | [[3.7e+03 5.0e+02 9.2e-02] [5.0e+02 8.2e+02 1.5e-01] [9.2e-02 1.5e-01 2.7e-02]]\n",
      "            3.50e+01 | -3.14e+03  | 1.00e-08   | 2.74e-01   | [2.7e-01]            | [[3.5e+03 5.6e+02 8.2e-02] [5.6e+02 7.8e+02 1.1e-01] [8.2e-02 1.1e-01 2.7e-02]]\n",
      "            4.00e+01 | -3.14e+03  | 1.00e-08   | 2.71e-01   | [2.8e-01]            | [[3.6e+03 7.4e+02 1.1e-01] [7.4e+02 7.8e+02 1.1e-01] [1.1e-01 1.1e-01 2.6e-02]]\n",
      "            4.50e+01 | -3.15e+03  | 1.00e-08   | 2.66e-01   | [3.0e-01]            | [[3.6e+03 1.0e+03 1.3e-01] [1.0e+03 8.2e+02 1.1e-01] [1.3e-01 1.1e-01 2.5e-02]]\n",
      "            5.00e+01 | -3.15e+03  | 1.00e-08   | 2.65e-01   | [3.0e-01]            | [[3.0e+03 1.2e+03 1.5e-01] [1.2e+03 9.1e+02 1.1e-01] [1.5e-01 1.1e-01 2.4e-02]]\n",
      "            5.50e+01 | -3.15e+03  | 1.00e-08   | 2.70e-01   | [3.2e-01]            | [[2.8e+03 1.7e+03 1.7e-01] [1.7e+03 1.2e+03 1.1e-01] [1.7e-01 1.1e-01 2.3e-02]]\n",
      "            6.00e+01 | -3.17e+03  | 1.00e-08   | 2.85e-01   | [3.7e-01]            | [[2.0e+03 2.0e+03 1.2e-01] [2.0e+03 2.0e+03 1.2e-01] [1.2e-01 1.2e-01 2.0e-02]]\n",
      "            6.50e+01 | -3.17e+03  | 1.00e-08   | 2.92e-01   | [4.0e-01]            | [[2.1e+03 2.2e+03 5.7e-02] [2.2e+03 2.2e+03 5.9e-02] [5.7e-02 5.9e-02 1.7e-02]]\n",
      "            7.00e+01 | -3.17e+03  | 1.00e-08   | 2.97e-01   | [4.4e-01]            | [[2.1e+03 2.2e+03 6.4e-02] [2.2e+03 2.2e+03 6.6e-02] [6.4e-02 6.6e-02 1.5e-02]]\n",
      "            7.50e+01 | -3.17e+03  | 1.00e-08   | 2.97e-01   | [4.5e-01]            | [[2.1e+03 2.1e+03 5.9e-02] [2.1e+03 2.2e+03 6.1e-02] [5.9e-02 6.1e-02 1.5e-02]]\n",
      "            8.00e+01 | -3.17e+03  | 1.00e-08   | 2.98e-01   | [4.6e-01]            | [[2.0e+03 2.1e+03 5.9e-02] [2.1e+03 2.1e+03 6.1e-02] [5.9e-02 6.1e-02 1.4e-02]]\n",
      "            8.50e+01 | -3.17e+03  | 1.00e-08   | 2.97e-01   | [4.9e-01]            | [[1.9e+03 1.9e+03 5.5e-02] [1.9e+03 2.0e+03 5.7e-02] [5.5e-02 5.7e-02 1.3e-02]]\n",
      "            9.00e+01 | -3.18e+03  | 1.00e-08   | 2.97e-01   | [5.6e-01]            | [[1.7e+03 1.8e+03 5.0e-02] [1.8e+03 1.8e+03 5.2e-02] [5.0e-02 5.2e-02 1.1e-02]]\n",
      "            9.50e+01 | -3.18e+03  | 1.00e-08   | 2.97e-01   | [5.8e-01]            | [[1.6e+03 1.7e+03 4.6e-02] [1.7e+03 1.7e+03 4.8e-02] [4.6e-02 4.8e-02 1.1e-02]]\n",
      "            1.00e+02 | -3.18e+03  | 1.00e-08   | 2.97e-01   | [5.8e-01]            | [[1.6e+03 1.6e+03 4.6e-02] [1.6e+03 1.7e+03 4.8e-02] [4.6e-02 4.8e-02 1.1e-02]]\n",
      "            1.05e+02 | -3.18e+03  | 1.00e-08   | 2.97e-01   | [5.9e-01]            | [[1.6e+03 1.6e+03 4.6e-02] [1.6e+03 1.7e+03 4.8e-02] [4.6e-02 4.8e-02 1.1e-02]]\n",
      "            1.10e+02 | -3.18e+03  | 1.00e-08   | 2.97e-01   | [6.1e-01]            | [[1.6e+03 1.6e+03 4.5e-02] [1.6e+03 1.7e+03 4.7e-02] [4.5e-02 4.7e-02 1.1e-02]]\n",
      "            1.15e+02 | -3.18e+03  | 1.00e-08   | 2.96e-01   | [6.3e-01]            | [[1.5e+03 1.6e+03 4.4e-02] [1.6e+03 1.6e+03 4.5e-02] [4.4e-02 4.5e-02 1.1e-02]]\n",
      "            1.20e+02 | -3.18e+03  | 1.00e-08   | 2.96e-01   | [6.5e-01]            | [[1.5e+03 1.5e+03 4.3e-02] [1.5e+03 1.6e+03 4.4e-02] [4.3e-02 4.4e-02 1.0e-02]]\n",
      "            1.25e+02 | -3.18e+03  | 1.00e-08   | 2.96e-01   | [6.5e-01]            | [[1.5e+03 1.5e+03 4.2e-02] [1.5e+03 1.6e+03 4.4e-02] [4.2e-02 4.4e-02 1.0e-02]]\n",
      "            1.30e+02 | -3.18e+03  | 1.00e-08   | 2.96e-01   | [6.5e-01]            | [[1.4e+03 1.5e+03 4.2e-02] [1.5e+03 1.5e+03 4.3e-02] [4.2e-02 4.3e-02 1.0e-02]]\n",
      "            1.35e+02 | -3.18e+03  | 1.00e-08   | 2.96e-01   | [6.6e-01]            | [[1.4e+03 1.4e+03 4.1e-02] [1.4e+03 1.5e+03 4.3e-02] [4.1e-02 4.3e-02 1.0e-02]]\n",
      "            1.40e+02 | -3.18e+03  | 1.00e-08   | 2.96e-01   | [6.9e-01]            | [[1.4e+03 1.4e+03 4.1e-02] [1.4e+03 1.5e+03 4.2e-02] [4.1e-02 4.2e-02 9.6e-03]]\n",
      "            1.45e+02 | -3.18e+03  | 1.00e-08   | 2.96e-01   | [7.1e-01]            | [[1.3e+03 1.4e+03 4.0e-02] [1.4e+03 1.4e+03 4.1e-02] [4.0e-02 4.1e-02 9.4e-03]]\n",
      "            1.50e+02 | -3.18e+03  | 1.00e-08   | 2.95e-01   | [7.1e-01]            | [[1.3e+03 1.3e+03 3.8e-02] [1.3e+03 1.4e+03 3.9e-02] [3.8e-02 3.9e-02 9.3e-03]]\n",
      "            1.55e+02 | -3.18e+03  | 1.00e-08   | 2.95e-01   | [7.3e-01]            | [[1.2e+03 1.3e+03 3.8e-02] [1.3e+03 1.3e+03 3.9e-02] [3.8e-02 3.9e-02 9.2e-03]]\n",
      "            1.60e+02 | -3.18e+03  | 1.00e-08   | 2.94e-01   | [7.5e-01]            | [[1.2e+03 1.3e+03 3.7e-02] [1.3e+03 1.3e+03 3.8e-02] [3.7e-02 3.8e-02 9.0e-03]]\n",
      "            1.65e+02 | -3.18e+03  | 1.00e-08   | 2.93e-01   | [7.6e-01]            | [[1.2e+03 1.3e+03 3.6e-02] [1.3e+03 1.3e+03 3.8e-02] [3.6e-02 3.8e-02 8.8e-03]]\n",
      "            1.70e+02 | -3.18e+03  | 1.00e-08   | 2.91e-01   | [7.9e-01]            | [[1.2e+03 1.3e+03 3.5e-02] [1.3e+03 1.3e+03 3.7e-02] [3.5e-02 3.7e-02 8.5e-03]]\n",
      "            1.75e+02 | -3.18e+03  | 1.00e-08   | 2.92e-01   | [7.9e-01]            | [[1.2e+03 1.3e+03 3.5e-02] [1.3e+03 1.3e+03 3.7e-02] [3.5e-02 3.7e-02 8.5e-03]]\n",
      "            1.76e+02 | -3.18e+03  | 1.00e-08   | 2.91e-01   | [7.9e-01]            | [[1.2e+03 1.3e+03 3.5e-02] [1.3e+03 1.3e+03 3.6e-02] [3.5e-02 3.6e-02 8.5e-03]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['mll_hist', 'scale_hist', 'lengthscales_hist', 'task_kernel_hist']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = fgp.fit()\n",
    "list(data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pmean.shape = (3, 128)\n",
      "pvar.shape = (3, 128)\n",
      "q = 2.58\n",
      "ci_low.shape = (3, 128)\n",
      "ci_high.shape = (3, 128)\n",
      "l2 relative error = tensor([4.7058e-03, 5.7482e-02, 1.1861e-08])\n",
      "pcov.shape = (3, 3, 128, 128)\n",
      "pcov2.shape = (3, 3, 128, 256)\n"
     ]
    }
   ],
   "source": [
    "pmean,pvar,q,ci_low,ci_high = fgp.post_ci(x,confidence=0.99)\n",
    "print(\"pmean.shape = %s\"%str(tuple(pmean.shape)))\n",
    "print(\"pvar.shape = %s\"%str(tuple(pvar.shape)))\n",
    "print(\"q = %.2f\"%q)\n",
    "print(\"ci_low.shape = %s\"%str(tuple(ci_low.shape)))\n",
    "print(\"ci_high.shape = %s\"%str(tuple(ci_high.shape)))\n",
    "print(\"l2 relative error =\",(torch.linalg.norm(y-pmean,dim=1)/torch.linalg.norm(y,dim=1)))\n",
    "pcov = fgp.post_cov(x,x)\n",
    "print(\"pcov.shape = %s\"%str(tuple(pcov.shape)))\n",
    "_range0,_rangen1 = torch.arange(pcov.size(0)),torch.arange(pcov.size(-1))\n",
    "assert torch.allclose(pcov[_range0,_range0][:,_rangen1,_rangen1],pvar) and (pvar>=0).all()\n",
    "pcov2 = fgp.post_cov(x,z)\n",
    "print(\"pcov2.shape = %s\"%str(tuple(pcov2.shape)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pcmean = tensor([ 1.6943e+01,  1.8140e+01, -9.6895e-13])\n",
      "pcvar = tensor([3.6033e-05, 2.9637e-03, 1.3171e-12])\n",
      "cci_low = tensor([ 1.6928e+01,  1.8000e+01, -2.9561e-06])\n",
      "cci_high tensor([1.6959e+01, 1.8281e+01, 2.9561e-06])\n"
     ]
    }
   ],
   "source": [
    "pcmean,pcvar,q,cci_low,cci_high = fgp.post_cubature_ci(confidence=0.99)\n",
    "print(\"pcmean =\",pcmean)\n",
    "print(\"pcvar =\",pcvar)\n",
    "print(\"cci_low =\",cci_low)\n",
    "print(\"cci_high\",cci_high)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project and Increase Sample Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_new = fgp.n*torch.tensor([4,2,8])\n",
    "pcov_future = fgp.post_cov(x,z,n=n_new)\n",
    "pvar_future = fgp.post_var(x,n=n_new)\n",
    "pcvar_future = fgp.post_cubature_var(n=n_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([192])\n",
      "torch.Size([8])\n",
      "torch.Size([1792])\n",
      "l2 relative error = tensor([3.5162e-04, 5.9589e-02, 1.1945e-09])\n"
     ]
    }
   ],
   "source": [
    "x_next = fgp.get_x_next(n_new)\n",
    "y_next = [fs[i](x_next[i]) for i in range(fgp.num_tasks)]\n",
    "for _y in y_next:\n",
    "    print(_y.shape)\n",
    "fgp.add_y_next(y_next)\n",
    "print(\"l2 relative error =\",(torch.linalg.norm(y-fgp.post_mean(x),dim=1)/torch.linalg.norm(y,dim=1)))\n",
    "assert torch.allclose(fgp.post_cov(x,z),pcov_future)\n",
    "assert torch.allclose(fgp.post_var(x),pvar_future)\n",
    "assert torch.allclose(fgp.post_cubature_var(),pcvar_future)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l2 relative error = tensor([3.4562e-04, 5.9565e-02, 5.7267e-12])\n"
     ]
    }
   ],
   "source": [
    "data = fgp.fit(verbose=False,store_mll_hist=False,store_scale_hist=False,store_lengthscales_hist=False,store_noise_hist=False)\n",
    "print(\"l2 relative error =\",(torch.linalg.norm(y-fgp.post_mean(x),dim=1)/torch.linalg.norm(y,dim=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_new = fgp.n*torch.tensor([4,8,2])\n",
    "pcov_new = fgp.post_cov(x,z,n=n_new)\n",
    "pvar_new = fgp.post_var(x,n=n_new)\n",
    "pcvar_new = fgp.post_cubature_var(n=n_new)\n",
    "x_next = fgp.get_x_next(n_new)\n",
    "y_next = [fs[i](x_next[i]) for i in range(fgp.num_tasks)]\n",
    "fgp.add_y_next(y_next)\n",
    "assert torch.allclose(fgp.post_cov(x,z),pcov_new)\n",
    "assert torch.allclose(fgp.post_var(x),pvar_new)\n",
    "assert torch.allclose(fgp.post_cubature_var(),pcvar_new)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fgp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
