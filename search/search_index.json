{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"<code>fastgps</code>: Fast Gaussian Process Regression in Python","text":""},{"location":"#installation","title":"Installation","text":"<pre><code>pip install fastgps\n</code></pre>"},{"location":"#overview","title":"Overview","text":"<p>Gaussian process (GP) regression on \\(n\\) data points typically require \\(\\mathcal{O}(n^3)\\) computations and \\(\\mathcal{O}(n^2)\\) storage. Fast GPs only require \\(\\mathcal{O}(n \\log n)\\) computations and \\(\\mathcal{O}(n)\\) storage by forcing nice structure into the \\(n \\times n\\) Gram matrix of pairwise kernel evaluations. Fast GPs require</p> <ul> <li>Control over the design of experiments, i.e., sampling at fixed locations which we will choose to be quasi-random (low-discrepancy) sequences, and</li> <li>Using special kernel forms that are practically performant but generally uncommon, e.g., one cannot use common kernels such as the Squared Exponential, Matern, or Rational Quadratic. We will use (digitally)-shift invariant kernels.</li> </ul>"},{"location":"#scope","title":"Scope","text":"<p><code>fastgps</code> currently support two flavors:</p> <ol> <li>Pairing rank-1 integration lattices with shift-invariant (SI) kernels creates circulant Gram matrices that are diagonalizable by Fast Fourier Transforms (FFTs). SI kernels are periodic and arbitrarily smooth.</li> <li>Pairing digital sequences (e.g. Sobol' sequences) with digitally-shift-invariant (DSI) kernels creates Gram matrices diagonalizable by Fast Walsh-Hadamard Transforms (FWHTs). DSI kernels are discontinuous, yet versions exist for which the corresponding Reproducing Kernel Hilbert Space (RKHSs) contains arbitrarily smooth functions.</li> </ol>"},{"location":"#features","title":"Features","text":"<p>A reference standard GP implementation is available alongside the fast GP implementations. All GP methods support:</p> <ul> <li>GPU computations as <code>fastgps</code> is built on the <code>PyTorch</code> stack.</li> <li>Batching of both outputs (for functions with tensor outputs) and parameters (with flexibly shareable parameters among batched outputs).</li> <li>Multi-Task GPs with product kernels and generalized fast multi-task GPs.</li> <li>Derivative Information of arbitrarily high order.</li> <li>Bayesian Cubature for approximating integrals or expectations.</li> <li>Flexible kernel parameterizations from the <code>QMCPy</code> package.</li> <li>Efficient variance projections for determining if and where to sample next.</li> </ul>"},{"location":"#resources","title":"Resources","text":"<p>The <code>fastgps</code> documentation contains a detailed package reference documenting classes including thorough doctests. A number of example notebooks are also rendered into the documentation from <code>fastgps/docs/examples/</code>. We recommend reading Aleksei Sorokin's slides on Fast GPs which he presented at MCM 2025 Chicago.</p>"},{"location":"#citation","title":"Citation","text":"<p>If you find the <code>fastgps</code> package helpful in your work, please consider citing the following papers</p> <pre><code>@phdthesis{sorokin.thesis,\n  title               = {Algorithms and scientific software for quasi-{M}onte {C}arlo, fast {G}aussian process regression, and scientific machine learning},\n  author              = {Aleksei G. Sorokin},\n  year                = {2025},\n  school              = {Illinois Institute of Technology},\n  journal             = {ArXiv preprint},\n  volume              = {abs/2511.21915},\n  url                 = {https://arxiv.org/abs/2511.21915},\n}\n\n@inproceedings{sorokin.fastgps_probnum25,\n  title               = {Fast {G}aussian process regression for high dimensional functions with derivative information},\n  author              = {Sorokin, Aleksei G. and Robbe, Pieterjan and Hickernell, Fred J.},\n  year                = {2025},\n  booktitle           = {Proceedings of the First International Conference on Probabilistic Numerics},\n  publisher           = {{PMLR}},\n  series              = {Proceedings of Machine Learning Research},\n  volume              = {271},\n  pages               = {35--49},\n  url                 = {https://proceedings.mlr.press/v271/sorokin25a.html},\n  editor              = {Kanagawa, Motonobu and Cockayne, Jon and Gessner, Alexandra and Hennig, Philipp},\n  pdf                 = {https://raw.githubusercontent.com/mlresearch/v271/main/assets/sorokin25a/sorokin25a.pdf},\n}\n\n@article{sorokin.FastBayesianMLQMC,\n  title               = {Fast {B}ayesian multilevel quasi-{M}onte {C}arlo},\n  author              = {Aleksei G. Sorokin and Pieterjan Robbe and Gianluca  Geraci and Michael S. Eldred and Fred J. Hickernell},\n  year                = {2025},\n  journal             = {ArXiv preprint},\n  volume              = {abs/2510.24604},\n  url                 = {https://arxiv.org/abs/2510.24604},\n}\n</code></pre>"},{"location":"api/","title":"API","text":""},{"location":"api/#fastgps.abstract_gp.AbstractGP","title":"AbstractGP","text":"<pre><code>AbstractGP(kernel, seqs, num_tasks, default_task, solo_task, noise, tfs_noise, requires_grad_noise, shape_noise, derivatives, derivatives_coeffs, adaptive_nugget, ptransform)\n</code></pre> <p>               Bases: <code>Module</code></p> Source code in <code>fastgps/abstract_gp.py</code> <pre><code>def __init__(\n        self,\n        kernel,\n        seqs,\n        num_tasks,\n        default_task,\n        solo_task,\n        noise,\n        tfs_noise,\n        requires_grad_noise,\n        shape_noise,\n        derivatives,\n        derivatives_coeffs,\n        adaptive_nugget,\n        ptransform\n    ):\n    super().__init__()\n    if not torch.get_default_dtype()==torch.float64:\n        warnings.warn('''\n            Using torch.float32 precision may significantly hurt FastGPs accuracy. \n            This is especailly evident when computing posterior variance and covariance values. \n            If possible, please use\n                torch.set_default_dtype(torch.float64)''')\n    # copy kernel parameters \n    self.kernel = kernel\n    assert self.kernel.torchify, \"requires torchify=True for the kernel\"\n    self.device = self.kernel.device\n    self.d = self.kernel.d\n    # multi-task kernel abstraction\n    assert isinstance(num_tasks,int) and num_tasks&gt;0\n    self.num_tasks = num_tasks\n    self.default_task = default_task\n    self.solo_task = solo_task\n    self.task_range = torch.arange(num_tasks,device=self.device)\n    if solo_task:\n        self.kernel = qp.KernelMultiTask(\n            base_kernel = self.kernel,\n            num_tasks = 1, \n            factor = 1.,\n            diag =  0.,\n            requires_grad_factor = False, \n            requires_grad_diag = False,\n            tfs_diag = (qp.util.transforms.tf_identity,qp.util.transforms.tf_identity),\n            rank_factor = 1)\n    assert isinstance(self.kernel,qp.KernelMultiTask)\n    # seqs setup \n    assert isinstance(seqs,np.ndarray) and seqs.shape==(self.num_tasks,)\n    assert all(seqs[i].d==self.d for i in range(self.num_tasks))\n    self.seqs = seqs\n    self.n = torch.zeros(self.num_tasks,dtype=int,device=self.device)\n    self.n_cumsum = torch.zeros(self.num_tasks,dtype=int,device=self.device)\n    self.m = -1*torch.ones(self.num_tasks,dtype=int,device=self.device)\n    # derivatives setup \n    if derivatives is None: derivatives = [torch.zeros((1,self.d),dtype=torch.int64,device=self.device) for i in range(self.num_tasks)]\n    if isinstance(derivatives,torch.Tensor): derivatives = [derivatives]\n    assert isinstance(derivatives,list) and len(derivatives)==self.num_tasks\n    derivatives = [deriv[None,:] if deriv.ndim==1 else deriv for deriv in derivatives]\n    assert all((derivatives[i].ndim==2 and derivatives[i].size(1)==self.d) for i in range(self.num_tasks))\n    if derivatives_coeffs is None: derivatives_coeffs = [torch.ones(len(derivatives[i]),device=self.device) for i in range(self.num_tasks)]\n    assert isinstance(derivatives_coeffs,list) and len(derivatives_coeffs)==self.num_tasks\n    assert all((derivatives_coeffs[i].ndim==1 and len(derivatives_coeffs[i]))==len(derivatives[i]) for i in range(self.num_tasks))\n    self.derivatives_cross = [[[None,None] for l1 in range(self.num_tasks)] for l0 in range(self.num_tasks)]\n    self.derivatives_coeffs_cross = [[None for l1 in range(self.num_tasks)] for l0 in range(self.num_tasks)]\n    self.derivatives_flag = any((derivatives_i!=0).any() for derivatives_i in derivatives) \n    if not self.derivatives_flag:\n        assert all((derivatives_coeffs_i==1).all() for derivatives_coeffs_i in derivatives_coeffs) \n    for l0 in range(self.num_tasks):\n        p0r = torch.arange(len(derivatives_coeffs[l0]),device=self.device)\n        for l1 in range(l0+1):\n            p1r = torch.arange(len(derivatives_coeffs[l1]),device=self.device)\n            i0m,i1m = torch.meshgrid(p0r,p1r,indexing=\"ij\")\n            i0,i1 = i0m.flatten(),i1m.flatten()\n            self.derivatives_cross[l0][l1][0] = derivatives[l0][i0]\n            self.derivatives_cross[l0][l1][1] = derivatives[l1][i1]\n            self.derivatives_coeffs_cross[l0][l1] = derivatives_coeffs[l0][i0]*derivatives_coeffs[l1][i1]\n            if l0!=l1:\n                self.derivatives_cross[l1][l0][0] = self.derivatives_cross[l0][l1][1]\n                self.derivatives_cross[l1][l0][1] = self.derivatives_cross[l0][l1][0]\n                self.derivatives_coeffs_cross[l1][l0] = self.derivatives_coeffs_cross[l0][l1]\n    # noise\n    self.raw_noise = self.kernel.parse_assign_param(\n        pname = \"noise\",\n        param = noise,\n        shape_param = shape_noise,\n        requires_grad_param = requires_grad_noise,\n        tfs_param = tfs_noise,\n        endsize_ops = [1],\n        constraints = [\"NON-NEGATIVE\"])\n    self.tfs_noise = tfs_noise\n    self.prior_mean = torch.zeros(self.num_tasks,device=self.device)\n    # storage and dynamic caches\n    self._y = [torch.empty(0,device=self.device) for l in range(self.num_tasks)]\n    self.xxb_seqs = np.array([_XXbSeq(self,self.seqs[i]) for i in range(self.num_tasks)],dtype=object)\n    self.n_x = torch.zeros(self.num_tasks,dtype=int)\n    self.inv_log_det_cache_dict = {}\n    # derivative multitask setting checks \n    if any((derivatives[i]&gt;0).any() or (derivatives_coeffs[i]!=1).any() for i in range(self.num_tasks)):\n        self.kernel.raw_factor.requires_grad_(False)\n        self.kernel.raw_diag.requires_grad_(False)\n        assert (self.kernel.taskmat==1).all()\n    self.adaptive_nugget = adaptive_nugget\n    self.batch_param_names = [\"noise\"]\n    self.stable = False # maybe change this in the future if we come across any more calcellation error issues\n    self.ptransform = str(ptransform).upper()\n    if self.ptransform=='TENT': self.ptransform = 'BAKER'\n    assert self.ptransform in ['NONE','BAKER'], \"invalid ptransform = %s\"%self.ptransform\n</code></pre>"},{"location":"api/#fastgps.abstract_gp.AbstractGP.noise","title":"noise  <code>property</code>","text":"<pre><code>noise\n</code></pre> <p>Noise parameter.</p>"},{"location":"api/#fastgps.abstract_gp.AbstractGP.coeffs","title":"coeffs  <code>property</code>","text":"<pre><code>coeffs\n</code></pre> <p>Coefficients \\(\\mathsf{K}^{-1} \\boldsymbol{y}\\).</p>"},{"location":"api/#fastgps.abstract_gp.AbstractGP.x","title":"x  <code>property</code>","text":"<pre><code>x\n</code></pre> <p>Current sampling locations.  A <code>torch.Tensor</code> for single task problems. A <code>list</code> for multitask problems.</p>"},{"location":"api/#fastgps.abstract_gp.AbstractGP.y","title":"y  <code>property</code>","text":"<pre><code>y\n</code></pre> <p>Current sampling values.  A <code>torch.Tensor</code> for single task problems. A <code>list</code> for multitask problems.</p>"},{"location":"api/#fastgps.abstract_gp.AbstractGP.save_params","title":"save_params","text":"<pre><code>save_params(path)\n</code></pre> <p>Save the state dict to path </p> Arg <p>path (str): the path.</p> Source code in <code>fastgps/abstract_gp.py</code> <pre><code>def save_params(self, path):\n    \"\"\" Save the state dict to path \n\n    Arg:\n        path (str): the path. \n    \"\"\"\n    torch.save(self.state_dict(),path)\n</code></pre>"},{"location":"api/#fastgps.abstract_gp.AbstractGP.load_params","title":"load_params","text":"<pre><code>load_params(path)\n</code></pre> <p>Load the state dict from path </p> Arg <p>path (str): the path.</p> Source code in <code>fastgps/abstract_gp.py</code> <pre><code>def load_params(self, path):\n    \"\"\" Load the state dict from path \n\n    Arg:\n        path (str): the path. \n    \"\"\"\n    self.load_state_dict(torch.load(path,weights_only=True))\n</code></pre>"},{"location":"api/#fastgps.abstract_gp.AbstractGP.fit","title":"fit","text":"<pre><code>fit(loss_metric='MLL', iterations=5000, lr=None, optimizer=None, stop_crit_improvement_threshold=0.05, stop_crit_wait_iterations=10, store_hists=False, verbose=5, verbose_indent=4, cv_weights=1, update_prior_mean=True)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>loss_metric</code> <code>str</code> <p>either \"MLL\" (Marginal Log Likelihood) or \"CV\" (Cross Validation) or \"GCV\" (Generalized CV)</p> <code>'MLL'</code> <code>iterations</code> <code>int</code> <p>number of optimization iterations</p> <code>5000</code> <code>lr</code> <code>float</code> <p>learning rate for default optimizer</p> <code>None</code> <code>optimizer</code> <code>Optimizer</code> <p>optimizer defaulted to <code>torch.optim.Rprop(self.parameters(),lr=lr)</code></p> <code>None</code> <code>stop_crit_improvement_threshold</code> <code>float</code> <p>stop fitting when the maximum number of iterations is reached or the best loss is note reduced by <code>stop_crit_improvement_threshold</code> for <code>stop_crit_wait_iterations</code> iterations </p> <code>0.05</code> <code>stop_crit_wait_iterations</code> <code>int</code> <p>number of iterations to wait for improved loss before early stopping, see the argument description for <code>stop_crit_improvement_threshold</code></p> <code>10</code> <code>store_hists</code> <code>Union[bool, int]</code> <p>store parameter data every <code>store_hists</code> iterations.</p> <code>False</code> <code>verbose</code> <code>int</code> <p>log every <code>verbose</code> iterations, set to <code>0</code> for silent mode</p> <code>5</code> <code>verbose_indent</code> <code>int</code> <p>size of the indent to be applied when logging, helpful for logging multiple models</p> <code>4</code> <code>cv_weights</code> <code>Union[str, Tensor]</code> <p>weights for cross validation</p> <code>1</code> <code>update_prior_mean</code> <code>bool</code> <p>if <code>True</code>, then update the prior mean to optimize the loss.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>hist_data</code> <code>dict</code> <p>iteration history data.</p> Source code in <code>fastgps/abstract_gp.py</code> <pre><code>def fit(\n        self,\n        loss_metric:str = \"MLL\",\n        iterations:int = 5000,\n        lr:float = None,\n        optimizer:torch.optim.Optimizer = None,\n        stop_crit_improvement_threshold:float = 5e-2,\n        stop_crit_wait_iterations:int = 10,\n        store_hists:bool = False,\n        verbose:int = 5,\n        verbose_indent:int = 4,\n        cv_weights:torch.Tensor = 1,\n        update_prior_mean:bool = True,\n        ):\n    \"\"\"\n    Args:\n        loss_metric (str): either \"MLL\" (Marginal Log Likelihood) or \"CV\" (Cross Validation) or \"GCV\" (Generalized CV)\n        iterations (int): number of optimization iterations\n        lr (float): learning rate for default optimizer\n        optimizer (torch.optim.Optimizer): optimizer defaulted to `torch.optim.Rprop(self.parameters(),lr=lr)`\n        stop_crit_improvement_threshold (float): stop fitting when the maximum number of iterations is reached or the best loss is note reduced by `stop_crit_improvement_threshold` for `stop_crit_wait_iterations` iterations \n        stop_crit_wait_iterations (int): number of iterations to wait for improved loss before early stopping, see the argument description for `stop_crit_improvement_threshold`\n        store_hists (Union[bool,int]): store parameter data every `store_hists` iterations.\n        verbose (int): log every `verbose` iterations, set to `0` for silent mode\n        verbose_indent (int): size of the indent to be applied when logging, helpful for logging multiple models\n        cv_weights (Union[str,torch.Tensor]): weights for cross validation\n        update_prior_mean (bool): if `True`, then update the prior mean to optimize the loss.\n\n    Returns:\n        hist_data (dict): iteration history data.\n    \"\"\"\n    assert isinstance(loss_metric,str) and loss_metric.upper() in [\"MLL\",\"GCV\",\"CV\"] \n    assert (self.n&gt;0).any(), \"cannot fit without data\"\n    assert isinstance(iterations,int) and iterations&gt;=0\n    if optimizer is None:\n        optimizer = self.get_default_optimizer(lr)\n    assert isinstance(optimizer,torch.optim.Optimizer)\n    assert isinstance(store_hists,int), \"require int store_mll_hist\" \n    assert (isinstance(verbose,int) or isinstance(verbose,bool)) and verbose&gt;=0, \"require verbose is a non-negative int\"\n    assert isinstance(verbose_indent,int) and verbose_indent&gt;=0, \"require verbose_indent is a non-negative int\"\n    assert np.isscalar(stop_crit_improvement_threshold) and 0&lt;stop_crit_improvement_threshold, \"require stop_crit_improvement_threshold is a positive float\"\n    assert (isinstance(stop_crit_wait_iterations,int) or stop_crit_wait_iterations==np.inf) and stop_crit_wait_iterations&gt;0\n    loss_metric = loss_metric.upper()\n    logtol = np.log(1+stop_crit_improvement_threshold)\n    if isinstance(cv_weights,str) and cv_weights.upper()==\"L2R\":\n        _y = self.y \n        if _y.ndim==1:\n            cv_weights = 1/torch.abs(_y) \n        else:\n            cv_weights = 1/torch.linalg.norm(_y,2,dim=[i for i in range(_y.ndim-1)])\n    if store_hists:\n        hist_data = {}\n        hist_data[\"iteration\"] = []\n        hist_data[\"loss\"] = []\n        hist_data[\"best_loss\"] = []\n        hist_data[\"prior_mean\"] = []\n        for pname in self.batch_param_names:\n            hist_data[pname] = []\n        for pname in self.kernel.batch_param_names:\n            hist_data[pname] = []\n        for pname in self.kernel.base_kernel.batch_param_names:\n            hist_data[pname] = []\n    else:\n        hist_data = {}\n    if verbose:\n        _s = \"%16s | %-10s | %-10s\"%(\"iter of %.1e\"%iterations,\"best loss\",\"loss\")\n        print(\" \"*verbose_indent+_s)\n        print(\" \"*verbose_indent+\"~\"*len(_s))\n    stop_crit_best_loss = torch.inf \n    stop_crit_save_loss = torch.inf \n    stop_crit_iterations_without_improvement_loss = 0\n    update_prior_mean = update_prior_mean and (not self.derivatives_flag) and loss_metric!=\"CV\"\n    inv_log_det_cache = self.get_inv_log_det_cache()\n    best_params = None\n    try:\n        pcstd = torch.sqrt(self.post_cubature_var(eval=False))\n        can_compute_pcstd = True\n    except Exception as e:\n        if \"parsed_single_integral_01d\" not in str(e): raise\n        can_compute_pcstd = False\n    for i in range(iterations+1):\n        os.environ[\"FASTGP_FORCE_RECOMPILE\"] = \"True\"\n        if loss_metric==\"MLL\":\n            loss = inv_log_det_cache.mll_loss(self,update_prior_mean)\n        elif loss_metric==\"GCV\":\n            loss = inv_log_det_cache.gcv_loss(self,update_prior_mean)\n        elif loss_metric==\"CV\":\n            loss = inv_log_det_cache.cv_loss(self,cv_weights,update_prior_mean)\n        else:\n            assert False, \"loss_metric parsing implementation error\"\n        del os.environ[\"FASTGP_FORCE_RECOMPILE\"]\n        pcstd = torch.sqrt(self.post_cubature_var()) if can_compute_pcstd else torch.inf*torch.ones(1,device=self.device)\n        if loss.item()&lt;stop_crit_best_loss and (pcstd&gt;0).all():\n            stop_crit_best_loss = loss.item()\n            best_params = (self.prior_mean.clone().detach(),OrderedDict([(pname,pval.clone()) for pname,pval in self.state_dict().items()]))\n        if (stop_crit_save_loss-loss.item())&gt;logtol:\n            stop_crit_iterations_without_improvement_loss = 0\n            stop_crit_save_loss = stop_crit_best_loss\n        else:\n            stop_crit_iterations_without_improvement_loss += 1\n        break_condition = i==iterations or stop_crit_iterations_without_improvement_loss==stop_crit_wait_iterations or (pcstd&lt;=0).any()\n        if store_hists and (break_condition or i%store_hists==0):\n            hist_data[\"iteration\"].append(i)\n            hist_data[\"loss\"].append(loss.item())\n            hist_data[\"best_loss\"].append(stop_crit_best_loss)\n            hist_data[\"prior_mean\"].append(self.prior_mean.clone())\n            for pname in self.batch_param_names:\n                hist_data[pname].append(getattr(self,pname).data.detach().clone().cpu())\n            for pname in self.kernel.batch_param_names:\n                hist_data[pname].append(getattr(self.kernel,pname).data.detach().clone().cpu())\n            for pname in self.kernel.base_kernel.batch_param_names:\n                hist_data[pname].append(getattr(self.kernel.base_kernel,pname).data.detach().clone().cpu())\n        if verbose and (i%verbose==0 or break_condition):\n            _s = \"%16.2e | %-10.2e | %-10.2e\"%(i,stop_crit_best_loss,loss.item())\n            print(\" \"*verbose_indent+_s)\n        if break_condition: break\n        # with torch.autograd.set_detect_anomaly(True):\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n    if best_params is not None:\n        self.prior_mean = best_params[0]\n        self.load_state_dict(best_params[1])\n    if store_hists:\n        hist_data[\"iteration\"] = torch.tensor(hist_data[\"iteration\"])\n        hist_data[\"loss\"] = torch.tensor(hist_data[\"loss\"])\n        hist_data[\"best_loss\"] = torch.tensor(hist_data[\"best_loss\"])\n        hist_data[\"prior_mean\"] = torch.stack(hist_data[\"prior_mean\"],dim=0)\n        for pname in self.batch_param_names:\n            hist_data[pname] = torch.stack(hist_data[pname],dim=0)\n        for pname in self.kernel.batch_param_names:\n            hist_data[pname] = torch.stack(hist_data[pname],dim=0)\n        for pname in self.kernel.base_kernel.batch_param_names:\n            hist_data[pname] = torch.stack(hist_data[pname],dim=0)\n    return hist_data\n</code></pre>"},{"location":"api/#fastgps.abstract_gp.AbstractGP.get_x_next","title":"get_x_next","text":"<pre><code>get_x_next(n, task=None)\n</code></pre> <p>Get the next sampling locations. </p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>Union[int, Tensor]</code> <p>maximum sample index per task</p> required <code>task</code> <code>Union[int, Tensor]</code> <p>task index</p> <code>None</code> <p>Returns:</p> Name Type Description <code>x_next</code> <code>Union[Tensor, List]</code> <p>next samples in the sequence</p> Source code in <code>fastgps/abstract_gp.py</code> <pre><code>def get_x_next(self, n:Union[int,torch.Tensor], task:Union[int,torch.Tensor]=None):\n    \"\"\"\n    Get the next sampling locations. \n\n    Args:\n        n (Union[int,torch.Tensor]): maximum sample index per task\n        task (Union[int,torch.Tensor]): task index\n\n    Returns:\n        x_next (Union[torch.Tensor,List]): next samples in the sequence\n    \"\"\"\n    if isinstance(n,(int,np.int64)): n = torch.tensor([n],dtype=int,device=self.device) \n    if isinstance(n,list): n = torch.tensor(n,dtype=int,device=self.device)\n    if task is None: task = self.default_task\n    inttask = isinstance(task,int)\n    if inttask: task = torch.tensor([task],dtype=int,device=self.device)\n    if isinstance(task,list): task = torch.tensor(task,dtype=int,device=self.device)\n    assert isinstance(n,torch.Tensor) and isinstance(task,torch.Tensor) and n.ndim==task.ndim==1 and len(n)==len(task)\n    assert (n&gt;=self.n[task]).all(), \"maximum sequence index must be greater than the current number of samples\"\n    x_next = [self.xxb_seqs[l].getitem_x(self,slice(self.n[l],n[i])) for i,l in enumerate(task)]\n    return x_next[0] if inttask else x_next\n</code></pre>"},{"location":"api/#fastgps.abstract_gp.AbstractGP.add_y_next","title":"add_y_next","text":"<pre><code>add_y_next(y_next, task=None)\n</code></pre> <p>Add samples to the GP. </p> <p>Parameters:</p> Name Type Description Default <code>y_next</code> <code>Union[Tensor, List]</code> <p>new function evaluations at next sampling locations</p> required <code>task</code> <code>Union[int, Tensor]</code> <p>task index</p> <code>None</code> Source code in <code>fastgps/abstract_gp.py</code> <pre><code>def add_y_next(self, y_next:Union[torch.Tensor,List], task:Union[int,torch.Tensor]=None):\n    \"\"\"\n    Add samples to the GP. \n\n    Args:\n        y_next (Union[torch.Tensor,List]): new function evaluations at next sampling locations\n        task (Union[int,torch.Tensor]): task index\n    \"\"\"\n    if isinstance(y_next,torch.Tensor): y_next = [y_next]\n    if task is None: task = self.default_task\n    if isinstance(task,int): task = torch.tensor([task],dtype=int,device=self.device)\n    if isinstance(task,list): task = torch.tensor(task,dtype=int,device=self.device)\n    assert isinstance(y_next,list) and isinstance(task,torch.Tensor) and task.ndim==1 and len(y_next)==len(task)\n    for i,l in enumerate(task):\n        self._y[l] = torch.cat([self._y[l],y_next[i]],-1)\n    shape_batch = list(self._y[0].shape[:-1])\n    if (self.n==0).all() and len(shape_batch)&gt;0:\n        self.prior_mean = torch.zeros(shape_batch+[self.num_tasks],device=self.device)\n    self.n = torch.tensor([self._y[i].size(-1) for i in range(self.num_tasks)],dtype=int,device=self.device)\n    self.m = torch.where(self.n==0,-1,torch.log2(self.n).round()).to(int) # round to avoid things like torch.log2(torch.tensor([2**3],dtype=torch.int64,device=\"cuda\")).item() = 2.9999999999999996\n    self.n_cumsum = torch.hstack([torch.zeros(1,dtype=self.n.dtype,device=self.n.device),self.n.cumsum(0)[:-1]])\n    for key in list(self.inv_log_det_cache_dict.keys()):\n        if (torch.tensor(key)&lt;self.n.cpu()).any():\n            del self.inv_log_det_cache_dict[key]\n</code></pre>"},{"location":"api/#fastgps.abstract_gp.AbstractGP.post_mean","title":"post_mean","text":"<pre><code>post_mean(x, task=None, eval=True)\n</code></pre> <p>Posterior mean. </p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor[N, d]</code> <p>sampling locations</p> required <code>task</code> <code>Union[int, Tensor[T]]</code> <p>task index</p> <code>None</code> <code>eval</code> <code>bool</code> <p>if <code>True</code>, disable gradients, otherwise use <code>torch.is_grad_enabled()</code></p> <code>True</code> <p>Returns:</p> Name Type Description <code>pmean</code> <code>Tensor[..., T, N]</code> <p>posterior mean</p> Source code in <code>fastgps/abstract_gp.py</code> <pre><code>def post_mean(self, x:torch.Tensor, task:Union[int,torch.Tensor]=None, eval:bool=True):\n    \"\"\"\n    Posterior mean. \n\n    Args:\n        x (torch.Tensor[N,d]): sampling locations\n        task (Union[int,torch.Tensor[T]]): task index\n        eval (bool): if `True`, disable gradients, otherwise use `torch.is_grad_enabled()`\n\n    Returns:\n        pmean (torch.Tensor[...,T,N]): posterior mean\n    \"\"\"\n    coeffs = self.coeffs\n    if eval:\n        incoming_grad_enabled = torch.is_grad_enabled()\n        torch.set_grad_enabled(False)\n    assert x.ndim==2 and x.size(1)==self.d, \"x must a torch.Tensor with shape (-1,d)\"\n    if task is None: task = self.default_task\n    inttask = isinstance(task,int)\n    if inttask: task = torch.tensor([task],dtype=int,device=self.device)\n    if isinstance(task,list): task = torch.tensor(task,dtype=int,device=self.device)\n    assert task.ndim==1 and (task&gt;=0).all() and (task&lt;self.num_tasks).all()\n    if self.ptransform==\"NONE\":\n        kmat = torch.cat([torch.cat([self.kernel(task[l0],l1,x[:,None,:],self.get_xb(l1)[None,:,:],*self.derivatives_cross[task[l0]][l1],self.derivatives_coeffs_cross[task[l0]][l1]) for l1 in range(self.num_tasks)],dim=-1)[...,None,:,:] for l0 in range(len(task))],dim=-3)\n    elif self.ptransform==\"BAKER\":\n        kmat_left = torch.cat([torch.cat([self.kernel(task[l0],l1,x[:,None,:]/2,self.get_xb(l1)[None,:,:],*self.derivatives_cross[task[l0]][l1],self.derivatives_coeffs_cross[task[l0]][l1]) for l1 in range(self.num_tasks)],dim=-1)[...,None,:,:] for l0 in range(len(task))],dim=-3)\n        kmat_right = torch.cat([torch.cat([self.kernel(task[l0],l1,1-x[:,None,:]/2,self.get_xb(l1)[None,:,:],*self.derivatives_cross[task[l0]][l1],self.derivatives_coeffs_cross[task[l0]][l1]) for l1 in range(self.num_tasks)],dim=-1)[...,None,:,:] for l0 in range(len(task))],dim=-3)\n        kmat = 1/2*(kmat_left+kmat_right)\n    else:\n        raise Exception(\"invalid ptransform = %s\"%self.ptransform)\n    pmean = self.prior_mean[...,task,None]+torch.einsum(\"...i,...i-&gt;...\",kmat,coeffs[...,None,None,:])\n    if eval:\n        torch.set_grad_enabled(incoming_grad_enabled)\n    return pmean[...,0,:] if inttask else pmean\n</code></pre>"},{"location":"api/#fastgps.abstract_gp.AbstractGP.post_var","title":"post_var","text":"<pre><code>post_var(x, task=None, n=None, eval=True)\n</code></pre> <p>Posterior variance.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor[N, d]</code> <p>sampling locations</p> required <code>task</code> <code>Union[int, Tensor[T]]</code> <p>task indices</p> <code>None</code> <code>n</code> <code>Union[int, Tensor[num_tasks]]</code> <p>number of points at which to evaluate the posterior cubature variance.</p> <code>None</code> <code>eval</code> <code>bool</code> <p>if <code>True</code>, disable gradients, otherwise use <code>torch.is_grad_enabled()</code></p> <code>True</code> <p>Returns:</p> Name Type Description <code>pvar</code> <code>Tensor[T, N]</code> <p>posterior variance</p> Source code in <code>fastgps/abstract_gp.py</code> <pre><code>def post_var(self, x:torch.Tensor, task:Union[int,torch.Tensor]=None, n:Union[int,torch.Tensor]=None, eval:bool=True):\n    \"\"\"\n    Posterior variance.\n\n    Args:\n        x (torch.Tensor[N,d]): sampling locations\n        task (Union[int,torch.Tensor[T]]): task indices\n        n (Union[int,torch.Tensor[num_tasks]]): number of points at which to evaluate the posterior cubature variance.\n        eval (bool): if `True`, disable gradients, otherwise use `torch.is_grad_enabled()`\n\n    Returns:\n        pvar (torch.Tensor[T,N]): posterior variance\n    \"\"\"\n    if n is None: n = self.n\n    if isinstance(n,int): n = torch.tensor([n],dtype=int,device=self.device)\n    assert isinstance(n,torch.Tensor)\n    assert x.ndim==2 and x.size(1)==self.d, \"x must a torch.Tensor with shape (-1,d)\"\n    inv_log_det_cache = self.get_inv_log_det_cache(n)\n    if eval:\n        incoming_grad_enabled = torch.is_grad_enabled()\n        torch.set_grad_enabled(False)\n    if task is None: task = self.default_task\n    inttask = isinstance(task,int)\n    if inttask: task = torch.tensor([task],dtype=int,device=self.device)\n    if isinstance(task,list): task = torch.tensor(task,dtype=int,device=self.device)\n    assert task.ndim==1 and (task&gt;=0).all() and (task&lt;self.num_tasks).all()\n    if self.ptransform=='NONE':\n        kmat_new = torch.cat([self.kernel(task[l0],task[l0],x,x,*self.derivatives_cross[task[l0]][task[l0]],self.derivatives_coeffs_cross[task[l0]][task[l0]])[...,None,:] for l0 in range(len(task))],dim=-2)\n        kmat = torch.cat([torch.cat([self.kernel(task[l0],l1,x[:,None,:],self.get_xb(l1,n[l1])[None,:,:],*self.derivatives_cross[task[l0]][l1],self.derivatives_coeffs_cross[task[l0]][l1]) for l1 in range(self.num_tasks)],dim=-1)[...,None,:,:] for l0 in range(len(task))],dim=-3)\n        kmat_perm = torch.permute(kmat,[-3,-2]+[i for i in range(kmat.ndim-3)]+[-1])\n        t_perm = inv_log_det_cache.gram_matrix_solve(self,kmat_perm)\n        t = torch.permute(t_perm,[2+i for i in range(t_perm.ndim-3)]+[0,1,-1])\n        diag = kmat_new-(t*kmat).sum(-1)\n    elif self.ptransform=='BAKER':\n        kmat_new_1 = torch.cat([self.kernel(task[l0],task[l0],x/2,x/2,*self.derivatives_cross[task[l0]][task[l0]],self.derivatives_coeffs_cross[task[l0]][task[l0]])[...,None,:] for l0 in range(len(task))],dim=-2)\n        kmat_1 = torch.cat([torch.cat([self.kernel(task[l0],l1,x[:,None,:]/2,self.get_xb(l1,n[l1])[None,:,:],*self.derivatives_cross[task[l0]][l1],self.derivatives_coeffs_cross[task[l0]][l1]) for l1 in range(self.num_tasks)],dim=-1)[...,None,:,:] for l0 in range(len(task))],dim=-3)\n        kmat_perm_1 = torch.permute(kmat_1,[-3,-2]+[i for i in range(kmat_1.ndim-3)]+[-1])\n        t_perm_1 = inv_log_det_cache.gram_matrix_solve(self,kmat_perm_1)\n        t_1 = torch.permute(t_perm_1,[2+i for i in range(t_perm_1.ndim-3)]+[0,1,-1])\n        diag_1 = kmat_new_1-(t_1*kmat_1).sum(-1)\n        kmat_new_2 = torch.cat([self.kernel(task[l0],task[l0],1-x/2,1-x/2,*self.derivatives_cross[task[l0]][task[l0]],self.derivatives_coeffs_cross[task[l0]][task[l0]])[...,None,:] for l0 in range(len(task))],dim=-2)\n        kmat_2 = torch.cat([torch.cat([self.kernel(task[l0],l1,1-x[:,None,:]/2,self.get_xb(l1,n[l1])[None,:,:],*self.derivatives_cross[task[l0]][l1],self.derivatives_coeffs_cross[task[l0]][l1]) for l1 in range(self.num_tasks)],dim=-1)[...,None,:,:] for l0 in range(len(task))],dim=-3)\n        kmat_perm_2 = torch.permute(kmat_2,[-3,-2]+[i for i in range(kmat_2.ndim-3)]+[-1])\n        t_perm_2 = inv_log_det_cache.gram_matrix_solve(self,kmat_perm_2)\n        t_2 = torch.permute(t_perm_2,[2+i for i in range(t_perm_2.ndim-3)]+[0,1,-1])\n        diag_2 = kmat_new_2-(t_2*kmat_2).sum(-1)\n        kmat_new_3 = torch.cat([self.kernel(task[l0],task[l0],x/2,1-x/2,*self.derivatives_cross[task[l0]][task[l0]],self.derivatives_coeffs_cross[task[l0]][task[l0]])[...,None,:] for l0 in range(len(task))],dim=-2)\n        kmat_3 = torch.cat([torch.cat([self.kernel(task[l0],l1,x[:,None,:]/2,self.get_xb(l1,n[l1])[None,:,:],*self.derivatives_cross[task[l0]][l1],self.derivatives_coeffs_cross[task[l0]][l1]) for l1 in range(self.num_tasks)],dim=-1)[...,None,:,:] for l0 in range(len(task))],dim=-3)\n        kmat_p3 = torch.cat([torch.cat([self.kernel(task[l0],l1,1-x[:,None,:]/2,self.get_xb(l1,n[l1])[None,:,:],*self.derivatives_cross[task[l0]][l1],self.derivatives_coeffs_cross[task[l0]][l1]) for l1 in range(self.num_tasks)],dim=-1)[...,None,:,:] for l0 in range(len(task))],dim=-3)\n        kmat_perm_3 = torch.permute(kmat_p3,[-3,-2]+[i for i in range(kmat_p3.ndim-3)]+[-1])\n        t_perm_3 = inv_log_det_cache.gram_matrix_solve(self,kmat_perm_3)\n        t_3 = torch.permute(t_perm_3,[2+i for i in range(t_perm_3.ndim-3)]+[0,1,-1])\n        diag_3 = kmat_new_3-(t_3*kmat_3).sum(-1)\n        diag = 1/4*(diag_1+diag_2+2*diag_3)\n    else:\n        raise Exception(\"invalid ptransform = %s\"%self.ptransform)\n    diag[diag&lt;0] = 0 \n    if eval:\n        torch.set_grad_enabled(incoming_grad_enabled)\n    return diag[...,0,:] if inttask else diag\n</code></pre>"},{"location":"api/#fastgps.abstract_gp.AbstractGP.post_cov","title":"post_cov","text":"<pre><code>post_cov(x0, x1, task0=None, task1=None, n=None, eval=True)\n</code></pre> <p>Posterior covariance. </p> <p>Parameters:</p> Name Type Description Default <code>x0</code> <code>Tensor[N, d]</code> <p>left sampling locations</p> required <code>x1</code> <code>Tensor[M, d]</code> <p>right sampling locations</p> required <code>task0</code> <code>Union[int, Tensor[T1]]</code> <p>left task index</p> <code>None</code> <code>task1</code> <code>Union[int, Tensor[T2]]</code> <p>right task index</p> <code>None</code> <code>n</code> <code>Union[int, Tensor[num_tasks]]</code> <p>number of points at which to evaluate the posterior cubature variance.</p> <code>None</code> <code>eval</code> <code>bool</code> <p>if <code>True</code>, disable gradients, otherwise use <code>torch.is_grad_enabled()</code></p> <code>True</code> <p>Returns:</p> Name Type Description <code>pcov</code> <code>Tensor[T1, T2, N, M]</code> <p>posterior covariance matrix</p> Source code in <code>fastgps/abstract_gp.py</code> <pre><code>def post_cov(self, x0:torch.Tensor, x1:torch.Tensor, task0:Union[int,torch.Tensor]=None, task1:Union[int,torch.Tensor]=None, n:Union[int,torch.Tensor]=None, eval:bool=True):\n    \"\"\"\n    Posterior covariance. \n\n    Args:\n        x0 (torch.Tensor[N,d]): left sampling locations\n        x1 (torch.Tensor[M,d]): right sampling locations\n        task0 (Union[int,torch.Tensor[T1]]): left task index\n        task1 (Union[int,torch.Tensor[T2]]): right task index\n        n (Union[int,torch.Tensor[num_tasks]]): number of points at which to evaluate the posterior cubature variance.\n        eval (bool): if `True`, disable gradients, otherwise use `torch.is_grad_enabled()`\n\n    Returns:\n        pcov (torch.Tensor[T1,T2,N,M]): posterior covariance matrix\n    \"\"\"\n    if n is None: n = self.n\n    if isinstance(n,int): n = torch.tensor([n],dtype=int,device=self.device)\n    assert isinstance(n,torch.Tensor)\n    assert x0.ndim==2 and x0.size(1)==self.d, \"x must a torch.Tensor with shape (-1,d)\"\n    assert x1.ndim==2 and x1.size(1)==self.d, \"z must a torch.Tensor with shape (-1,d)\"\n    inv_log_det_cache = self.get_inv_log_det_cache(n)\n    if eval:\n        incoming_grad_enabled = torch.is_grad_enabled()\n        torch.set_grad_enabled(False)\n    if task0 is None: task0 = self.default_task\n    inttask0 = isinstance(task0,int)\n    if inttask0: task0 = torch.tensor([task0],dtype=int,device=self.device)\n    if isinstance(task0,list): task0 = torch.tensor(task0,dtype=int,device=self.device)\n    assert task0.ndim==1 and (task0&gt;=0).all() and (task0&lt;self.num_tasks).all()\n    if task1 is None: task1 = self.default_task\n    inttask1 = isinstance(task1,int)\n    if inttask1: task1 = torch.tensor([task1],dtype=int,device=self.device)\n    if isinstance(task1,list): task1 = torch.tensor(task1,dtype=int,device=self.device)\n    assert task1.ndim==1 and (task1&gt;=0).all() and (task1&lt;self.num_tasks).all()\n    if self.ptransform==\"NONE\":\n        equal = torch.equal(x0,x1) and torch.equal(task0,task1)\n        kmat_new = torch.cat([torch.cat([self.kernel(task0[l0],task1[l1],x0[:,None,:],x1[None,:,:],*self.derivatives_cross[task0[l0]][task1[l1]],self.derivatives_coeffs_cross[task0[l0]][task1[l1]])[...,None,None,:,:] for l1 in range(len(task1))],dim=-3) for l0 in range(len(task0))],dim=-4)\n        kmat1 = torch.cat([torch.cat([self.kernel(task0[l0],l1,x0[:,None,:],self.get_xb(l1,n[l1])[None,:,:],*self.derivatives_cross[task0[l0]][l1],self.derivatives_coeffs_cross[task0[l0]][l1]) for l1 in range(self.num_tasks)],dim=-1)[...,None,:,:] for l0 in range(len(task0))],dim=-3)\n        kmat2 = kmat1 if equal else torch.cat([torch.cat([self.kernel(task1[l0],l1,x1[:,None,:],self.get_xb(l1,n[l1])[None,:,:],*self.derivatives_cross[task1[l0]][l1],self.derivatives_coeffs_cross[task1[l0]][l1]) for l1 in range(self.num_tasks)],dim=-1)[...,None,:,:] for l0 in range(len(task1))],dim=-3)\n        kmat2_perm = torch.permute(kmat2,[-3,-2]+[i for i in range(kmat2.ndim-3)]+[-1])\n        t_perm = inv_log_det_cache.gram_matrix_solve(self,kmat2_perm)\n        t = torch.permute(t_perm,[2+i for i in range(t_perm.ndim-3)]+[0,1,-1])\n        kmat = kmat_new-(kmat1[...,:,None,:,None,:]*t[...,None,:,None,:,:]).sum(-1)\n    elif self.ptransform==\"BAKER\":\n        kmat = 0\n        for x0i,x1i in [[x0/2,x1/2],[x0/2,1-x1/2],[1-x0/2,x1/2],[1-x0/2,1-x1/2]]:\n            equal = torch.equal(x0i,x1i) and torch.equal(task0,task1)\n            kmat_new = torch.cat([torch.cat([self.kernel(task0[l0],task1[l1],x0i[:,None,:],x1i[None,:,:],*self.derivatives_cross[task0[l0]][task1[l1]],self.derivatives_coeffs_cross[task0[l0]][task1[l1]])[...,None,None,:,:] for l1 in range(len(task1))],dim=-3) for l0 in range(len(task0))],dim=-4)\n            kmat1 = torch.cat([torch.cat([self.kernel(task0[l0],l1,x0i[:,None,:],self.get_xb(l1,n[l1])[None,:,:],*self.derivatives_cross[task0[l0]][l1],self.derivatives_coeffs_cross[task0[l0]][l1]) for l1 in range(self.num_tasks)],dim=-1)[...,None,:,:] for l0 in range(len(task0))],dim=-3)\n            kmat2 = kmat1 if equal else torch.cat([torch.cat([self.kernel(task1[l0],l1,x1i[:,None,:],self.get_xb(l1,n[l1])[None,:,:],*self.derivatives_cross[task1[l0]][l1],self.derivatives_coeffs_cross[task1[l0]][l1]) for l1 in range(self.num_tasks)],dim=-1)[...,None,:,:] for l0 in range(len(task1))],dim=-3)\n            kmat2_perm = torch.permute(kmat2,[-3,-2]+[i for i in range(kmat2.ndim-3)]+[-1])\n            t_perm = inv_log_det_cache.gram_matrix_solve(self,kmat2_perm)\n            t = torch.permute(t_perm,[2+i for i in range(t_perm.ndim-3)]+[0,1,-1])\n            kmat += kmat_new-(kmat1[...,:,None,:,None,:]*t[...,None,:,None,:,:]).sum(-1)\n        kmat = kmat/4\n    else:\n        raise Exception(\"invalid ptransform = %s\"%self.ptransform)\n    if equal:\n        tmesh,nmesh = torch.meshgrid(torch.arange(kmat.size(0),device=self.device),torch.arange(x0.size(0),device=x0.device),indexing=\"ij\")            \n        tidx,nidx = tmesh.ravel(),nmesh.ravel()\n        diag = kmat[...,tidx,tidx,nidx,nidx]\n        diag[diag&lt;0] = 0 \n        kmat[...,tidx,tidx,nidx,nidx] = diag \n    if eval:\n        torch.set_grad_enabled(incoming_grad_enabled)\n    if inttask0 and inttask1:\n        return kmat[...,0,0,:,:]\n    elif inttask0 and not inttask1:\n        return kmat[...,0,:,:,:]\n    elif not inttask0 and inttask1:\n        return kmat[...,:,0,:,:]\n    else: # not inttask0 and not inttask1\n        return kmat\n</code></pre>"},{"location":"api/#fastgps.abstract_gp.AbstractGP.post_error","title":"post_error","text":"<pre><code>post_error(x, task=None, n=None, confidence=0.99, eval=True)\n</code></pre> <p>Posterior error. </p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor[N, d]</code> <p>sampling locations</p> required <code>task</code> <code>Union[int, Tensor[T]]</code> <p>task indices</p> <code>None</code> <code>n</code> <code>Union[int, Tensor[num_tasks]]</code> <p>number of points at which to evaluate the posterior cubature variance.</p> <code>None</code> <code>eval</code> <code>bool</code> <p>if <code>True</code>, disable gradients, otherwise use <code>torch.is_grad_enabled()</code></p> <code>True</code> <code>confidence</code> <code>float</code> <p>confidence level in \\((0,1)\\) for the credible interval</p> <code>0.99</code> <p>Returns:</p> Name Type Description <code>cvar</code> <code>Tensor[T]</code> <p>posterior variance</p> <code>quantile</code> <code>float64</code> <pre><code>scipy.stats.norm.ppf(1-(1-confidence)/2)\n</code></pre> <code>perror</code> <code>Tensor[T]</code> <p>posterior error</p> Source code in <code>fastgps/abstract_gp.py</code> <pre><code>def post_error(self, x:torch.Tensor, task:Union[int,torch.Tensor]=None, n:Union[int,torch.Tensor]=None, confidence:float=0.99, eval:bool=True):\n    \"\"\"\n    Posterior error. \n\n    Args:\n        x (torch.Tensor[N,d]): sampling locations\n        task (Union[int,torch.Tensor[T]]): task indices\n        n (Union[int,torch.Tensor[num_tasks]]): number of points at which to evaluate the posterior cubature variance.\n        eval (bool): if `True`, disable gradients, otherwise use `torch.is_grad_enabled()`\n        confidence (float): confidence level in $(0,1)$ for the credible interval\n\n    Returns:\n        cvar (torch.Tensor[T]): posterior variance\n        quantile (np.float64):\n            ```python\n            scipy.stats.norm.ppf(1-(1-confidence)/2)\n            ```\n        perror (torch.Tensor[T]): posterior error\n    \"\"\"\n    assert np.isscalar(confidence) and 0&lt;confidence&lt;1, \"confidence must be between 0 and 1\"\n    q = scipy.stats.norm.ppf(1-(1-confidence)/2)\n    pvar = self.post_var(x,task=task,n=n,eval=eval,)\n    pstd = torch.sqrt(pvar)\n    perror = q*pstd\n    return pvar,q,perror\n</code></pre>"},{"location":"api/#fastgps.abstract_gp.AbstractGP.post_ci","title":"post_ci","text":"<pre><code>post_ci(x, task=None, confidence=0.99, eval=True)\n</code></pre> <p>Posterior credible interval.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor[N, d]</code> <p>sampling locations</p> required <code>task</code> <code>Union[int, Tensor[T]]</code> <p>task indices</p> <code>None</code> <code>confidence</code> <code>float</code> <p>confidence level in \\((0,1)\\) for the credible interval</p> <code>0.99</code> <code>eval</code> <code>bool</code> <p>if <code>True</code>, disable gradients, otherwise use <code>torch.is_grad_enabled()</code></p> <code>True</code> <p>Returns:</p> Name Type Description <code>pmean</code> <code>Tensor[..., T, N]</code> <p>posterior mean</p> <code>pvar</code> <code>Tensor[T, N]</code> <p>posterior variance </p> <code>quantile</code> <code>float64</code> <pre><code>scipy.stats.norm.ppf(1-(1-confidence)/2)\n</code></pre> <code>pci_low</code> <code>Tensor[..., T, N]</code> <p>posterior credible interval lower bound</p> <code>pci_high</code> <code>Tensor[..., T, N]</code> <p>posterior credible interval upper bound</p> Source code in <code>fastgps/abstract_gp.py</code> <pre><code>def post_ci(self, x:torch.Tensor, task:Union[int,torch.Tensor]=None, confidence:float=0.99, eval:bool=True):\n    \"\"\"\n    Posterior credible interval.\n\n    Args:\n        x (torch.Tensor[N,d]): sampling locations\n        task (Union[int,torch.Tensor[T]]): task indices\n        confidence (float): confidence level in $(0,1)$ for the credible interval\n        eval (bool): if `True`, disable gradients, otherwise use `torch.is_grad_enabled()`\n\n    Returns:\n        pmean (torch.Tensor[...,T,N]): posterior mean\n        pvar (torch.Tensor[T,N]): posterior variance \n        quantile (np.float64):\n            ```python\n            scipy.stats.norm.ppf(1-(1-confidence)/2)\n            ```\n        pci_low (torch.Tensor[...,T,N]): posterior credible interval lower bound\n        pci_high (torch.Tensor[...,T,N]): posterior credible interval upper bound\n    \"\"\"\n    assert np.isscalar(confidence) and 0&lt;confidence&lt;1, \"confidence must be between 0 and 1\"\n    q = scipy.stats.norm.ppf(1-(1-confidence)/2)\n    pmean = self.post_mean(x,task=task,eval=eval)\n    pvar,q,perror = self.post_error(x,task=task,confidence=confidence)\n    pci_low = pmean-q*perror \n    pci_high = pmean+q*perror\n    return pmean,pvar,q,pci_low,pci_high\n</code></pre>"},{"location":"api/#fastgps.abstract_gp.AbstractGP.post_cubature_mean","title":"post_cubature_mean","text":"<pre><code>post_cubature_mean(task=None, eval=True)\n</code></pre> <p>Posterior cubature mean. </p> <p>Parameters:</p> Name Type Description Default <code>eval</code> <code>bool</code> <p>if <code>True</code>, disable gradients, otherwise use <code>torch.is_grad_enabled()</code></p> <code>True</code> <code>task</code> <code>Union[int, Tensor[T]]</code> <p>task indices</p> <code>None</code> <p>Returns:</p> Name Type Description <code>pcmean</code> <code>Tensor[..., T]</code> <p>posterior cubature mean</p> Source code in <code>fastgps/abstract_gp.py</code> <pre><code>def post_cubature_mean(self, task:Union[int,torch.Tensor]=None, eval:bool=True):\n    \"\"\"\n    Posterior cubature mean. \n\n    Args:\n        eval (bool): if `True`, disable gradients, otherwise use `torch.is_grad_enabled()`\n        task (Union[int,torch.Tensor[T]]): task indices\n\n    Returns:\n        pcmean (torch.Tensor[...,T]): posterior cubature mean\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"api/#fastgps.abstract_gp.AbstractGP.post_cubature_var","title":"post_cubature_var","text":"<pre><code>post_cubature_var(task=None, n=None, eval=True)\n</code></pre> <p>Posterior cubature variance. </p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>Union[int, Tensor[T]]</code> <p>task indices</p> <code>None</code> <code>n</code> <code>Union[int, Tensor[num_tasks]]</code> <p>number of points at which to evaluate the posterior cubature variance.</p> <code>None</code> <code>eval</code> <code>bool</code> <p>if <code>True</code>, disable gradients, otherwise use <code>torch.is_grad_enabled()</code></p> <code>True</code> <p>Returns:</p> Name Type Description <code>pcvar</code> <code>Tensor[T]</code> <p>posterior cubature variance</p> Source code in <code>fastgps/abstract_gp.py</code> <pre><code>def post_cubature_var(self, task:Union[int,torch.Tensor]=None, n:Union[int,torch.Tensor]=None, eval:bool=True):\n    \"\"\"\n    Posterior cubature variance. \n\n    Args:\n        task (Union[int,torch.Tensor[T]]): task indices\n        n (Union[int,torch.Tensor[num_tasks]]): number of points at which to evaluate the posterior cubature variance.\n        eval (bool): if `True`, disable gradients, otherwise use `torch.is_grad_enabled()`\n\n    Returns:\n        pcvar (torch.Tensor[T]): posterior cubature variance\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"api/#fastgps.abstract_gp.AbstractGP.post_cubature_cov","title":"post_cubature_cov","text":"<pre><code>post_cubature_cov(task0=None, task1=None, n=None, eval=True)\n</code></pre> <p>Posterior cubature covariance. </p> <p>Parameters:</p> Name Type Description Default <code>task0</code> <code>Union[int, Tensor[T1]]</code> <p>task indices</p> <code>None</code> <code>task1</code> <code>Union[int, Tensor[T2]]</code> <p>task indices</p> <code>None</code> <code>n</code> <code>Union[int, Tensor[num_tasks]]</code> <p>number of points at which to evaluate the posterior cubature covariance.</p> <code>None</code> <code>eval</code> <code>bool</code> <p>if <code>True</code>, disable gradients, otherwise use <code>torch.is_grad_enabled()</code></p> <code>True</code> <p>Returns:</p> Name Type Description <code>pcvar</code> <code>Tensor[T1, T2]</code> <p>posterior cubature covariance</p> Source code in <code>fastgps/abstract_gp.py</code> <pre><code>def post_cubature_cov(self, task0:Union[int,torch.Tensor]=None, task1:Union[int,torch.Tensor]=None, n:Union[int,torch.Tensor]=None, eval:bool=True):\n    \"\"\"\n    Posterior cubature covariance. \n\n    Args:\n        task0 (Union[int,torch.Tensor[T1]]): task indices\n        task1 (Union[int,torch.Tensor[T2]]): task indices\n        n (Union[int,torch.Tensor[num_tasks]]): number of points at which to evaluate the posterior cubature covariance.\n        eval (bool): if `True`, disable gradients, otherwise use `torch.is_grad_enabled()`\n\n    Returns:\n        pcvar (torch.Tensor[T1,T2]): posterior cubature covariance\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"api/#fastgps.abstract_gp.AbstractGP.post_cubature_error","title":"post_cubature_error","text":"<pre><code>post_cubature_error(task=None, n=None, confidence=0.99, eval=True)\n</code></pre> <p>Posterior cubature error. </p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>Union[int, Tensor[T]]</code> <p>task indices</p> <code>None</code> <code>n</code> <code>Union[int, Tensor[num_tasks]]</code> <p>number of points at which to evaluate the posterior cubature variance.</p> <code>None</code> <code>eval</code> <code>bool</code> <p>if <code>True</code>, disable gradients, otherwise use <code>torch.is_grad_enabled()</code></p> <code>True</code> <code>confidence</code> <code>float</code> <p>confidence level in \\((0,1)\\) for the credible interval</p> <code>0.99</code> <p>Returns:</p> Name Type Description <code>pcvar</code> <code>Tensor[T]</code> <p>posterior cubature variance</p> <code>quantile</code> <code>float64</code> <pre><code>scipy.stats.norm.ppf(1-(1-confidence)/2)\n</code></pre> <code>pcerror</code> <code>Tensor[T]</code> <p>posterior cubature error</p> Source code in <code>fastgps/abstract_gp.py</code> <pre><code>def post_cubature_error(self, task:Union[int,torch.Tensor]=None, n:Union[int,torch.Tensor]=None, confidence:float=0.99, eval:bool=True):\n    \"\"\"\n    Posterior cubature error. \n\n    Args:\n        task (Union[int,torch.Tensor[T]]): task indices\n        n (Union[int,torch.Tensor[num_tasks]]): number of points at which to evaluate the posterior cubature variance.\n        eval (bool): if `True`, disable gradients, otherwise use `torch.is_grad_enabled()`\n        confidence (float): confidence level in $(0,1)$ for the credible interval\n\n    Returns:\n        pcvar (torch.Tensor[T]): posterior cubature variance\n        quantile (np.float64):\n            ```python\n            scipy.stats.norm.ppf(1-(1-confidence)/2)\n            ```\n        pcerror (torch.Tensor[T]): posterior cubature error\n    \"\"\"\n    assert np.isscalar(confidence) and 0&lt;confidence&lt;1, \"confidence must be between 0 and 1\"\n    q = scipy.stats.norm.ppf(1-(1-confidence)/2)\n    pcvar = self.post_cubature_var(task=task,n=n,eval=eval)\n    pcstd = torch.sqrt(pcvar)\n    pcerror = q*pcstd\n    return pcvar,q,pcerror\n</code></pre>"},{"location":"api/#fastgps.abstract_gp.AbstractGP.post_cubature_ci","title":"post_cubature_ci","text":"<pre><code>post_cubature_ci(task=None, confidence=0.99, eval=True)\n</code></pre> <p>Posterior cubature credible.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>Union[int, Tensor[T]]</code> <p>task indices</p> <code>None</code> <code>confidence</code> <code>float</code> <p>confidence level in \\((0,1)\\) for the credible interval</p> <code>0.99</code> <code>eval</code> <code>bool</code> <p>if <code>True</code>, disable gradients, otherwise use <code>torch.is_grad_enabled()</code></p> <code>True</code> <p>Returns:</p> Name Type Description <code>pcmean</code> <code>Tensor[..., T]</code> <p>posterior cubature mean</p> <code>pcvar</code> <code>Tensor[T]</code> <p>posterior cubature variance</p> <code>quantile</code> <code>float64</code> <pre><code>scipy.stats.norm.ppf(1-(1-confidence)/2)\n</code></pre> <code>pcci_low</code> <code>Tensor[..., T]</code> <p>posterior cubature credible interval lower bound</p> <code>pcci_high</code> <code>Tensor[..., T]</code> <p>posterior cubature credible interval upper bound</p> Source code in <code>fastgps/abstract_gp.py</code> <pre><code>def post_cubature_ci(self, task:Union[int,torch.Tensor]=None, confidence:float=0.99, eval:bool=True):\n    \"\"\"\n    Posterior cubature credible.\n\n    Args:\n        task (Union[int,torch.Tensor[T]]): task indices\n        confidence (float): confidence level in $(0,1)$ for the credible interval\n        eval (bool): if `True`, disable gradients, otherwise use `torch.is_grad_enabled()`\n\n    Returns:\n        pcmean (torch.Tensor[...,T]): posterior cubature mean\n        pcvar (torch.Tensor[T]): posterior cubature variance\n        quantile (np.float64):\n            ```python\n            scipy.stats.norm.ppf(1-(1-confidence)/2)\n            ```\n        pcci_low (torch.Tensor[...,T]): posterior cubature credible interval lower bound\n        pcci_high (torch.Tensor[...,T]): posterior cubature credible interval upper bound\n    \"\"\"\n    assert np.isscalar(confidence) and 0&lt;confidence&lt;1, \"confidence must be between 0 and 1\"\n    q = scipy.stats.norm.ppf(1-(1-confidence)/2)\n    pcmean = self.post_cubature_mean(task=task,eval=eval) \n    pcvar,q,pcerror = self.post_cubature_error(task=task,confidence=confidence,eval=eval)\n    pcci_low = pcmean-pcerror\n    pcci_high = pcmean+pcerror\n    return pcmean,pcvar,q,pcci_low,pcci_high\n</code></pre>"},{"location":"api/#fastgps.standard_gp.StandardGP","title":"StandardGP","text":"<pre><code>StandardGP(kernel, seqs, noise=0.0001, tfs_noise=(qp.util.transforms.tf_exp_eps_inv, qp.util.transforms.tf_exp_eps), requires_grad_noise=False, shape_noise=torch.Size([1]), derivatives=None, derivatives_coeffs=None, adaptive_nugget=True, data=None, ptransform=None)\n</code></pre> <p>               Bases: <code>AbstractGP</code></p> <p>Standard Gaussian process regression</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; device = \"cpu\"\n&gt;&gt;&gt; if device!=\"mps\":\n...     torch.set_default_dtype(torch.float64)\n</code></pre> <pre><code>&gt;&gt;&gt; def f_ackley(x, a=20, b=0.2, c=2*np.pi, scaling=32.768):\n...     # https://www.sfu.ca/~ssurjano/ackley.html\n...     assert x.ndim==2\n...     x = 2*scaling*x-scaling\n...     t1 = a*torch.exp(-b*torch.sqrt(torch.mean(x**2,1)))\n...     t2 = torch.exp(torch.mean(torch.cos(c*x),1))\n...     t3 = a+np.exp(1)\n...     y = -t1-t2+t3\n...     return y\n</code></pre> <pre><code>&gt;&gt;&gt; n = 2**6\n&gt;&gt;&gt; d = 2\n&gt;&gt;&gt; sgp = StandardGP(\n...     qp.KernelSquaredExponential(d,torchify=True,device=device),\n...     qp.DigitalNetB2(dimension=d,seed=7))\n&gt;&gt;&gt; x_next = sgp.get_x_next(n)\n&gt;&gt;&gt; y_next = f_ackley(x_next)\n&gt;&gt;&gt; sgp.add_y_next(y_next)\n</code></pre> <pre><code>&gt;&gt;&gt; rng = torch.Generator().manual_seed(17)\n&gt;&gt;&gt; x = torch.rand((2**7,d),generator=rng).to(device)\n&gt;&gt;&gt; y = f_ackley(x)\n</code></pre> <pre><code>&gt;&gt;&gt; pmean = sgp.post_mean(x)\n</code></pre> <pre><code>&gt;&gt;&gt; pmean.shape\ntorch.Size([128])\n&gt;&gt;&gt; torch.linalg.norm(y-pmean)/torch.linalg.norm(y)\ntensor(0.0817)\n&gt;&gt;&gt; torch.linalg.norm(sgp.post_mean(sgp.x)-sgp.y)/torch.linalg.norm(y)\ntensor(0.0402)\n</code></pre> <pre><code>&gt;&gt;&gt; data = sgp.fit(verbose=0)\n&gt;&gt;&gt; list(data.keys())\n[]\n</code></pre> <pre><code>&gt;&gt;&gt; torch.linalg.norm(y-sgp.post_mean(x))/torch.linalg.norm(y)\ntensor(0.0472)\n&gt;&gt;&gt; z = torch.rand((2**8,d),generator=rng).to(device)\n&gt;&gt;&gt; pcov = sgp.post_cov(x,z)\n&gt;&gt;&gt; pcov.shape\ntorch.Size([128, 256])\n</code></pre> <pre><code>&gt;&gt;&gt; pcov = sgp.post_cov(x,x)\n&gt;&gt;&gt; pcov.shape\ntorch.Size([128, 128])\n&gt;&gt;&gt; (pcov.diagonal()&gt;=0).all()\ntensor(True)\n</code></pre> <pre><code>&gt;&gt;&gt; pvar = sgp.post_var(x)\n&gt;&gt;&gt; pvar.shape\ntorch.Size([128])\n&gt;&gt;&gt; torch.allclose(pcov.diagonal(),pvar)\nTrue\n</code></pre> <pre><code>&gt;&gt;&gt; pmean,pstd,q,ci_low,ci_high = sgp.post_ci(x,confidence=0.99)\n&gt;&gt;&gt; ci_low.shape\ntorch.Size([128])\n&gt;&gt;&gt; ci_high.shape\ntorch.Size([128])\n</code></pre> <pre><code>&gt;&gt;&gt; sgp.post_cubature_mean()\ntensor(20.3665)\n&gt;&gt;&gt; sgp.post_cubature_var()\ntensor(0.0015)\n</code></pre> <pre><code>&gt;&gt;&gt; pcmean,pcvar,q,pcci_low,pcci_high = sgp.post_cubature_ci(confidence=0.99)\n&gt;&gt;&gt; pcci_low\ntensor(20.2684)\n&gt;&gt;&gt; pcci_high\ntensor(20.4647)\n</code></pre> <pre><code>&gt;&gt;&gt; pcov_future = sgp.post_cov(x,z,n=2*n)\n&gt;&gt;&gt; pvar_future = sgp.post_var(x,n=2*n)\n&gt;&gt;&gt; pcvar_future = sgp.post_cubature_var(n=2*n)\n</code></pre> <pre><code>&gt;&gt;&gt; x_next = sgp.get_x_next(2*n)\n&gt;&gt;&gt; y_next = f_ackley(x_next)\n&gt;&gt;&gt; sgp.add_y_next(y_next)\n&gt;&gt;&gt; torch.linalg.norm(y-sgp.post_mean(x))/torch.linalg.norm(y)\ntensor(0.0589)\n</code></pre> <pre><code>&gt;&gt;&gt; torch.allclose(sgp.post_cov(x,z),pcov_future)\nTrue\n&gt;&gt;&gt; torch.allclose(sgp.post_var(x),pvar_future)\nTrue\n&gt;&gt;&gt; torch.allclose(sgp.post_cubature_var(),pcvar_future)\nTrue\n</code></pre> <pre><code>&gt;&gt;&gt; data = sgp.fit(verbose=False)\n&gt;&gt;&gt; torch.linalg.norm(y-sgp.post_mean(x))/torch.linalg.norm(y)\ntensor(0.0440)\n</code></pre> <pre><code>&gt;&gt;&gt; x_next = sgp.get_x_next(4*n)\n&gt;&gt;&gt; y_next = f_ackley(x_next)\n&gt;&gt;&gt; sgp.add_y_next(y_next)\n&gt;&gt;&gt; torch.linalg.norm(y-sgp.post_mean(x))/torch.linalg.norm(y)\ntensor(0.0923)\n</code></pre> <pre><code>&gt;&gt;&gt; data = sgp.fit(verbose=False)\n&gt;&gt;&gt; torch.linalg.norm(y-sgp.post_mean(x))/torch.linalg.norm(y)\ntensor(0.0412)\n</code></pre> <pre><code>&gt;&gt;&gt; pcov_16n = sgp.post_cov(x,z,n=16*n)\n&gt;&gt;&gt; pvar_16n = sgp.post_var(x,n=16*n)\n&gt;&gt;&gt; pcvar_16n = sgp.post_cubature_var(n=16*n)\n&gt;&gt;&gt; x_next = sgp.get_x_next(16*n)\n&gt;&gt;&gt; y_next = f_ackley(x_next)\n&gt;&gt;&gt; sgp.add_y_next(y_next)\n&gt;&gt;&gt; torch.allclose(sgp.post_cov(x,z),pcov_16n)\nTrue\n&gt;&gt;&gt; torch.allclose(sgp.post_var(x),pvar_16n)\nTrue\n&gt;&gt;&gt; torch.allclose(sgp.post_cubature_var(),pcvar_16n)\nTrue\n</code></pre> <p>Different loss metrics for fitting </p> <pre><code>&gt;&gt;&gt; n = 2**6\n&gt;&gt;&gt; d = 3\n&gt;&gt;&gt; sgp = StandardGP(\n...     qp.KernelMatern52(d,torchify=True,device=device),\n...     qp.DigitalNetB2(dimension=d,seed=7))\n&gt;&gt;&gt; x_next = sgp.get_x_next(n)\n&gt;&gt;&gt; y_next = torch.stack([torch.sin(x_next).sum(-1),torch.cos(x_next).sum(-1)],axis=0)\n&gt;&gt;&gt; sgp.add_y_next(y_next)\n&gt;&gt;&gt; data = sgp.fit(loss_metric=\"MLL\",iterations=5,verbose=0)\n&gt;&gt;&gt; data = sgp.fit(loss_metric=\"CV\",iterations=5,verbose=0,cv_weights=1/torch.arange(1,2*n+1,device=device).reshape((2,n)))\n&gt;&gt;&gt; data = sgp.fit(loss_metric=\"CV\",iterations=5,verbose=0,cv_weights=\"L2R\")\n&gt;&gt;&gt; data = sgp.fit(loss_metric=\"GCV\",iterations=5,verbose=0)\n</code></pre> <p>Data Driven</p> <pre><code>&gt;&gt;&gt; x = [\n...     torch.rand((3,1),generator=rng).to(device),\n...     torch.rand((12,1),generator=rng).to(device),\n... ]\n&gt;&gt;&gt; y = [\n...     torch.stack([torch.sin(2*np.pi*x[0][:,0]),torch.cos(2*np.pi*x[0][:,0]),torch.acos(x[0][:,0])],dim=0).to(device),\n...     torch.stack([4*torch.sin(2*np.pi*x[1][:,0]),4*torch.cos(2*np.pi*x[1][:,0]),4*torch.acos(x[1][:,0])],dim=0).to(device),\n... ]\n&gt;&gt;&gt; sgp = StandardGP(\n...     qp.KernelMultiTask(\n...         qp.KernelGaussian(d=1,torchify=True,device=device),\n...         num_tasks = 2,\n...     ),\n...     seqs={\"x\":x,\"y\":y},\n...     noise = 1e-3,\n... )\n&gt;&gt;&gt; data = sgp.fit(verbose=0,iterations=10)\n&gt;&gt;&gt; xticks = torch.linspace(0,1,101,device=device) \n&gt;&gt;&gt; pmean,pvar,q,pci_low,pci_high = sgp.post_ci(xticks[:,None])\n&gt;&gt;&gt; pcmean,pcvar,q,pcci_low,pcci_high =  sgp.post_cubature_ci()\n</code></pre> <p>Batch Inference </p> <pre><code>&gt;&gt;&gt; d = 4\n&gt;&gt;&gt; n = 2**10\n&gt;&gt;&gt; dnb2 = qp.DigitalNetB2(d,seed=11) \n&gt;&gt;&gt; kernel = qp.KernelGaussian(d,torchify=True,shape_scale=(2,1),shape_lengthscales=(3,2,d))\n&gt;&gt;&gt; fgp = StandardGP(kernel,dnb2) \n&gt;&gt;&gt; x = fgp.get_x_next(n) \n&gt;&gt;&gt; x.shape\ntorch.Size([1024, 4])\n&gt;&gt;&gt; y = (x**torch.arange(6).reshape((3,2))[:,:,None,None]).sum(-1)\n&gt;&gt;&gt; y.shape\ntorch.Size([3, 2, 1024])\n&gt;&gt;&gt; fgp.add_y_next(y) \n&gt;&gt;&gt; data = fgp.fit(verbose=0)\n&gt;&gt;&gt; fgp.post_cubature_mean()\ntensor([[4.0000, 2.0000],\n        [1.3333, 1.0000],\n        [0.8000, 0.6666]])\n&gt;&gt;&gt; pcv = fgp.post_cubature_var()\n&gt;&gt;&gt; pcv.shape\ntorch.Size([3, 2])\n&gt;&gt;&gt; (pcv&lt;5e-6).all()\ntensor(True)\n&gt;&gt;&gt; pcv4 = fgp.post_cubature_var(n=4*n)\n&gt;&gt;&gt; pcv4.shape\ntorch.Size([3, 2])\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>kernel</code> <code>AbstractKernel</code> <p>Kernel object. Set to <code>qp.KernelMultiTask</code> for a multi-task GP.</p> required <code>seqs</code> <code>Union[int,qp.DiscreteDistribution,List]]</code> <p>list of sequence generators. If an int <code>seed</code> is passed in we use  <pre><code>[qp.DigitalNetB2(d,seed=seed_i) for seed_i in np.random.SeedSequence(seed).spawn(num_tasks)]\n</code></pre> See the <code>qp.DiscreteDistribution</code> docs for more info. </p> required <code>noise</code> <code>float</code> <p>positive noise variance i.e. nugget term</p> <code>0.0001</code> <code>tfs_noise</code> <code>Tuple[callable, callable]</code> <p>the first argument transforms to the raw value to be optimized, the second applies the inverse transform</p> <code>(tf_exp_eps_inv, tf_exp_eps)</code> <code>requires_grad_noise</code> <code>bool</code> <p>wheather or not to optimize the noise parameter</p> <code>False</code> <code>shape_noise</code> <code>Size</code> <p>shape of the noise parameter, defaults to <code>torch.Size([1])</code></p> <code>Size([1])</code> <code>derivatives</code> <code>list</code> <p>list of derivative orders e.g. to include a function and its gradient set  <pre><code>derivatives = [torch.zeros(d,dtype=int)]+[ej for ej in torch.eye(d,dtype=int)]\n</code></pre></p> <code>None</code> <code>derivatives_coeffs</code> <code>list</code> <p>list of derivative coefficients where if <code>derivatives[k].shape==(p,d)</code> then we should have <code>derivatives_coeffs[k].shape==(p,)</code></p> <code>None</code> <code>adaptive_nugget</code> <code>bool</code> <p>if True, use the adaptive nugget which modifies noises based on trace ratios.  </p> <code>True</code> <code>data</code> <code>dict</code> <p>dictory of data with keys 'x' and 'y' where data['x'] and data['y'] are both <code>torch.Tensor</code>s or list of <code>torch.Tensor</code>s with lengths equal to the number of tasks</p> <code>None</code> <code>ptransform</code> <code>str</code> <p>periodization transform in <code>[None, 'BAKER']</code> where <code>'BAKER'</code> is also known as the tent transform.</p> <code>None</code> Source code in <code>fastgps/standard_gp.py</code> <pre><code>def __init__(self,\n        kernel:qp.kernel.abstract_kernel.AbstractKernel,\n        seqs:Union[qp.IIDStdUniform,int],\n        noise:float = 1e-4,\n        tfs_noise:Tuple[callable,callable] = (qp.util.transforms.tf_exp_eps_inv,qp.util.transforms.tf_exp_eps),\n        requires_grad_noise:bool = False, \n        shape_noise:torch.Size = torch.Size([1]),\n        derivatives:list = None,\n        derivatives_coeffs:list = None,\n        adaptive_nugget:bool = True,\n        data:dict = None,\n        ptransform:str = None,\n        ):\n    \"\"\"\n    Args:\n        kernel (qp.AbstractKernel): Kernel object. Set to `qp.KernelMultiTask` for a multi-task GP.\n        seqs (Union[int,qp.DiscreteDistribution,List]]): list of sequence generators. If an int `seed` is passed in we use \n            ```python\n            [qp.DigitalNetB2(d,seed=seed_i) for seed_i in np.random.SeedSequence(seed).spawn(num_tasks)]\n            ```\n            See the &lt;a href=\"https://qp.readthedocs.io/en/latest/algorithms.html#discrete-distribution-class\" target=\"_blank\"&gt;`qp.DiscreteDistribution` docs&lt;/a&gt; for more info. \n        noise (float): positive noise variance i.e. nugget term\n        tfs_noise (Tuple[callable,callable]): the first argument transforms to the raw value to be optimized, the second applies the inverse transform\n        requires_grad_noise (bool): wheather or not to optimize the noise parameter\n        shape_noise (torch.Size): shape of the noise parameter, defaults to `torch.Size([1])`\n        derivatives (list): list of derivative orders e.g. to include a function and its gradient set \n            ```python\n            derivatives = [torch.zeros(d,dtype=int)]+[ej for ej in torch.eye(d,dtype=int)]\n            ```\n        derivatives_coeffs (list): list of derivative coefficients where if `derivatives[k].shape==(p,d)` then we should have `derivatives_coeffs[k].shape==(p,)`\n        adaptive_nugget (bool): if True, use the adaptive nugget which modifies noises based on trace ratios.  \n        data (dict): dictory of data with keys 'x' and 'y' where data['x'] and data['y'] are both `torch.Tensor`s or list of `torch.Tensor`s with lengths equal to the number of tasks\n        ptransform (str): periodization transform in `[None, 'BAKER']` where `'BAKER'` is also known as the tent transform.\n    \"\"\"\n    self._XBDTYPE = torch.get_default_dtype()\n    self._FTOUTDTYPE = torch.get_default_dtype()\n    if isinstance(kernel,qp.KernelMultiTask):\n        solo_task = False\n        num_tasks = kernel.num_tasks\n        default_task = torch.arange(num_tasks)\n    else:\n        solo_task = True\n        default_task = 0 \n        num_tasks = 1\n    if isinstance(seqs,dict):\n        data = seqs\n        assert \"x\" in data and \"y\" in data, \"dict seqs must have keys 'x' and 'y'\"\n        if isinstance(data[\"x\"],torch.Tensor): data[\"x\"] = [data[\"x\"]]\n        if isinstance(data[\"y\"],torch.Tensor): data[\"y\"] = [data[\"y\"]]\n        assert isinstance(data[\"x\"],list) and len(data[\"x\"])==num_tasks and all(isinstance(x_l,torch.Tensor) and x_l.ndim==2 and x_l.size(1)==kernel.d for x_l in data[\"x\"]), \"data['x'] should be a list of 2d tensors of length num_tasks with each number of columns equal to the dimension\"\n        assert isinstance(data[\"y\"],list) and len(data[\"y\"])==num_tasks and all(isinstance(y_l,torch.Tensor) and y_l.ndim&gt;=1 for y_l in data[\"y\"]), \"data['y'] should be a list of tensors of length num_tasks\"\n        seqs = np.array([DummyDiscreteDistrib(data[\"x\"][l].cpu().detach().numpy()) for l in range(num_tasks)],dtype=object)\n    else:\n        data = None\n        if isinstance(seqs,int):\n            global_seed = seqs\n            seqs = np.array([qp.DigitalNetB2(kernel.d,seed=seed,order=\"GRAY\") for seed in np.random.SeedSequence(global_seed).spawn(num_tasks)],dtype=object)\n        if isinstance(seqs,qp.DiscreteDistribution):\n            seqs = np.array([seqs],dtype=object)\n        if isinstance(seqs,list):\n            seqs = np.array(seqs,dtype=object)\n    assert seqs.shape==(num_tasks,), \"seqs should be a length num_tasks=%d list\"%num_tasks\n    assert all(seqs[i].replications==1 for i in range(num_tasks)) and \"each seq should have only 1 replication\"\n    super().__init__(\n        kernel,\n        seqs,\n        num_tasks,\n        default_task,\n        solo_task,\n        noise,\n        tfs_noise,\n        requires_grad_noise,\n        shape_noise,\n        derivatives,\n        derivatives_coeffs,\n        adaptive_nugget,\n        ptransform,\n    )\n    if data is not None:\n        self.add_y_next(data[\"y\"],task=torch.arange(self.num_tasks))\n</code></pre>"},{"location":"api/#fastgps.abstract_fast_gp.AbstractFastGP","title":"AbstractFastGP","text":"<pre><code>AbstractFastGP(ft, ift, omega, *args, **kwargs)\n</code></pre> <p>               Bases: <code>AbstractGP</code></p> Source code in <code>fastgps/abstract_fast_gp.py</code> <pre><code>def __init__(self,\n        ft,\n        ift,\n        omega,\n        *args,\n        **kwargs\n    ):\n    super().__init__(*args,**kwargs)\n    # fast transforms \n    self.ft_unstable = ft\n    self.ift_unstable = ift\n    self.omega = omega\n    # storage and dynamic caches\n    self.k1parts_seq = np.array([[_K1PartsSeq(self,l0,l1,*self.derivatives_cross[l0][l1]) if l1&gt;=l0 else None for l1 in range(self.num_tasks)] for l0 in range(self.num_tasks)],dtype=object)\n    self.lam_caches = np.array([[_LamCaches(self,l0,l1,*self.derivatives_cross[l0][l1],self.derivatives_coeffs_cross[l0][l1]) if l1&gt;=l0 else None for l1 in range(self.num_tasks)] for l0 in range(self.num_tasks)],dtype=object)\n    self.ytilde_cache = np.array([_YtildeCache(self,i) for i in range(self.num_tasks)],dtype=object)\n</code></pre>"},{"location":"api/#fastgps.abstract_fast_gp.AbstractFastGP.ft","title":"ft","text":"<pre><code>ft(x)\n</code></pre> <p>One dimensional fast transform along the last dimenions.      For <code>FastGPLattice</code> this is the orthonormal Fast Fourier Transform (FFT).      For <code>FastGPDigitalNetB2</code> this is the orthonormal Fast Walsh Hadamard Transform (FWHT). </p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>inputs to be transformed along the last dimension. Require <code>n = x.size(-1)</code> is a power of 2. </p> required <p>Returns:</p> Name Type Description <code>y</code> <code>Tensor</code> <p>transformed inputs with the same shape as <code>x</code></p> Source code in <code>fastgps/abstract_fast_gp.py</code> <pre><code>def ft(self, x):\n    \"\"\"\n    One dimensional fast transform along the last dimenions. \n        For `FastGPLattice` this is the orthonormal Fast Fourier Transform (FFT). \n        For `FastGPDigitalNetB2` this is the orthonormal Fast Walsh Hadamard Transform (FWHT). \n\n    Args: \n        x (torch.Tensor): inputs to be transformed along the last dimension. Require `n = x.size(-1)` is a power of 2. \n\n    Returns: \n        y (torch.Tensor): transformed inputs with the same shape as `x` \n    \"\"\"\n    xmean = x.mean(-1)\n    y = self.ft_unstable(x-xmean[...,None])\n    y[...,0] += xmean*np.sqrt(x.size(-1))\n    return y\n</code></pre>"},{"location":"api/#fastgps.abstract_fast_gp.AbstractFastGP.ift","title":"ift","text":"<pre><code>ift(x)\n</code></pre> <p>One dimensional inverse fast transform along the last dimenions.      For <code>FastGPLattice</code> this is the orthonormal Inverse Fast Fourier Transform (IFFT).      For <code>FastGPDigitalNetB2</code> this is the orthonormal Fast Walsh Hadamard Transform (FWHT). </p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>inputs to be transformed along the last dimension. Require <code>n = x.size(-1)</code> is a power of 2. </p> required <p>Returns:</p> Name Type Description <code>y</code> <code>Tensor</code> <p>transformed inputs with the same shape as <code>x</code></p> Source code in <code>fastgps/abstract_fast_gp.py</code> <pre><code>def ift(self, x):\n    \"\"\"\n    One dimensional inverse fast transform along the last dimenions. \n        For `FastGPLattice` this is the orthonormal Inverse Fast Fourier Transform (IFFT). \n        For `FastGPDigitalNetB2` this is the orthonormal Fast Walsh Hadamard Transform (FWHT). \n\n    Args: \n        x (torch.Tensor): inputs to be transformed along the last dimension. Require `n = x.size(-1)` is a power of 2. \n\n    Returns: \n        y (torch.Tensor): transformed inputs with the same shape as `x` \n    \"\"\"\n    xmean = x.mean(-1)\n    y = self.ift_unstable(x-xmean[...,None])\n    y[...,0] += xmean*np.sqrt(x.size(-1))\n    return y\n</code></pre>"},{"location":"api/#fastgps.fast_gp_lattice.FastGPLattice","title":"FastGPLattice","text":"<pre><code>FastGPLattice(kernel, seqs, noise=2 * qp.util.transforms.EPS64, tfs_noise=(qp.util.transforms.tf_exp_eps_inv, qp.util.transforms.tf_exp_eps), requires_grad_noise=False, shape_noise=torch.Size([1]), derivatives=None, derivatives_coeffs=None, adaptive_nugget=False, ptransform=None)\n</code></pre> <p>               Bases: <code>AbstractFastGP</code></p> <p>Fast Gaussian process regression using lattice points and shift invariant kernels</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; device = \"cpu\"\n&gt;&gt;&gt; if device!=\"mps\":\n...     torch.set_default_dtype(torch.float64)\n</code></pre> <pre><code>&gt;&gt;&gt; def f_ackley(x, a=20, b=0.2, c=2*np.pi, scaling=32.768):\n...     # https://www.sfu.ca/~ssurjano/ackley.html\n...     assert x.ndim==2\n...     x = 2*scaling*x-scaling\n...     t1 = a*torch.exp(-b*torch.sqrt(torch.mean(x**2,1)))\n...     t2 = torch.exp(torch.mean(torch.cos(c*x),1))\n...     t3 = a+np.exp(1)\n...     y = -t1-t2+t3\n...     return y\n</code></pre> <pre><code>&gt;&gt;&gt; n = 2**10\n&gt;&gt;&gt; d = 2\n&gt;&gt;&gt; fgp = FastGPLattice(\n...     qp.KernelShiftInvar(d,torchify=True,device=device),\n...     seqs = qp.Lattice(dimension=d,seed=7))\n&gt;&gt;&gt; x_next = fgp.get_x_next(n)\n&gt;&gt;&gt; y_next = f_ackley(x_next)\n&gt;&gt;&gt; fgp.add_y_next(y_next)\n</code></pre> <pre><code>&gt;&gt;&gt; rng = torch.Generator().manual_seed(17)\n&gt;&gt;&gt; x = torch.rand((2**7,d),generator=rng).to(device)\n&gt;&gt;&gt; y = f_ackley(x)\n</code></pre> <pre><code>&gt;&gt;&gt; pmean = fgp.post_mean(x)\n&gt;&gt;&gt; pmean.shape\ntorch.Size([128])\n&gt;&gt;&gt; torch.linalg.norm(y-pmean)/torch.linalg.norm(y)\ntensor(0.0334)\n&gt;&gt;&gt; torch.allclose(fgp.post_mean(fgp.x),fgp.y,atol=1e-3)\nTrue\n</code></pre> <pre><code>&gt;&gt;&gt; fgp.post_cubature_mean()\ntensor(20.1842)\n&gt;&gt;&gt; fgp.post_cubature_var()\ntensor(1.2005e-09)\n</code></pre> <pre><code>&gt;&gt;&gt; data = fgp.fit(verbose=0)\n&gt;&gt;&gt; list(data.keys())\n[]\n</code></pre> <pre><code>&gt;&gt;&gt; torch.linalg.norm(y-fgp.post_mean(x))/torch.linalg.norm(y)\ntensor(0.0360)\n&gt;&gt;&gt; z = torch.rand((2**8,d),generator=rng).to(device)\n&gt;&gt;&gt; pcov = fgp.post_cov(x,z)\n&gt;&gt;&gt; pcov.shape\ntorch.Size([128, 256])\n</code></pre> <pre><code>&gt;&gt;&gt; pcov = fgp.post_cov(x,x)\n&gt;&gt;&gt; pcov.shape\ntorch.Size([128, 128])\n&gt;&gt;&gt; (pcov.diagonal()&gt;=0).all()\ntensor(True)\n</code></pre> <pre><code>&gt;&gt;&gt; pvar = fgp.post_var(x)\n&gt;&gt;&gt; pvar.shape\ntorch.Size([128])\n&gt;&gt;&gt; torch.allclose(pcov.diagonal(),pvar)\nTrue\n</code></pre> <pre><code>&gt;&gt;&gt; pmean,pstd,q,ci_low,ci_high = fgp.post_ci(x,confidence=0.99)\n&gt;&gt;&gt; ci_low.shape\ntorch.Size([128])\n&gt;&gt;&gt; ci_high.shape\ntorch.Size([128])\n</code></pre> <pre><code>&gt;&gt;&gt; fgp.post_cubature_mean()\ntensor(20.1842)\n&gt;&gt;&gt; fgp.post_cubature_var()\ntensor(2.8903e-06)\n</code></pre> <pre><code>&gt;&gt;&gt; pcmean,pcvar,q,pcci_low,pcci_high = fgp.post_cubature_ci(confidence=0.99)\n&gt;&gt;&gt; pcci_low\ntensor(20.1798)\n&gt;&gt;&gt; pcci_high\ntensor(20.1886)\n</code></pre> <pre><code>&gt;&gt;&gt; pcov_future = fgp.post_cov(x,z,n=2*n)\n&gt;&gt;&gt; pvar_future = fgp.post_var(x,n=2*n)\n&gt;&gt;&gt; pcvar_future = fgp.post_cubature_var(n=2*n)\n</code></pre> <pre><code>&gt;&gt;&gt; x_next = fgp.get_x_next(2*n)\n&gt;&gt;&gt; y_next = f_ackley(x_next)\n&gt;&gt;&gt; fgp.add_y_next(y_next)\n&gt;&gt;&gt; torch.linalg.norm(y-fgp.post_mean(x))/torch.linalg.norm(y)\ntensor(0.0295)\n</code></pre> <pre><code>&gt;&gt;&gt; torch.allclose(fgp.post_cov(x,z),pcov_future)\nTrue\n&gt;&gt;&gt; torch.allclose(fgp.post_var(x),pvar_future)\nTrue\n&gt;&gt;&gt; torch.allclose(fgp.post_cubature_var(),pcvar_future)\nTrue\n</code></pre> <pre><code>&gt;&gt;&gt; data = fgp.fit(verbose=False)\n&gt;&gt;&gt; torch.linalg.norm(y-fgp.post_mean(x))/torch.linalg.norm(y)\ntensor(0.0274)\n</code></pre> <pre><code>&gt;&gt;&gt; x_next = fgp.get_x_next(4*n)\n&gt;&gt;&gt; y_next = f_ackley(x_next)\n&gt;&gt;&gt; fgp.add_y_next(y_next)\n&gt;&gt;&gt; torch.linalg.norm(y-fgp.post_mean(x))/torch.linalg.norm(y)\ntensor(0.0277)\n</code></pre> <pre><code>&gt;&gt;&gt; data = fgp.fit(verbose=False)\n&gt;&gt;&gt; torch.linalg.norm(y-fgp.post_mean(x))/torch.linalg.norm(y)\ntensor(0.0276)\n</code></pre> <pre><code>&gt;&gt;&gt; pcov_16n = fgp.post_cov(x,z,n=16*n)\n&gt;&gt;&gt; pvar_16n = fgp.post_var(x,n=16*n)\n&gt;&gt;&gt; pcvar_16n = fgp.post_cubature_var(n=16*n)\n&gt;&gt;&gt; x_next = fgp.get_x_next(16*n)\n&gt;&gt;&gt; y_next = f_ackley(x_next)\n&gt;&gt;&gt; fgp.add_y_next(y_next)\n&gt;&gt;&gt; torch.allclose(fgp.post_cov(x,z),pcov_16n)\nTrue\n&gt;&gt;&gt; torch.allclose(fgp.post_var(x),pvar_16n)\nTrue\n&gt;&gt;&gt; torch.allclose(fgp.post_cubature_var(),pcvar_16n)\nTrue\n</code></pre> <p>Different loss metrics for fitting </p> <pre><code>&gt;&gt;&gt; n = 2**6\n&gt;&gt;&gt; d = 3\n&gt;&gt;&gt; sgp = FastGPLattice(\n...     qp.KernelShiftInvar(d,torchify=True,device=device),\n...     qp.Lattice(dimension=d,seed=7))\n&gt;&gt;&gt; x_next = sgp.get_x_next(n)\n&gt;&gt;&gt; y_next = torch.stack([torch.sin(x_next).sum(-1),torch.cos(x_next).sum(-1)],axis=0)\n&gt;&gt;&gt; sgp.add_y_next(y_next)\n&gt;&gt;&gt; data = sgp.fit(loss_metric=\"MLL\",iterations=5,verbose=0)\n&gt;&gt;&gt; data = sgp.fit(loss_metric=\"CV\",iterations=5,verbose=0,cv_weights=1/torch.arange(1,2*n+1,device=device).reshape((2,n)))\n&gt;&gt;&gt; data = sgp.fit(loss_metric=\"CV\",iterations=5,verbose=0,cv_weights=\"L2R\")\n&gt;&gt;&gt; data = sgp.fit(loss_metric=\"GCV\",iterations=5,verbose=0)\n</code></pre> <p>Batch Inference </p> <pre><code>&gt;&gt;&gt; d = 4\n&gt;&gt;&gt; n = 2**10\n&gt;&gt;&gt; dnb2 = qp.Lattice(d,seed=7) \n&gt;&gt;&gt; kernel = qp.KernelSICombined(d,torchify=True,shape_alpha=(2,4,d),shape_scale=(2,1),shape_lengthscales=(3,2,d))\n&gt;&gt;&gt; fgp = FastGPLattice(kernel,dnb2) \n&gt;&gt;&gt; x = fgp.get_x_next(n) \n&gt;&gt;&gt; x.shape\ntorch.Size([1024, 4])\n&gt;&gt;&gt; y = (x**torch.arange(6).reshape((3,2))[:,:,None,None]).sum(-1)\n&gt;&gt;&gt; y.shape\ntorch.Size([3, 2, 1024])\n&gt;&gt;&gt; fgp.add_y_next(y) \n&gt;&gt;&gt; data = fgp.fit(verbose=0)\n&gt;&gt;&gt; fgp.post_cubature_mean()\ntensor([[4.0000, 2.0001],\n        [1.3334, 1.0001],\n        [0.8001, 0.6668]])\n&gt;&gt;&gt; fgp.post_cubature_var()\ntensor([[0.0008, 0.0008],\n        [0.0008, 0.0008],\n        [0.0008, 0.0008]])\n&gt;&gt;&gt; fgp.post_cubature_var(n=4*n)\ntensor([[0., 0.],\n        [0., 0.],\n        [0., 0.]])\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>kernel</code> <code>(KernelShiftInvar, KernelShiftInvarCombined)</code> <p>Kernel object. Set to <code>qp.KernelMultiTask</code> for a multi-task GP.</p> required <code>seqs</code> <code>[int, Lattice, List]</code> <p>list of lattice sequence generators with order=\"RADICAL INVERSE\" and randomize in <code>[\"FALSE\",\"SHIFT\"]</code>. If an int <code>seed</code> is passed in we use  <pre><code>[qp.Lattice(d,seed=seed_i,randomize=\"SHIFT\") for seed_i in np.random.SeedSequence(seed).spawn(num_tasks)]\n</code></pre> See the <code>qp.Lattice</code> docs for more info</p> required <code>noise</code> <code>float</code> <p>positive noise variance i.e. nugget term</p> <code>2 * EPS64</code> <code>tfs_noise</code> <code>Tuple[callable, callable]</code> <p>the first argument transforms to the raw value to be optimized, the second applies the inverse transform</p> <code>(tf_exp_eps_inv, tf_exp_eps)</code> <code>requires_grad_noise</code> <code>bool</code> <p>wheather or not to optimize the noise parameter</p> <code>False</code> <code>shape_noise</code> <code>Size</code> <p>shape of the noise parameter, defaults to <code>torch.Size([1])</code></p> <code>Size([1])</code> <code>derivatives</code> <code>list</code> <p>list of derivative orders e.g. to include a function and its gradient set  <pre><code>derivatives = [torch.zeros(d,dtype=int)]+[ej for ej in torch.eye(d,dtype=int)]\n</code></pre></p> <code>None</code> <code>derivatives_coeffs</code> <code>list</code> <p>list of derivative coefficients where if <code>derivatives[k].shape==(p,d)</code> then we should have <code>derivatives_coeffs[k].shape==(p,)</code></p> <code>None</code> <code>adaptive_nugget</code> <code>bool</code> <p>if True, use the adaptive nugget which modifies noises based on trace ratios.  </p> <code>False</code> <code>ptransform</code> <code>str</code> <p>periodization transform in <code>[None, 'BAKER']</code> where <code>'BAKER'</code> is also known as the tent transform.</p> <code>None</code> Source code in <code>fastgps/fast_gp_lattice.py</code> <pre><code>def __init__(self,\n        kernel:Union[qp.KernelShiftInvar,qp.KernelShiftInvarCombined],\n        seqs:qp.Lattice,\n        noise:float = 2*qp.util.transforms.EPS64, \n        tfs_noise:Tuple[callable,callable] = (qp.util.transforms.tf_exp_eps_inv,qp.util.transforms.tf_exp_eps),\n        requires_grad_noise:bool = False, \n        shape_noise:torch.Size = torch.Size([1]),\n        derivatives:list = None,\n        derivatives_coeffs:list = None,\n        adaptive_nugget:bool = False,\n        ptransform:str = None,\n        ):\n    \"\"\"\n    Args:\n        kernel (qp.KernelShiftInvar,qp.KernelShiftInvarCombined): Kernel object. Set to `qp.KernelMultiTask` for a multi-task GP.\n        seqs ([int,qp.Lattice,List]): list of lattice sequence generators\n            with order=\"RADICAL INVERSE\" and randomize in `[\"FALSE\",\"SHIFT\"]`. If an int `seed` is passed in we use \n            ```python\n            [qp.Lattice(d,seed=seed_i,randomize=\"SHIFT\") for seed_i in np.random.SeedSequence(seed).spawn(num_tasks)]\n            ```\n            See the &lt;a href=\"https://qp.readthedocs.io/en/latest/algorithms.html#module-qp.discrete_distribution.lattice.lattice\" target=\"_blank\"&gt;`qp.Lattice` docs&lt;/a&gt; for more info\n        noise (float): positive noise variance i.e. nugget term\n        tfs_noise (Tuple[callable,callable]): the first argument transforms to the raw value to be optimized, the second applies the inverse transform\n        requires_grad_noise (bool): wheather or not to optimize the noise parameter\n        shape_noise (torch.Size): shape of the noise parameter, defaults to `torch.Size([1])`\n        derivatives (list): list of derivative orders e.g. to include a function and its gradient set \n            ```python\n            derivatives = [torch.zeros(d,dtype=int)]+[ej for ej in torch.eye(d,dtype=int)]\n            ```\n        derivatives_coeffs (list): list of derivative coefficients where if `derivatives[k].shape==(p,d)` then we should have `derivatives_coeffs[k].shape==(p,)`\n        adaptive_nugget (bool): if True, use the adaptive nugget which modifies noises based on trace ratios.  \n        ptransform (str): periodization transform in `[None, 'BAKER']` where `'BAKER'` is also known as the tent transform.\n    \"\"\"\n    self._XBDTYPE = torch.get_default_dtype()\n    self._FTOUTDTYPE = torch.complex64 if torch.get_default_dtype()==torch.float32 else torch.complex128\n    if isinstance(kernel,qp.KernelMultiTask):\n        solo_task = False\n        num_tasks = kernel.num_tasks\n        default_task = torch.arange(num_tasks)\n    else:\n        solo_task = True\n        default_task = 0 \n        num_tasks = 1\n    if isinstance(seqs,int):\n        global_seed = seqs\n        seqs = np.array([qp.Lattice(kernel.d,seed=seed,randomize=\"SHIFT\") for seed in np.random.SeedSequence(global_seed).spawn(num_tasks)],dtype=object)\n    if isinstance(seqs,qp.Lattice):\n        seqs = np.array([seqs],dtype=object)\n    if isinstance(seqs,list):\n        seqs = np.array(seqs,dtype=object)\n    assert seqs.shape==(num_tasks,), \"seqs should be a length num_tasks=%d list\"%num_tasks\n    assert all(isinstance(seqs[i],qp.Lattice) for i in range(num_tasks)), \"each seq should be a qp.Lattice instances\"\n    assert all(seqs[i].order==\"RADICAL INVERSE\" for i in range(num_tasks)), \"each seq should be in 'RADICAL INVERSE' order \"\n    assert all(seqs[i].replications==1 for i in range(num_tasks)) and \"each seq should have only 1 replication\"\n    assert all(seqs[i].randomize in ['FALSE','SHIFT'] for i in range(num_tasks)), \"each seq should have randomize in ['FALSE','SHIFT']\"\n    ft = qp.fftbr_torch\n    ift = qp.ifftbr_torch\n    omega = qp.omega_fftbr_torch\n    super().__init__(\n        ft,\n        ift,\n        omega,\n        kernel,\n        seqs,\n        num_tasks,\n        default_task,\n        solo_task,\n        noise,\n        tfs_noise,\n        requires_grad_noise,\n        shape_noise,\n        derivatives,\n        derivatives_coeffs,\n        adaptive_nugget,\n        ptransform,\n    )\n</code></pre>"},{"location":"api/#fastgps.fast_gp_digital_net_b2.FastGPDigitalNetB2","title":"FastGPDigitalNetB2","text":"<pre><code>FastGPDigitalNetB2(kernel, seqs, noise=2 * qp.util.transforms.EPS64, tfs_noise=(qp.util.transforms.tf_exp_eps_inv, qp.util.transforms.tf_exp_eps), requires_grad_noise=False, shape_noise=torch.Size([1]), derivatives=None, derivatives_coeffs=None, adaptive_nugget=False, ptransform=None)\n</code></pre> <p>               Bases: <code>AbstractFastGP</code></p> <p>Fast Gaussian process regression using digitally shifted digital nets paired with digitally shift invariant kernels</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; device = \"cpu\"\n&gt;&gt;&gt; if device!=\"mps\":\n...     torch.set_default_dtype(torch.float64)\n</code></pre> <pre><code>&gt;&gt;&gt; def f_ackley(x, a=20, b=0.2, c=2*np.pi, scaling=32.768):\n...     # https://www.sfu.ca/~ssurjano/ackley.html\n...     assert x.ndim==2\n...     x = 2*scaling*x-scaling\n...     t1 = a*torch.exp(-b*torch.sqrt(torch.mean(x**2,1)))\n...     t2 = torch.exp(torch.mean(torch.cos(c*x),1))\n...     t3 = a+np.exp(1)\n...     y = -t1-t2+t3\n...     return y\n</code></pre> <pre><code>&gt;&gt;&gt; n = 2**10\n&gt;&gt;&gt; d = 2\n&gt;&gt;&gt; fgp = FastGPDigitalNetB2(\n...     qp.KernelDigShiftInvar(d,torchify=True,device=device),\n...     qp.DigitalNetB2(dimension=d,seed=7))\n&gt;&gt;&gt; x_next = fgp.get_x_next(n)\n&gt;&gt;&gt; y_next = f_ackley(x_next)\n&gt;&gt;&gt; fgp.add_y_next(y_next)\n</code></pre> <pre><code>&gt;&gt;&gt; rng = torch.Generator().manual_seed(17)\n&gt;&gt;&gt; x = torch.rand((2**7,d),generator=rng).to(device)\n&gt;&gt;&gt; y = f_ackley(x)\n</code></pre> <pre><code>&gt;&gt;&gt; pmean = fgp.post_mean(x)\n&gt;&gt;&gt; pmean.shape\ntorch.Size([128])\n&gt;&gt;&gt; torch.linalg.norm(y-pmean)/torch.linalg.norm(y)\ntensor(0.0308)\n&gt;&gt;&gt; torch.allclose(fgp.post_mean(fgp.x),fgp.y)\nTrue\n</code></pre> <pre><code>&gt;&gt;&gt; data = fgp.fit(verbose=0)\n&gt;&gt;&gt; list(data.keys())\n[]\n</code></pre> <pre><code>&gt;&gt;&gt; torch.linalg.norm(y-fgp.post_mean(x))/torch.linalg.norm(y)\ntensor(0.0328)\n&gt;&gt;&gt; z = torch.rand((2**8,d),generator=rng).to(device)\n&gt;&gt;&gt; pcov = fgp.post_cov(x,z)\n&gt;&gt;&gt; pcov.shape\ntorch.Size([128, 256])\n</code></pre> <pre><code>&gt;&gt;&gt; pcov = fgp.post_cov(x,x)\n&gt;&gt;&gt; pcov.shape\ntorch.Size([128, 128])\n&gt;&gt;&gt; (pcov.diagonal()&gt;=0).all()\ntensor(True)\n</code></pre> <pre><code>&gt;&gt;&gt; pvar = fgp.post_var(x)\n&gt;&gt;&gt; pvar.shape\ntorch.Size([128])\n&gt;&gt;&gt; torch.allclose(pcov.diagonal(),pvar,atol=1e-5)\nTrue\n</code></pre> <pre><code>&gt;&gt;&gt; pmean,pstd,q,ci_low,ci_high = fgp.post_ci(x,confidence=0.99)\n&gt;&gt;&gt; ci_low.shape\ntorch.Size([128])\n&gt;&gt;&gt; ci_high.shape\ntorch.Size([128])\n</code></pre> <pre><code>&gt;&gt;&gt; fgp.post_cubature_mean()\ntensor(20.1846)\n&gt;&gt;&gt; fgp.post_cubature_var()\ntensor(0.0002)\n</code></pre> <pre><code>&gt;&gt;&gt; pcmean,pcvar,q,pcci_low,pcci_high = fgp.post_cubature_ci(confidence=0.99)\n&gt;&gt;&gt; pcci_low\ntensor(20.1466)\n&gt;&gt;&gt; pcci_high\ntensor(20.2227)\n</code></pre> <pre><code>&gt;&gt;&gt; pcov_future = fgp.post_cov(x,z,n=2*n)\n&gt;&gt;&gt; pvar_future = fgp.post_var(x,n=2*n)\n&gt;&gt;&gt; pcvar_future = fgp.post_cubature_var(n=2*n)\n</code></pre> <pre><code>&gt;&gt;&gt; x_next = fgp.get_x_next(2*n)\n&gt;&gt;&gt; y_next = f_ackley(x_next)\n&gt;&gt;&gt; fgp.add_y_next(y_next)\n&gt;&gt;&gt; torch.linalg.norm(y-fgp.post_mean(x))/torch.linalg.norm(y)\ntensor(0.0267)\n</code></pre> <pre><code>&gt;&gt;&gt; torch.allclose(fgp.post_cov(x,z),pcov_future)\nTrue\n&gt;&gt;&gt; torch.allclose(fgp.post_var(x),pvar_future)\nTrue\n&gt;&gt;&gt; torch.allclose(fgp.post_cubature_var(),pcvar_future)\nTrue\n</code></pre> <pre><code>&gt;&gt;&gt; data = fgp.fit(verbose=False)\n&gt;&gt;&gt; torch.linalg.norm(y-fgp.post_mean(x))/torch.linalg.norm(y)\ntensor(0.0254)\n</code></pre> <pre><code>&gt;&gt;&gt; x_next = fgp.get_x_next(4*n)\n&gt;&gt;&gt; y_next = f_ackley(x_next)\n&gt;&gt;&gt; fgp.add_y_next(y_next)\n&gt;&gt;&gt; torch.linalg.norm(y-fgp.post_mean(x))/torch.linalg.norm(y)\ntensor(0.0162)\n</code></pre> <pre><code>&gt;&gt;&gt; data = fgp.fit(verbose=False)\n&gt;&gt;&gt; torch.linalg.norm(y-fgp.post_mean(x))/torch.linalg.norm(y)\ntensor(0.0132)\n</code></pre> <pre><code>&gt;&gt;&gt; pcov_16n = fgp.post_cov(x,z,n=16*n)\n&gt;&gt;&gt; pvar_16n = fgp.post_var(x,n=16*n)\n&gt;&gt;&gt; pcvar_16n = fgp.post_cubature_var(n=16*n)\n&gt;&gt;&gt; x_next = fgp.get_x_next(16*n)\n&gt;&gt;&gt; y_next = f_ackley(x_next)\n&gt;&gt;&gt; fgp.add_y_next(y_next)\n&gt;&gt;&gt; torch.allclose(fgp.post_cov(x,z),pcov_16n)\nTrue\n&gt;&gt;&gt; torch.allclose(fgp.post_var(x),pvar_16n)\nTrue\n&gt;&gt;&gt; torch.allclose(fgp.post_cubature_var(),pcvar_16n)\nTrue\n</code></pre> <p>Different loss metrics for fitting </p> <pre><code>&gt;&gt;&gt; n = 2**6\n&gt;&gt;&gt; d = 3\n&gt;&gt;&gt; sgp = FastGPDigitalNetB2(\n...     qp.KernelDigShiftInvar(d,torchify=True,device=device),\n...     qp.DigitalNetB2(dimension=d,seed=7))\n&gt;&gt;&gt; x_next = sgp.get_x_next(n)\n&gt;&gt;&gt; y_next = torch.stack([torch.sin(x_next).sum(-1),torch.cos(x_next).sum(-1)],axis=0)\n&gt;&gt;&gt; sgp.add_y_next(y_next)\n&gt;&gt;&gt; data = sgp.fit(loss_metric=\"MLL\",iterations=5,verbose=0)\n&gt;&gt;&gt; data = sgp.fit(loss_metric=\"CV\",iterations=5,verbose=0,cv_weights=1/torch.arange(1,2*n+1,device=device).reshape((2,n)))\n&gt;&gt;&gt; data = sgp.fit(loss_metric=\"CV\",iterations=5,verbose=0,cv_weights=\"L2R\")\n&gt;&gt;&gt; data = sgp.fit(loss_metric=\"GCV\",iterations=5,verbose=0)\n</code></pre> <p>Batch Inference </p> <pre><code>&gt;&gt;&gt; d = 4\n&gt;&gt;&gt; n = 2**10\n&gt;&gt;&gt; dnb2 = qp.DigitalNetB2(d,seed=7) \n&gt;&gt;&gt; kernel = qp.KernelDSICombined(d,torchify=True,shape_alpha=(2,4,d),shape_scale=(2,1),shape_lengthscales=(3,2,d))\n&gt;&gt;&gt; fgp = FastGPDigitalNetB2(kernel,dnb2) \n&gt;&gt;&gt; x = fgp.get_x_next(n) \n&gt;&gt;&gt; x.shape\ntorch.Size([1024, 4])\n&gt;&gt;&gt; y = (x**torch.arange(6).reshape((3,2))[:,:,None,None]).sum(-1)\n&gt;&gt;&gt; y.shape\ntorch.Size([3, 2, 1024])\n&gt;&gt;&gt; fgp.add_y_next(y) \n&gt;&gt;&gt; data = fgp.fit(verbose=0)\n&gt;&gt;&gt; fgp.post_cubature_mean()\ntensor([[4.0000, 2.0000],\n        [1.3333, 1.0000],\n        [0.8000, 0.6667]])\n&gt;&gt;&gt; fgp.post_cubature_var()\ntensor([[2.2939e-16, 9.6623e-09],\n        [1.6232e-08, 7.8090e-09],\n        [3.7257e-08, 2.0389e-08]])\n&gt;&gt;&gt; fgp.post_cubature_var(n=4*n)\ntensor([[2.9341e-18, 2.2246e-10],\n        [2.8796e-10, 1.6640e-10],\n        [5.9842e-10, 3.7897e-10]])\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>kernel</code> <code>Union[KernelDigShiftInvar, KernelDigShiftInvarAdaptiveAlpha, KernelDigShiftInvarCombined]</code> <p>Kernel object. Set to <code>qp.KernelMultiTask</code> for a multi-task GP.</p> required <code>seqs</code> <code>Union[int,qp.DigitalNetB2,List]]</code> <p>list of digital sequence generators in base \\(b=2\\)  with order=\"RADICAL INVERSE\" and randomize in <code>[\"FALSE\",\"DS\"]</code>. If an int <code>seed</code> is passed in we use  <pre><code>[qp.DigitalNetB2(d,seed=seed_i,randomize=\"DS\") for seed_i in np.random.SeedSequence(seed).spawn(num_tasks)]\n</code></pre> See the <code>qp.DigitalNetB2</code> docs for more info.  If <code>num_tasks==1</code> then randomize may be in <code>[\"FALSE\",\"DS\",\"LMS\",\"LMS DS\"]</code>. </p> required <code>noise</code> <code>float</code> <p>positive noise variance i.e. nugget term</p> <code>2 * EPS64</code> <code>tfs_noise</code> <code>Tuple[callable, callable]</code> <p>the first argument transforms to the raw value to be optimized, the second applies the inverse transform</p> <code>(tf_exp_eps_inv, tf_exp_eps)</code> <code>requires_grad_noise</code> <code>bool</code> <p>wheather or not to optimize the noise parameter</p> <code>False</code> <code>shape_noise</code> <code>Size</code> <p>shape of the noise parameter, defaults to <code>torch.Size([1])</code></p> <code>Size([1])</code> <code>derivatives</code> <code>list</code> <p>list of derivative orders e.g. to include a function and its gradient set  <pre><code>derivatives = [torch.zeros(d,dtype=int)]+[ej for ej in torch.eye(d,dtype=int)]\n</code></pre></p> <code>None</code> <code>derivatives_coeffs</code> <code>list</code> <p>list of derivative coefficients where if <code>derivatives[k].shape==(p,d)</code> then we should have <code>derivatives_coeffs[k].shape==(p,)</code></p> <code>None</code> <code>adaptive_nugget</code> <code>bool</code> <p>if True, use the adaptive nugget which modifies noises based on trace ratios.  </p> <code>False</code> <code>ptransform</code> <code>str</code> <p>periodization transform in <code>[None, 'BAKER']</code> where <code>'BAKER'</code> is also known as the tent transform.</p> <code>None</code> Source code in <code>fastgps/fast_gp_digital_net_b2.py</code> <pre><code>def __init__(self,\n        kernel:Union[qp.KernelDigShiftInvar,qp.KernelDigShiftInvarAdaptiveAlpha,qp.KernelDigShiftInvarCombined],\n        seqs:Union[qp.DigitalNetB2,int],\n        noise:float = 2*qp.util.transforms.EPS64,\n        tfs_noise:Tuple[callable,callable] = (qp.util.transforms.tf_exp_eps_inv,qp.util.transforms.tf_exp_eps),\n        requires_grad_noise:bool = False, \n        shape_noise:torch.Size = torch.Size([1]),\n        derivatives:list = None,\n        derivatives_coeffs:list = None,\n        adaptive_nugget:bool = False,\n        ptransform:str = None,\n        ):\n    \"\"\"\n    Args:\n        kernel (Union[qp.KernelDigShiftInvar,qp.KernelDigShiftInvarAdaptiveAlpha,qp.KernelDigShiftInvarCombined]): Kernel object. Set to `qp.KernelMultiTask` for a multi-task GP.\n        seqs (Union[int,qp.DigitalNetB2,List]]): list of digital sequence generators in base $b=2$ \n            with order=\"RADICAL INVERSE\" and randomize in `[\"FALSE\",\"DS\"]`. If an int `seed` is passed in we use \n            ```python\n            [qp.DigitalNetB2(d,seed=seed_i,randomize=\"DS\") for seed_i in np.random.SeedSequence(seed).spawn(num_tasks)]\n            ```\n            See the &lt;a href=\"https://qp.readthedocs.io/en/latest/algorithms.html#module-qp.discrete_distribution.digital_net_b2.digital_net_b2\" target=\"_blank\"&gt;`qp.DigitalNetB2` docs&lt;/a&gt; for more info. \n            If `num_tasks==1` then randomize may be in `[\"FALSE\",\"DS\",\"LMS\",\"LMS DS\"]`. \n        noise (float): positive noise variance i.e. nugget term\n        tfs_noise (Tuple[callable,callable]): the first argument transforms to the raw value to be optimized, the second applies the inverse transform\n        requires_grad_noise (bool): wheather or not to optimize the noise parameter\n        shape_noise (torch.Size): shape of the noise parameter, defaults to `torch.Size([1])`\n        derivatives (list): list of derivative orders e.g. to include a function and its gradient set \n            ```python\n            derivatives = [torch.zeros(d,dtype=int)]+[ej for ej in torch.eye(d,dtype=int)]\n            ```\n        derivatives_coeffs (list): list of derivative coefficients where if `derivatives[k].shape==(p,d)` then we should have `derivatives_coeffs[k].shape==(p,)`\n        adaptive_nugget (bool): if True, use the adaptive nugget which modifies noises based on trace ratios.  \n        ptransform (str): periodization transform in `[None, 'BAKER']` where `'BAKER'` is also known as the tent transform.\n    \"\"\"\n    self._XBDTYPE = torch.int64\n    self._FTOUTDTYPE = torch.get_default_dtype()\n    if isinstance(kernel,qp.KernelMultiTask):\n        solo_task = False\n        num_tasks = kernel.num_tasks\n        default_task = torch.arange(num_tasks)\n    else:\n        solo_task = True\n        default_task = 0 \n        num_tasks = 1\n    if isinstance(seqs,int):\n        global_seed = seqs\n        seqs = np.array([qp.DigitalNetB2(kernel.d,seed=seed,randomize=\"DS\") for seed in np.random.SeedSequence(global_seed).spawn(num_tasks)],dtype=object)\n    if isinstance(seqs,qp.DigitalNetB2):\n        seqs = np.array([seqs],dtype=object)\n    if isinstance(seqs,list):\n        seqs = np.array(seqs,dtype=object)\n    assert seqs.shape==(num_tasks,), \"seqs should be a length num_tasks=%d list\"%num_tasks\n    assert all(isinstance(seqs[i],qp.DigitalNetB2) for i in range(num_tasks)), \"each seq should be a qp.DigitalNetB2 instances\"\n    assert all(seqs[i].order==\"RADICAL INVERSE\" for i in range(num_tasks)), \"each seq should be in 'RADICAL INVERSE' order \"\n    assert all(seqs[i].replications==1 for i in range(num_tasks)) and \"each seq should have only 1 replication\"\n    if num_tasks==1:\n        assert seqs[0].randomize in ['FALSE','DS','LMS','LMS DS'], \"seq should have randomize in ['FALSE','DS','LMS','LMS DS']\"\n    else:\n        assert all(seqs[i].randomize in ['FALSE','DS'] for i in range(num_tasks)), \"each seq should have randomize in ['FALSE','DS']\"\n    ts = torch.tensor([seqs[i].t for i in range(num_tasks)])\n    assert (ts&lt;64).all(), \"each seq must have t&lt;64\"\n    assert (ts==ts[0]).all(), \"all seqs should have the same t\"\n    self.t = ts[0].item()\n    if isinstance(kernel,qp.KernelMultiTask):\n        kernel.base_kernel.set_t(self.t)\n    else:\n        kernel.set_t(self.t)\n    ift = ft = qp.fwht_torch\n    omega = qp.omega_fwht_torch\n    super().__init__(\n        ft,\n        ift,\n        omega,\n        kernel,\n        seqs,\n        num_tasks,\n        default_task,\n        solo_task,\n        noise,\n        tfs_noise,\n        requires_grad_noise,\n        shape_noise,\n        derivatives,\n        derivatives_coeffs,\n        adaptive_nugget,\n        ptransform,\n    )\n</code></pre>"},{"location":"examples/batch_multitask/fgp_dnb2/","title":"Fast GP Net","text":"In\u00a0[1]: Copied! <pre>import fastgps\nimport qmcpy as qp\nimport torch\nimport numpy as np\n</pre> import fastgps import qmcpy as qp import torch import numpy as np In\u00a0[2]: Copied! <pre>device = \"cpu\"\nif device!=\"mps\":\n    torch.set_default_dtype(torch.float64)\n</pre> device = \"cpu\" if device!=\"mps\":     torch.set_default_dtype(torch.float64) In\u00a0[3]: Copied! <pre>d = 6\nrng = torch.Generator().manual_seed(7)\nshape_batch = [2,3,4]\nnum_tasks = 5 \ndef f(l, x):\n    consts = torch.arange(torch.prod(torch.tensor(shape_batch)),device=device).reshape(shape_batch)\n    y = (consts[...,None,None]*x**torch.arange(1,d+1,device=device)).sum(-1)+torch.randn(shape_batch+[x.size(0)],generator=rng).to(device)/(3+l)\n    return y\nx = torch.rand((2**7,d),generator=rng).to(device) # random testing locations\ny = torch.cat([f(l,x)[...,None,:] for l in range(num_tasks)],-2) # true values at random testing locations\nz = torch.rand((2**6,d),generator=rng).to(device) # other random locations at which to evaluate covariance\nprint(\"x.shape = %s\"%str(tuple(x.shape)))\nprint(\"y.shape = %s\"%str(tuple(y.shape)))\nprint(\"z.shape = %s\"%str(tuple(z.shape)))\n</pre> d = 6 rng = torch.Generator().manual_seed(7) shape_batch = [2,3,4] num_tasks = 5  def f(l, x):     consts = torch.arange(torch.prod(torch.tensor(shape_batch)),device=device).reshape(shape_batch)     y = (consts[...,None,None]*x**torch.arange(1,d+1,device=device)).sum(-1)+torch.randn(shape_batch+[x.size(0)],generator=rng).to(device)/(3+l)     return y x = torch.rand((2**7,d),generator=rng).to(device) # random testing locations y = torch.cat([f(l,x)[...,None,:] for l in range(num_tasks)],-2) # true values at random testing locations z = torch.rand((2**6,d),generator=rng).to(device) # other random locations at which to evaluate covariance print(\"x.shape = %s\"%str(tuple(x.shape))) print(\"y.shape = %s\"%str(tuple(y.shape))) print(\"z.shape = %s\"%str(tuple(z.shape))) <pre>x.shape = (128, 6)\ny.shape = (2, 3, 4, 5, 128)\nz.shape = (64, 6)\n</pre> In\u00a0[4]: Copied! <pre>fgp = fastgps.FastGPDigitalNetB2(\n    qp.KernelMultiTask(\n        qp.KernelDigShiftInvar(\n            d = d,\n            shape_scale = shape_batch[:]+[1],\n            shape_lengthscales = shape_batch[1:]+[d],\n            torchify = True,\n            device = device),\n        num_tasks = num_tasks,\n        shape_factor = shape_batch[:]+[num_tasks,num_tasks],\n        shape_diag = shape_batch[1:]+[num_tasks]),\n    seqs = 7,\n    shape_noise = shape_batch[2:]+[1],\n)\nprint(\"fgp.kernel.base_kernel.scale.shape = %s\"%str(tuple(fgp.kernel.base_kernel.scale.shape)))\nprint(\"fgp.kernel.base_kernel.lengthscales.shape = %s\"%str(tuple(fgp.kernel.base_kernel.lengthscales.shape)))\nprint(\"fgp.kernel.factor.shape = %s\"%str(tuple(fgp.kernel.factor.shape)))\nprint(\"fgp.kernel.diag.shape = %s\"%str(tuple(fgp.kernel.diag.shape)))\nprint(\"fgp.noise.shape = %s\"%str(tuple(fgp.noise.shape)))\n</pre> fgp = fastgps.FastGPDigitalNetB2(     qp.KernelMultiTask(         qp.KernelDigShiftInvar(             d = d,             shape_scale = shape_batch[:]+[1],             shape_lengthscales = shape_batch[1:]+[d],             torchify = True,             device = device),         num_tasks = num_tasks,         shape_factor = shape_batch[:]+[num_tasks,num_tasks],         shape_diag = shape_batch[1:]+[num_tasks]),     seqs = 7,     shape_noise = shape_batch[2:]+[1], ) print(\"fgp.kernel.base_kernel.scale.shape = %s\"%str(tuple(fgp.kernel.base_kernel.scale.shape))) print(\"fgp.kernel.base_kernel.lengthscales.shape = %s\"%str(tuple(fgp.kernel.base_kernel.lengthscales.shape))) print(\"fgp.kernel.factor.shape = %s\"%str(tuple(fgp.kernel.factor.shape))) print(\"fgp.kernel.diag.shape = %s\"%str(tuple(fgp.kernel.diag.shape))) print(\"fgp.noise.shape = %s\"%str(tuple(fgp.noise.shape))) <pre>fgp.kernel.base_kernel.scale.shape = (2, 3, 4, 1)\nfgp.kernel.base_kernel.lengthscales.shape = (3, 4, 6)\nfgp.kernel.factor.shape = (2, 3, 4, 5, 5)\nfgp.kernel.diag.shape = (3, 4, 5)\nfgp.noise.shape = (4, 1)\n</pre> In\u00a0[5]: Copied! <pre>x_next = fgp.get_x_next(n=2**torch.arange(num_tasks+1,1,-1,device=device))\ny_next = [f(l,x_next[l]) for l in range(num_tasks)]\nfgp.add_y_next(y_next)\nfor i in range(len(x_next)):  \n    print(\"i = %d\"%i)\n    print(\"\\tx_next[%d].shape = %s\"%(i,str(tuple(x_next[i].shape))))\n    print(\"\\ty_next[%d].shape = %s\"%(i,str(tuple(y_next[i].shape))))\n</pre> x_next = fgp.get_x_next(n=2**torch.arange(num_tasks+1,1,-1,device=device)) y_next = [f(l,x_next[l]) for l in range(num_tasks)] fgp.add_y_next(y_next) for i in range(len(x_next)):       print(\"i = %d\"%i)     print(\"\\tx_next[%d].shape = %s\"%(i,str(tuple(x_next[i].shape))))     print(\"\\ty_next[%d].shape = %s\"%(i,str(tuple(y_next[i].shape)))) <pre>i = 0\n\tx_next[0].shape = (64, 6)\n\ty_next[0].shape = (2, 3, 4, 64)\ni = 1\n\tx_next[1].shape = (32, 6)\n\ty_next[1].shape = (2, 3, 4, 32)\ni = 2\n\tx_next[2].shape = (16, 6)\n\ty_next[2].shape = (2, 3, 4, 16)\ni = 3\n\tx_next[3].shape = (8, 6)\n\ty_next[3].shape = (2, 3, 4, 8)\ni = 4\n\tx_next[4].shape = (4, 6)\n\ty_next[4].shape = (2, 3, 4, 4)\n</pre> In\u00a0[6]: Copied! <pre>pmean = fgp.post_mean(x)\nprint(\"pmean.shape = %s\"%str(tuple(pmean.shape)))\nl2rerror = torch.linalg.norm(y-pmean,dim=-1)/torch.linalg.norm(y,dim=-1)\nprint(\"l2rerror.shape = %s\"%str(tuple(l2rerror.shape)))\n</pre> pmean = fgp.post_mean(x) print(\"pmean.shape = %s\"%str(tuple(pmean.shape))) l2rerror = torch.linalg.norm(y-pmean,dim=-1)/torch.linalg.norm(y,dim=-1) print(\"l2rerror.shape = %s\"%str(tuple(l2rerror.shape))) <pre>pmean.shape = (2, 3, 4, 5, 128)\nl2rerror.shape = (2, 3, 4, 5)\n</pre> In\u00a0[7]: Copied! <pre>data = fgp.fit(stop_crit_improvement_threshold=1e3)\nlist(data.keys())\n</pre> data = fgp.fit(stop_crit_improvement_threshold=1e3) list(data.keys()) <pre>     iter of 5.0e+03 | best loss  | loss      \n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n            0.00e+00 | 1.07e+04   | 1.07e+04  \n            5.00e+00 | 8.07e+03   | 8.07e+03  \n            1.00e+01 | 7.69e+03   | 7.69e+03  \n            1.50e+01 | 7.42e+03   | 7.42e+03  \n            2.00e+01 | 7.19e+03   | 7.19e+03  \n            2.50e+01 | 7.04e+03   | 7.04e+03  \n            3.00e+01 | 6.94e+03   | 6.94e+03  \n            3.50e+01 | 6.87e+03   | 6.87e+03  \n            4.00e+01 | 6.84e+03   | 6.84e+03  \n            4.50e+01 | 6.81e+03   | 6.81e+03  \n            5.00e+01 | 6.79e+03   | 6.79e+03  \n            5.50e+01 | 6.78e+03   | 6.78e+03  \n            6.00e+01 | 6.77e+03   | 6.77e+03  \n            6.50e+01 | 6.76e+03   | 6.76e+03  \n            7.00e+01 | 6.75e+03   | 6.75e+03  \n            7.50e+01 | 6.74e+03   | 6.74e+03  \n            8.00e+01 | 6.74e+03   | 6.74e+03  \n            8.50e+01 | 6.74e+03   | 6.74e+03  \n            9.00e+01 | 6.73e+03   | 6.73e+03  \n            9.50e+01 | 6.73e+03   | 6.73e+03  \n            9.60e+01 | 6.73e+03   | 6.73e+03  \n</pre> Out[7]: <pre>[]</pre> In\u00a0[8]: Copied! <pre>pmean,pvar,q,ci_low,ci_high = fgp.post_ci(x,confidence=0.99)\nprint(\"pmean.shape = %s\"%str(tuple(pmean.shape)))\nprint(\"pvar.shape = %s\"%str(tuple(pvar.shape)))\nprint(\"q = %.2f\"%q)\nprint(\"ci_low.shape = %s\"%str(tuple(ci_low.shape)))\nprint(\"ci_high.shape = %s\"%str(tuple(ci_high.shape)))\nl2rerror = torch.linalg.norm(y-pmean,dim=-1)/torch.linalg.norm(y,dim=-1)\nprint(\"l2rerror.shape = %s\"%str(tuple(l2rerror.shape)))\npcov = fgp.post_cov(x,x)\nprint(\"pcov.shape = %s\"%str(tuple(pcov.shape)))\n_range0,_rangen1 = torch.arange(pcov.size(-3)),torch.arange(pcov.size(-1))\npcov2 = fgp.post_cov(x,z)\nprint(\"pcov2.shape = %s\"%str(tuple(pcov2.shape)))\nprint(\"\\npcov diag matches pvar: %s\"%torch.allclose(pcov[...,_range0,_range0,:,:][...,_rangen1,_rangen1],pvar))\nprint(\"non-negative pvar: %s\"%(pvar&gt;=0).all().item())\n</pre> pmean,pvar,q,ci_low,ci_high = fgp.post_ci(x,confidence=0.99) print(\"pmean.shape = %s\"%str(tuple(pmean.shape))) print(\"pvar.shape = %s\"%str(tuple(pvar.shape))) print(\"q = %.2f\"%q) print(\"ci_low.shape = %s\"%str(tuple(ci_low.shape))) print(\"ci_high.shape = %s\"%str(tuple(ci_high.shape))) l2rerror = torch.linalg.norm(y-pmean,dim=-1)/torch.linalg.norm(y,dim=-1) print(\"l2rerror.shape = %s\"%str(tuple(l2rerror.shape))) pcov = fgp.post_cov(x,x) print(\"pcov.shape = %s\"%str(tuple(pcov.shape))) _range0,_rangen1 = torch.arange(pcov.size(-3)),torch.arange(pcov.size(-1)) pcov2 = fgp.post_cov(x,z) print(\"pcov2.shape = %s\"%str(tuple(pcov2.shape))) print(\"\\npcov diag matches pvar: %s\"%torch.allclose(pcov[...,_range0,_range0,:,:][...,_rangen1,_rangen1],pvar)) print(\"non-negative pvar: %s\"%(pvar&gt;=0).all().item()) <pre>pmean.shape = (2, 3, 4, 5, 128)\npvar.shape = (2, 3, 4, 5, 128)\nq = 2.58\nci_low.shape = (2, 3, 4, 5, 128)\nci_high.shape = (2, 3, 4, 5, 128)\nl2rerror.shape = (2, 3, 4, 5)\npcov.shape = (2, 3, 4, 5, 5, 128, 128)\npcov2.shape = (2, 3, 4, 5, 5, 128, 64)\n\npcov diag matches pvar: True\nnon-negative pvar: True\n</pre> In\u00a0[9]: Copied! <pre>pcmean,pcvar,q,cci_low,cci_high = fgp.post_cubature_ci(confidence=0.99)\nprint(\"pcmean.shape = %s\"%str(tuple(pcmean.shape)))\nprint(\"pcvar.shape = %s\"%str(tuple(pcvar.shape)))\nprint(\"cci_low.shape = %s\"%str(tuple(cci_low.shape)))\nprint(\"cci_high.shape = %s\"%str(tuple(cci_high.shape)))\npccov = fgp.post_cubature_cov()\nprint(\"pccov.shape = %s\"%str(tuple(pccov.shape)))\n</pre> pcmean,pcvar,q,cci_low,cci_high = fgp.post_cubature_ci(confidence=0.99) print(\"pcmean.shape = %s\"%str(tuple(pcmean.shape))) print(\"pcvar.shape = %s\"%str(tuple(pcvar.shape))) print(\"cci_low.shape = %s\"%str(tuple(cci_low.shape))) print(\"cci_high.shape = %s\"%str(tuple(cci_high.shape))) pccov = fgp.post_cubature_cov() print(\"pccov.shape = %s\"%str(tuple(pccov.shape))) <pre>pcmean.shape = (2, 3, 4, 5)\npcvar.shape = (2, 3, 4, 5)\ncci_low.shape = (2, 3, 4, 5)\ncci_high.shape = (2, 3, 4, 5)\npccov.shape = (2, 3, 4, 5, 5)\n</pre> In\u00a0[10]: Copied! <pre>n_new = fgp.n*2\npcov_future = fgp.post_cov(x,z,n=n_new)\npvar_future = fgp.post_var(x,n=n_new)\npcvar_future = fgp.post_cubature_var(n=n_new)\n</pre> n_new = fgp.n*2 pcov_future = fgp.post_cov(x,z,n=n_new) pvar_future = fgp.post_var(x,n=n_new) pcvar_future = fgp.post_cubature_var(n=n_new) In\u00a0[11]: Copied! <pre>x_next = fgp.get_x_next(n_new)\ny_next = [f(l,x_next[l]) for l in range(num_tasks)]\nfor _y in y_next:\n    print(_y.shape)\nfgp.add_y_next(y_next)\nl2rerror = torch.linalg.norm(y-fgp.post_mean(x),dim=-1)/torch.linalg.norm(y,dim=-1)\nprint(\"l2rerror.shape = %s\"%str(tuple(l2rerror.shape)))\nassert torch.allclose(fgp.post_cov(x,z),pcov_future)\nassert torch.allclose(fgp.post_var(x),pvar_future)\nassert torch.allclose(fgp.post_cubature_var(),pcvar_future)\n</pre> x_next = fgp.get_x_next(n_new) y_next = [f(l,x_next[l]) for l in range(num_tasks)] for _y in y_next:     print(_y.shape) fgp.add_y_next(y_next) l2rerror = torch.linalg.norm(y-fgp.post_mean(x),dim=-1)/torch.linalg.norm(y,dim=-1) print(\"l2rerror.shape = %s\"%str(tuple(l2rerror.shape))) assert torch.allclose(fgp.post_cov(x,z),pcov_future) assert torch.allclose(fgp.post_var(x),pvar_future) assert torch.allclose(fgp.post_cubature_var(),pcvar_future) <pre>torch.Size([2, 3, 4, 64])\ntorch.Size([2, 3, 4, 32])\ntorch.Size([2, 3, 4, 16])\ntorch.Size([2, 3, 4, 8])\ntorch.Size([2, 3, 4, 4])\nl2rerror.shape = (2, 3, 4, 5)\n</pre> In\u00a0[12]: Copied! <pre>data = fgp.fit(iterations=5,verbose=False)\nl2rerror = torch.linalg.norm(y-fgp.post_mean(x),dim=-1)/torch.linalg.norm(y,dim=-1)\nprint(\"l2rerror.shape = %s\"%str(tuple(l2rerror.shape)))\n</pre> data = fgp.fit(iterations=5,verbose=False) l2rerror = torch.linalg.norm(y-fgp.post_mean(x),dim=-1)/torch.linalg.norm(y,dim=-1) print(\"l2rerror.shape = %s\"%str(tuple(l2rerror.shape))) <pre>l2rerror.shape = (2, 3, 4, 5)\n</pre> In\u00a0[13]: Copied! <pre>n_new = fgp.n*2\npcov_new = fgp.post_cov(x,z,n=n_new)\npvar_new = fgp.post_var(x,n=n_new)\npcvar_new = fgp.post_cubature_var(n=n_new)\nx_next = fgp.get_x_next(n_new)\ny_next = [f(l,x_next[l]) for l in range(num_tasks)]\nfgp.add_y_next(y_next)\nassert torch.allclose(fgp.post_cov(x,z),pcov_new)\nassert torch.allclose(fgp.post_var(x),pvar_new)\nassert torch.allclose(fgp.post_cubature_var(),pcvar_new)\n</pre> n_new = fgp.n*2 pcov_new = fgp.post_cov(x,z,n=n_new) pvar_new = fgp.post_var(x,n=n_new) pcvar_new = fgp.post_cubature_var(n=n_new) x_next = fgp.get_x_next(n_new) y_next = [f(l,x_next[l]) for l in range(num_tasks)] fgp.add_y_next(y_next) assert torch.allclose(fgp.post_cov(x,z),pcov_new) assert torch.allclose(fgp.post_var(x),pvar_new) assert torch.allclose(fgp.post_cubature_var(),pcvar_new) In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/batch_multitask/fgp_dnb2/#fast-batch-multitask-net-gp","title":"Fast Batch Multitask Net GP\u00b6","text":""},{"location":"examples/batch_multitask/fgp_dnb2/#true-function","title":"True Function\u00b6","text":""},{"location":"examples/batch_multitask/fgp_dnb2/#construct-fast-gp","title":"Construct Fast GP\u00b6","text":""},{"location":"examples/batch_multitask/fgp_dnb2/#project-and-increase-sample-size","title":"Project and Increase Sample Size\u00b6","text":""},{"location":"examples/batch_multitask/fgp_lattice/","title":"Fast GP Lattice","text":"In\u00a0[1]: Copied! <pre>import fastgps\nimport qmcpy as qp\nimport torch\nimport numpy as np\n</pre> import fastgps import qmcpy as qp import torch import numpy as np In\u00a0[2]: Copied! <pre>device = \"cpu\"\nif device!=\"mps\":\n    torch.set_default_dtype(torch.float64)\n</pre> device = \"cpu\" if device!=\"mps\":     torch.set_default_dtype(torch.float64) In\u00a0[3]: Copied! <pre>d = 6\nrng = torch.Generator().manual_seed(7)\nshape_batch = [2,3,4]\nnum_tasks = 5 \ndef f(l, x):\n    consts = torch.arange(torch.prod(torch.tensor(shape_batch)),device=device).reshape(shape_batch)\n    y = (consts[...,None,None]*x**torch.arange(1,d+1,device=device)).sum(-1)+torch.randn(shape_batch+[x.size(0)],generator=rng).to(device)/(3+l)\n    return y\nx = torch.rand((2**7,d),generator=rng).to(device) # random testing locations\ny = torch.cat([f(l,x)[...,None,:] for l in range(num_tasks)],-2) # true values at random testing locations\nz = torch.rand((2**6,d),generator=rng).to(device) # other random locations at which to evaluate covariance\nprint(\"x.shape = %s\"%str(tuple(x.shape)))\nprint(\"y.shape = %s\"%str(tuple(y.shape)))\nprint(\"z.shape = %s\"%str(tuple(z.shape)))\n</pre> d = 6 rng = torch.Generator().manual_seed(7) shape_batch = [2,3,4] num_tasks = 5  def f(l, x):     consts = torch.arange(torch.prod(torch.tensor(shape_batch)),device=device).reshape(shape_batch)     y = (consts[...,None,None]*x**torch.arange(1,d+1,device=device)).sum(-1)+torch.randn(shape_batch+[x.size(0)],generator=rng).to(device)/(3+l)     return y x = torch.rand((2**7,d),generator=rng).to(device) # random testing locations y = torch.cat([f(l,x)[...,None,:] for l in range(num_tasks)],-2) # true values at random testing locations z = torch.rand((2**6,d),generator=rng).to(device) # other random locations at which to evaluate covariance print(\"x.shape = %s\"%str(tuple(x.shape))) print(\"y.shape = %s\"%str(tuple(y.shape))) print(\"z.shape = %s\"%str(tuple(z.shape))) <pre>x.shape = (128, 6)\ny.shape = (2, 3, 4, 5, 128)\nz.shape = (64, 6)\n</pre> In\u00a0[4]: Copied! <pre>fgp = fastgps.FastGPLattice(\n    qp.KernelMultiTask(\n        qp.KernelShiftInvar(\n            d = d,\n            shape_scale = shape_batch[:]+[1],\n            shape_lengthscales = shape_batch[1:]+[d],\n            torchify = True,\n            device = device),\n        num_tasks = num_tasks,\n        shape_factor = shape_batch[:]+[num_tasks,num_tasks],\n        shape_diag = shape_batch[1:]+[num_tasks]),\n    seqs = 7,\n    shape_noise = shape_batch[2:]+[1],\n)\nprint(\"fgp.kernel.base_kernel.scale.shape = %s\"%str(tuple(fgp.kernel.base_kernel.scale.shape)))\nprint(\"fgp.kernel.base_kernel.lengthscales.shape = %s\"%str(tuple(fgp.kernel.base_kernel.lengthscales.shape)))\nprint(\"fgp.kernel.factor.shape = %s\"%str(tuple(fgp.kernel.factor.shape)))\nprint(\"fgp.kernel.diag.shape = %s\"%str(tuple(fgp.kernel.diag.shape)))\nprint(\"fgp.noise.shape = %s\"%str(tuple(fgp.noise.shape)))\n</pre> fgp = fastgps.FastGPLattice(     qp.KernelMultiTask(         qp.KernelShiftInvar(             d = d,             shape_scale = shape_batch[:]+[1],             shape_lengthscales = shape_batch[1:]+[d],             torchify = True,             device = device),         num_tasks = num_tasks,         shape_factor = shape_batch[:]+[num_tasks,num_tasks],         shape_diag = shape_batch[1:]+[num_tasks]),     seqs = 7,     shape_noise = shape_batch[2:]+[1], ) print(\"fgp.kernel.base_kernel.scale.shape = %s\"%str(tuple(fgp.kernel.base_kernel.scale.shape))) print(\"fgp.kernel.base_kernel.lengthscales.shape = %s\"%str(tuple(fgp.kernel.base_kernel.lengthscales.shape))) print(\"fgp.kernel.factor.shape = %s\"%str(tuple(fgp.kernel.factor.shape))) print(\"fgp.kernel.diag.shape = %s\"%str(tuple(fgp.kernel.diag.shape))) print(\"fgp.noise.shape = %s\"%str(tuple(fgp.noise.shape))) <pre>fgp.kernel.base_kernel.scale.shape = (2, 3, 4, 1)\nfgp.kernel.base_kernel.lengthscales.shape = (3, 4, 6)\nfgp.kernel.factor.shape = (2, 3, 4, 5, 5)\nfgp.kernel.diag.shape = (3, 4, 5)\nfgp.noise.shape = (4, 1)\n</pre> In\u00a0[5]: Copied! <pre>x_next = fgp.get_x_next(n=2**torch.arange(num_tasks+1,1,-1,device=device))\ny_next = [f(l,x_next[l]) for l in range(num_tasks)]\nfgp.add_y_next(y_next)\nfor i in range(len(x_next)):  \n    print(\"i = %d\"%i)\n    print(\"\\tx_next[%d].shape = %s\"%(i,str(tuple(x_next[i].shape))))\n    print(\"\\ty_next[%d].shape = %s\"%(i,str(tuple(y_next[i].shape))))\n</pre> x_next = fgp.get_x_next(n=2**torch.arange(num_tasks+1,1,-1,device=device)) y_next = [f(l,x_next[l]) for l in range(num_tasks)] fgp.add_y_next(y_next) for i in range(len(x_next)):       print(\"i = %d\"%i)     print(\"\\tx_next[%d].shape = %s\"%(i,str(tuple(x_next[i].shape))))     print(\"\\ty_next[%d].shape = %s\"%(i,str(tuple(y_next[i].shape)))) <pre>i = 0\n\tx_next[0].shape = (64, 6)\n\ty_next[0].shape = (2, 3, 4, 64)\ni = 1\n\tx_next[1].shape = (32, 6)\n\ty_next[1].shape = (2, 3, 4, 32)\ni = 2\n\tx_next[2].shape = (16, 6)\n\ty_next[2].shape = (2, 3, 4, 16)\ni = 3\n\tx_next[3].shape = (8, 6)\n\ty_next[3].shape = (2, 3, 4, 8)\ni = 4\n\tx_next[4].shape = (4, 6)\n\ty_next[4].shape = (2, 3, 4, 4)\n</pre> In\u00a0[6]: Copied! <pre>pmean = fgp.post_mean(x)\nprint(\"pmean.shape = %s\"%str(tuple(pmean.shape)))\nl2rerror = torch.linalg.norm(y-pmean,dim=-1)/torch.linalg.norm(y,dim=-1)\nprint(\"l2rerror.shape = %s\"%str(tuple(l2rerror.shape)))\n</pre> pmean = fgp.post_mean(x) print(\"pmean.shape = %s\"%str(tuple(pmean.shape))) l2rerror = torch.linalg.norm(y-pmean,dim=-1)/torch.linalg.norm(y,dim=-1) print(\"l2rerror.shape = %s\"%str(tuple(l2rerror.shape))) <pre>pmean.shape = (2, 3, 4, 5, 128)\nl2rerror.shape = (2, 3, 4, 5)\n</pre> In\u00a0[7]: Copied! <pre>data = fgp.fit(stop_crit_improvement_threshold=1e3)\nlist(data.keys())\n</pre> data = fgp.fit(stop_crit_improvement_threshold=1e3) list(data.keys()) <pre>     iter of 5.0e+03 | best loss  | loss      \n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n            0.00e+00 | 1.21e+04   | 1.21e+04  \n            5.00e+00 | 8.88e+03   | 8.88e+03  \n            1.00e+01 | 8.61e+03   | 8.61e+03  \n            1.50e+01 | 8.53e+03   | 8.53e+03  \n            2.00e+01 | 8.45e+03   | 8.45e+03  \n            2.50e+01 | 8.40e+03   | 8.40e+03  \n            3.00e+01 | 8.36e+03   | 8.36e+03  \n            3.50e+01 | 8.32e+03   | 8.32e+03  \n            4.00e+01 | 8.29e+03   | 8.29e+03  \n            4.50e+01 | 8.26e+03   | 8.26e+03  \n            5.00e+01 | 8.24e+03   | 8.24e+03  \n            5.50e+01 | 8.22e+03   | 8.22e+03  \n            6.00e+01 | 8.20e+03   | 8.20e+03  \n            6.50e+01 | 8.18e+03   | 8.18e+03  \n            7.00e+01 | 8.17e+03   | 8.17e+03  \n            7.50e+01 | 8.16e+03   | 8.16e+03  \n            8.00e+01 | 8.15e+03   | 8.15e+03  \n            8.50e+01 | 8.15e+03   | 8.15e+03  \n            9.00e+01 | 8.14e+03   | 8.14e+03  \n            9.50e+01 | 8.13e+03   | 8.13e+03  \n            1.00e+02 | 8.12e+03   | 8.12e+03  \n            1.05e+02 | 8.12e+03   | 8.12e+03  \n            1.10e+02 | 8.12e+03   | 8.12e+03  \n            1.15e+02 | 8.11e+03   | 8.11e+03  \n            1.17e+02 | 8.11e+03   | 8.11e+03  \n</pre> Out[7]: <pre>[]</pre> In\u00a0[8]: Copied! <pre>pmean,pvar,q,ci_low,ci_high = fgp.post_ci(x,confidence=0.99)\nprint(\"pmean.shape = %s\"%str(tuple(pmean.shape)))\nprint(\"pvar.shape = %s\"%str(tuple(pvar.shape)))\nprint(\"q = %.2f\"%q)\nprint(\"ci_low.shape = %s\"%str(tuple(ci_low.shape)))\nprint(\"ci_high.shape = %s\"%str(tuple(ci_high.shape)))\nl2rerror = torch.linalg.norm(y-pmean,dim=-1)/torch.linalg.norm(y,dim=-1)\nprint(\"l2rerror.shape = %s\"%str(tuple(l2rerror.shape)))\npcov = fgp.post_cov(x,x)\nprint(\"pcov.shape = %s\"%str(tuple(pcov.shape)))\n_range0,_rangen1 = torch.arange(pcov.size(-3)),torch.arange(pcov.size(-1))\npcov2 = fgp.post_cov(x,z)\nprint(\"pcov2.shape = %s\"%str(tuple(pcov2.shape)))\nprint(\"\\npcov diag matches pvar: %s\"%torch.allclose(pcov[...,_range0,_range0,:,:][...,_rangen1,_rangen1],pvar))\nprint(\"non-negative pvar: %s\"%(pvar&gt;=0).all().item())\n</pre> pmean,pvar,q,ci_low,ci_high = fgp.post_ci(x,confidence=0.99) print(\"pmean.shape = %s\"%str(tuple(pmean.shape))) print(\"pvar.shape = %s\"%str(tuple(pvar.shape))) print(\"q = %.2f\"%q) print(\"ci_low.shape = %s\"%str(tuple(ci_low.shape))) print(\"ci_high.shape = %s\"%str(tuple(ci_high.shape))) l2rerror = torch.linalg.norm(y-pmean,dim=-1)/torch.linalg.norm(y,dim=-1) print(\"l2rerror.shape = %s\"%str(tuple(l2rerror.shape))) pcov = fgp.post_cov(x,x) print(\"pcov.shape = %s\"%str(tuple(pcov.shape))) _range0,_rangen1 = torch.arange(pcov.size(-3)),torch.arange(pcov.size(-1)) pcov2 = fgp.post_cov(x,z) print(\"pcov2.shape = %s\"%str(tuple(pcov2.shape))) print(\"\\npcov diag matches pvar: %s\"%torch.allclose(pcov[...,_range0,_range0,:,:][...,_rangen1,_rangen1],pvar)) print(\"non-negative pvar: %s\"%(pvar&gt;=0).all().item()) <pre>pmean.shape = (2, 3, 4, 5, 128)\npvar.shape = (2, 3, 4, 5, 128)\nq = 2.58\nci_low.shape = (2, 3, 4, 5, 128)\nci_high.shape = (2, 3, 4, 5, 128)\nl2rerror.shape = (2, 3, 4, 5)\npcov.shape = (2, 3, 4, 5, 5, 128, 128)\npcov2.shape = (2, 3, 4, 5, 5, 128, 64)\n\npcov diag matches pvar: True\nnon-negative pvar: True\n</pre> In\u00a0[9]: Copied! <pre>pcmean,pcvar,q,cci_low,cci_high = fgp.post_cubature_ci(confidence=0.99)\nprint(\"pcmean.shape = %s\"%str(tuple(pcmean.shape)))\nprint(\"pcvar.shape = %s\"%str(tuple(pcvar.shape)))\nprint(\"cci_low.shape = %s\"%str(tuple(cci_low.shape)))\nprint(\"cci_high.shape = %s\"%str(tuple(cci_high.shape)))\npccov = fgp.post_cubature_cov()\nprint(\"pccov.shape = %s\"%str(tuple(pccov.shape)))\n</pre> pcmean,pcvar,q,cci_low,cci_high = fgp.post_cubature_ci(confidence=0.99) print(\"pcmean.shape = %s\"%str(tuple(pcmean.shape))) print(\"pcvar.shape = %s\"%str(tuple(pcvar.shape))) print(\"cci_low.shape = %s\"%str(tuple(cci_low.shape))) print(\"cci_high.shape = %s\"%str(tuple(cci_high.shape))) pccov = fgp.post_cubature_cov() print(\"pccov.shape = %s\"%str(tuple(pccov.shape))) <pre>pcmean.shape = (2, 3, 4, 5)\npcvar.shape = (2, 3, 4, 5)\ncci_low.shape = (2, 3, 4, 5)\ncci_high.shape = (2, 3, 4, 5)\npccov.shape = (2, 3, 4, 5, 5)\n</pre> In\u00a0[10]: Copied! <pre>n_new = fgp.n*2\npcov_future = fgp.post_cov(x,z,n=n_new)\npvar_future = fgp.post_var(x,n=n_new)\npcvar_future = fgp.post_cubature_var(n=n_new)\n</pre> n_new = fgp.n*2 pcov_future = fgp.post_cov(x,z,n=n_new) pvar_future = fgp.post_var(x,n=n_new) pcvar_future = fgp.post_cubature_var(n=n_new) In\u00a0[11]: Copied! <pre>x_next = fgp.get_x_next(n_new)\ny_next = [f(l,x_next[l]) for l in range(num_tasks)]\nfor _y in y_next:\n    print(_y.shape)\nfgp.add_y_next(y_next)\nl2rerror = torch.linalg.norm(y-fgp.post_mean(x),dim=-1)/torch.linalg.norm(y,dim=-1)\nprint(\"l2rerror.shape = %s\"%str(tuple(l2rerror.shape)))\nassert torch.allclose(fgp.post_cov(x,z),pcov_future)\nassert torch.allclose(fgp.post_var(x),pvar_future)\nassert torch.allclose(fgp.post_cubature_var(),pcvar_future)\n</pre> x_next = fgp.get_x_next(n_new) y_next = [f(l,x_next[l]) for l in range(num_tasks)] for _y in y_next:     print(_y.shape) fgp.add_y_next(y_next) l2rerror = torch.linalg.norm(y-fgp.post_mean(x),dim=-1)/torch.linalg.norm(y,dim=-1) print(\"l2rerror.shape = %s\"%str(tuple(l2rerror.shape))) assert torch.allclose(fgp.post_cov(x,z),pcov_future) assert torch.allclose(fgp.post_var(x),pvar_future) assert torch.allclose(fgp.post_cubature_var(),pcvar_future) <pre>torch.Size([2, 3, 4, 64])\ntorch.Size([2, 3, 4, 32])\ntorch.Size([2, 3, 4, 16])\ntorch.Size([2, 3, 4, 8])\ntorch.Size([2, 3, 4, 4])\nl2rerror.shape = (2, 3, 4, 5)\n</pre> In\u00a0[12]: Copied! <pre>data = fgp.fit(iterations=5,verbose=False)\nl2rerror = torch.linalg.norm(y-fgp.post_mean(x),dim=-1)/torch.linalg.norm(y,dim=-1)\nprint(\"l2rerror.shape = %s\"%str(tuple(l2rerror.shape)))\n</pre> data = fgp.fit(iterations=5,verbose=False) l2rerror = torch.linalg.norm(y-fgp.post_mean(x),dim=-1)/torch.linalg.norm(y,dim=-1) print(\"l2rerror.shape = %s\"%str(tuple(l2rerror.shape))) <pre>l2rerror.shape = (2, 3, 4, 5)\n</pre> In\u00a0[13]: Copied! <pre>n_new = fgp.n*2\npcov_new = fgp.post_cov(x,z,n=n_new)\npvar_new = fgp.post_var(x,n=n_new)\npcvar_new = fgp.post_cubature_var(n=n_new)\nx_next = fgp.get_x_next(n_new)\ny_next = [f(l,x_next[l]) for l in range(num_tasks)]\nfgp.add_y_next(y_next)\nassert torch.allclose(fgp.post_cov(x,z),pcov_new)\nassert torch.allclose(fgp.post_var(x),pvar_new)\nassert torch.allclose(fgp.post_cubature_var(),pcvar_new)\n</pre> n_new = fgp.n*2 pcov_new = fgp.post_cov(x,z,n=n_new) pvar_new = fgp.post_var(x,n=n_new) pcvar_new = fgp.post_cubature_var(n=n_new) x_next = fgp.get_x_next(n_new) y_next = [f(l,x_next[l]) for l in range(num_tasks)] fgp.add_y_next(y_next) assert torch.allclose(fgp.post_cov(x,z),pcov_new) assert torch.allclose(fgp.post_var(x),pvar_new) assert torch.allclose(fgp.post_cubature_var(),pcvar_new) In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/batch_multitask/fgp_lattice/#fast-batch-multitask-lattice-gp","title":"Fast Batch Multitask Lattice GP\u00b6","text":""},{"location":"examples/batch_multitask/fgp_lattice/#true-function","title":"True Function\u00b6","text":""},{"location":"examples/batch_multitask/fgp_lattice/#construct-fast-gp","title":"Construct Fast GP\u00b6","text":""},{"location":"examples/batch_multitask/fgp_lattice/#project-and-increase-sample-size","title":"Project and Increase Sample Size\u00b6","text":""},{"location":"examples/batch_multitask/standard_gp/","title":"Standard GP","text":"In\u00a0[1]: Copied! <pre>import fastgps\nimport qmcpy as qp\nimport torch\nimport numpy as np\n</pre> import fastgps import qmcpy as qp import torch import numpy as np In\u00a0[2]: Copied! <pre>device = \"cpu\"\nif device!=\"mps\":\n    torch.set_default_dtype(torch.float64)\n</pre> device = \"cpu\" if device!=\"mps\":     torch.set_default_dtype(torch.float64) In\u00a0[3]: Copied! <pre>d = 6\nrng = torch.Generator().manual_seed(7)\nshape_batch = [2,3,4]\nnum_tasks = 5 \ndef f(l, x):\n    consts = torch.arange(torch.prod(torch.tensor(shape_batch)),device=device).reshape(shape_batch)\n    y = (consts[...,None,None]*x**torch.arange(1,d+1,device=device)).sum(-1)+torch.randn(shape_batch+[x.size(0)],generator=rng).to(device)/(3+l)\n    return y\nx = torch.rand((2**7,d),generator=rng).to(device) # random testing locations\ny = torch.cat([f(l,x)[...,None,:] for l in range(num_tasks)],-2) # true values at random testing locations\nz = torch.rand((2**6,d),generator=rng).to(device) # other random locations at which to evaluate covariance\nprint(\"x.shape = %s\"%str(tuple(x.shape)))\nprint(\"y.shape = %s\"%str(tuple(y.shape)))\nprint(\"z.shape = %s\"%str(tuple(z.shape)))\n</pre> d = 6 rng = torch.Generator().manual_seed(7) shape_batch = [2,3,4] num_tasks = 5  def f(l, x):     consts = torch.arange(torch.prod(torch.tensor(shape_batch)),device=device).reshape(shape_batch)     y = (consts[...,None,None]*x**torch.arange(1,d+1,device=device)).sum(-1)+torch.randn(shape_batch+[x.size(0)],generator=rng).to(device)/(3+l)     return y x = torch.rand((2**7,d),generator=rng).to(device) # random testing locations y = torch.cat([f(l,x)[...,None,:] for l in range(num_tasks)],-2) # true values at random testing locations z = torch.rand((2**6,d),generator=rng).to(device) # other random locations at which to evaluate covariance print(\"x.shape = %s\"%str(tuple(x.shape))) print(\"y.shape = %s\"%str(tuple(y.shape))) print(\"z.shape = %s\"%str(tuple(z.shape))) <pre>x.shape = (128, 6)\ny.shape = (2, 3, 4, 5, 128)\nz.shape = (64, 6)\n</pre> In\u00a0[4]: Copied! <pre>fgp = fastgps.StandardGP(qp.KernelMultiTask(\n        qp.KernelSquaredExponential(\n            d = d,\n            shape_scale = shape_batch[:]+[1],\n            shape_lengthscales = shape_batch[1:]+[d],\n            torchify = True,\n            device = device),\n        num_tasks = num_tasks,\n        shape_factor = shape_batch[:]+[num_tasks,num_tasks],\n        shape_diag = shape_batch[1:]+[num_tasks]),\n    seqs = 7,\n    shape_noise = shape_batch[2:]+[1],\n)\nprint(\"fgp.kernel.base_kernel.scale.shape = %s\"%str(tuple(fgp.kernel.base_kernel.scale.shape)))\nprint(\"fgp.kernel.base_kernel.lengthscales.shape = %s\"%str(tuple(fgp.kernel.base_kernel.lengthscales.shape)))\nprint(\"fgp.kernel.factor.shape = %s\"%str(tuple(fgp.kernel.factor.shape)))\nprint(\"fgp.kernel.diag.shape = %s\"%str(tuple(fgp.kernel.diag.shape)))\nprint(\"fgp.noise.shape = %s\"%str(tuple(fgp.noise.shape)))\n</pre> fgp = fastgps.StandardGP(qp.KernelMultiTask(         qp.KernelSquaredExponential(             d = d,             shape_scale = shape_batch[:]+[1],             shape_lengthscales = shape_batch[1:]+[d],             torchify = True,             device = device),         num_tasks = num_tasks,         shape_factor = shape_batch[:]+[num_tasks,num_tasks],         shape_diag = shape_batch[1:]+[num_tasks]),     seqs = 7,     shape_noise = shape_batch[2:]+[1], ) print(\"fgp.kernel.base_kernel.scale.shape = %s\"%str(tuple(fgp.kernel.base_kernel.scale.shape))) print(\"fgp.kernel.base_kernel.lengthscales.shape = %s\"%str(tuple(fgp.kernel.base_kernel.lengthscales.shape))) print(\"fgp.kernel.factor.shape = %s\"%str(tuple(fgp.kernel.factor.shape))) print(\"fgp.kernel.diag.shape = %s\"%str(tuple(fgp.kernel.diag.shape))) print(\"fgp.noise.shape = %s\"%str(tuple(fgp.noise.shape))) <pre>fgp.kernel.base_kernel.scale.shape = (2, 3, 4, 1)\nfgp.kernel.base_kernel.lengthscales.shape = (3, 4, 6)\nfgp.kernel.factor.shape = (2, 3, 4, 5, 5)\nfgp.kernel.diag.shape = (3, 4, 5)\nfgp.noise.shape = (4, 1)\n</pre> In\u00a0[5]: Copied! <pre>x_next = fgp.get_x_next(n=2**torch.arange(num_tasks-1,-1,-1,device=device))\ny_next = [f(l,x_next[l]) for l in range(num_tasks)]\nfgp.add_y_next(y_next)\nfor i in range(len(x_next)):  \n    print(\"i = %d\"%i)\n    print(\"\\tx_next[%d].shape = %s\"%(i,str(tuple(x_next[i].shape))))\n    print(\"\\ty_next[%d].shape = %s\"%(i,str(tuple(y_next[i].shape))))\n</pre> x_next = fgp.get_x_next(n=2**torch.arange(num_tasks-1,-1,-1,device=device)) y_next = [f(l,x_next[l]) for l in range(num_tasks)] fgp.add_y_next(y_next) for i in range(len(x_next)):       print(\"i = %d\"%i)     print(\"\\tx_next[%d].shape = %s\"%(i,str(tuple(x_next[i].shape))))     print(\"\\ty_next[%d].shape = %s\"%(i,str(tuple(y_next[i].shape)))) <pre>i = 0\n\tx_next[0].shape = (16, 6)\n\ty_next[0].shape = (2, 3, 4, 16)\ni = 1\n\tx_next[1].shape = (8, 6)\n\ty_next[1].shape = (2, 3, 4, 8)\ni = 2\n\tx_next[2].shape = (4, 6)\n\ty_next[2].shape = (2, 3, 4, 4)\ni = 3\n\tx_next[3].shape = (2, 6)\n\ty_next[3].shape = (2, 3, 4, 2)\ni = 4\n\tx_next[4].shape = (1, 6)\n\ty_next[4].shape = (2, 3, 4, 1)\n</pre> In\u00a0[6]: Copied! <pre>pmean = fgp.post_mean(x)\nprint(\"pmean.shape = %s\"%str(tuple(pmean.shape)))\nl2rerror = torch.linalg.norm(y-pmean,dim=-1)/torch.linalg.norm(y,dim=-1)\nprint(\"l2rerror.shape = %s\"%str(tuple(l2rerror.shape)))\n</pre> pmean = fgp.post_mean(x) print(\"pmean.shape = %s\"%str(tuple(pmean.shape))) l2rerror = torch.linalg.norm(y-pmean,dim=-1)/torch.linalg.norm(y,dim=-1) print(\"l2rerror.shape = %s\"%str(tuple(l2rerror.shape))) <pre>pmean.shape = (2, 3, 4, 5, 128)\nl2rerror.shape = (2, 3, 4, 5)\n</pre> In\u00a0[7]: Copied! <pre>data = fgp.fit(stop_crit_improvement_threshold=1e3)\nlist(data.keys())\n</pre> data = fgp.fit(stop_crit_improvement_threshold=1e3) list(data.keys()) <pre>     iter of 5.0e+03 | best loss  | loss      \n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n            0.00e+00 | 9.03e+03   | 9.03e+03  \n            5.00e+00 | 2.10e+03   | 2.10e+03  \n            1.00e+01 | 1.81e+03   | 1.81e+03  \n            1.50e+01 | 1.65e+03   | 1.65e+03  \n            2.00e+01 | 1.47e+03   | 1.47e+03  \n            2.50e+01 | 1.41e+03   | 1.43e+03  \n            3.00e+01 | 1.32e+03   | 1.32e+03  \n            3.50e+01 | 1.32e+03   | 1.32e+03  \n            4.00e+01 | 1.23e+03   | 1.23e+03  \n            4.50e+01 | 1.23e+03   | 1.24e+03  \n            5.00e+01 | 1.16e+03   | 1.16e+03  \n            5.50e+01 | 1.16e+03   | 1.16e+03  \n            6.00e+01 | 1.11e+03   | 1.14e+03  \n            6.50e+01 | 1.10e+03   | 1.10e+03  \n            6.70e+01 | 1.10e+03   | 1.12e+03  \n</pre> Out[7]: <pre>[]</pre> In\u00a0[8]: Copied! <pre>pmean,pvar,q,ci_low,ci_high = fgp.post_ci(x,confidence=0.99)\nprint(\"pmean.shape = %s\"%str(tuple(pmean.shape)))\nprint(\"pvar.shape = %s\"%str(tuple(pvar.shape)))\nprint(\"q = %.2f\"%q)\nprint(\"ci_low.shape = %s\"%str(tuple(ci_low.shape)))\nprint(\"ci_high.shape = %s\"%str(tuple(ci_high.shape)))\nl2rerror = torch.linalg.norm(y-pmean,dim=-1)/torch.linalg.norm(y,dim=-1)\nprint(\"l2rerror.shape = %s\"%str(tuple(l2rerror.shape)))\npcov = fgp.post_cov(x,x)\nprint(\"pcov.shape = %s\"%str(tuple(pcov.shape)))\n_range0,_rangen1 = torch.arange(pcov.size(-3)),torch.arange(pcov.size(-1))\npcov2 = fgp.post_cov(x,z)\nprint(\"pcov2.shape = %s\"%str(tuple(pcov2.shape)))\nprint(\"\\npcov diag matches pvar: %s\"%torch.allclose(pcov[...,_range0,_range0,:,:][...,_rangen1,_rangen1],pvar))\nprint(\"non-negative pvar: %s\"%(pvar&gt;=0).all().item())\n</pre> pmean,pvar,q,ci_low,ci_high = fgp.post_ci(x,confidence=0.99) print(\"pmean.shape = %s\"%str(tuple(pmean.shape))) print(\"pvar.shape = %s\"%str(tuple(pvar.shape))) print(\"q = %.2f\"%q) print(\"ci_low.shape = %s\"%str(tuple(ci_low.shape))) print(\"ci_high.shape = %s\"%str(tuple(ci_high.shape))) l2rerror = torch.linalg.norm(y-pmean,dim=-1)/torch.linalg.norm(y,dim=-1) print(\"l2rerror.shape = %s\"%str(tuple(l2rerror.shape))) pcov = fgp.post_cov(x,x) print(\"pcov.shape = %s\"%str(tuple(pcov.shape))) _range0,_rangen1 = torch.arange(pcov.size(-3)),torch.arange(pcov.size(-1)) pcov2 = fgp.post_cov(x,z) print(\"pcov2.shape = %s\"%str(tuple(pcov2.shape))) print(\"\\npcov diag matches pvar: %s\"%torch.allclose(pcov[...,_range0,_range0,:,:][...,_rangen1,_rangen1],pvar)) print(\"non-negative pvar: %s\"%(pvar&gt;=0).all().item()) <pre>pmean.shape = (2, 3, 4, 5, 128)\npvar.shape = (2, 3, 4, 5, 128)\nq = 2.58\nci_low.shape = (2, 3, 4, 5, 128)\nci_high.shape = (2, 3, 4, 5, 128)\nl2rerror.shape = (2, 3, 4, 5)\npcov.shape = (2, 3, 4, 5, 5, 128, 128)\npcov2.shape = (2, 3, 4, 5, 5, 128, 64)\n\npcov diag matches pvar: True\nnon-negative pvar: True\n</pre> In\u00a0[9]: Copied! <pre>pcmean,pcvar,q,cci_low,cci_high = fgp.post_cubature_ci(confidence=0.99)\nprint(\"pcmean.shape = %s\"%str(tuple(pcmean.shape)))\nprint(\"pcvar.shape = %s\"%str(tuple(pcvar.shape)))\nprint(\"cci_low.shape = %s\"%str(tuple(cci_low.shape)))\nprint(\"cci_high.shape = %s\"%str(tuple(cci_high.shape)))\npccov = fgp.post_cubature_cov()\nprint(\"pccov.shape = %s\"%str(tuple(pccov.shape)))\n</pre> pcmean,pcvar,q,cci_low,cci_high = fgp.post_cubature_ci(confidence=0.99) print(\"pcmean.shape = %s\"%str(tuple(pcmean.shape))) print(\"pcvar.shape = %s\"%str(tuple(pcvar.shape))) print(\"cci_low.shape = %s\"%str(tuple(cci_low.shape))) print(\"cci_high.shape = %s\"%str(tuple(cci_high.shape))) pccov = fgp.post_cubature_cov() print(\"pccov.shape = %s\"%str(tuple(pccov.shape))) <pre>pcmean.shape = (2, 3, 4, 5)\npcvar.shape = (2, 3, 4, 5)\ncci_low.shape = (2, 3, 4, 5)\ncci_high.shape = (2, 3, 4, 5)\npccov.shape = (2, 3, 4, 5, 5)\n</pre> In\u00a0[10]: Copied! <pre>n_new = fgp.n*2\npcov_future = fgp.post_cov(x,z,n=n_new)\npvar_future = fgp.post_var(x,n=n_new)\npcvar_future = fgp.post_cubature_var(n=n_new)\n</pre> n_new = fgp.n*2 pcov_future = fgp.post_cov(x,z,n=n_new) pvar_future = fgp.post_var(x,n=n_new) pcvar_future = fgp.post_cubature_var(n=n_new) In\u00a0[11]: Copied! <pre>x_next = fgp.get_x_next(n_new)\ny_next = [f(l,x_next[l]) for l in range(num_tasks)]\nfor _y in y_next:\n    print(_y.shape)\nfgp.add_y_next(y_next)\nl2rerror = torch.linalg.norm(y-fgp.post_mean(x),dim=-1)/torch.linalg.norm(y,dim=-1)\nprint(\"l2rerror.shape = %s\"%str(tuple(l2rerror.shape)))\nassert torch.allclose(fgp.post_cov(x,z),pcov_future)\nassert torch.allclose(fgp.post_var(x),pvar_future)\nassert torch.allclose(fgp.post_cubature_var(),pcvar_future)\n</pre> x_next = fgp.get_x_next(n_new) y_next = [f(l,x_next[l]) for l in range(num_tasks)] for _y in y_next:     print(_y.shape) fgp.add_y_next(y_next) l2rerror = torch.linalg.norm(y-fgp.post_mean(x),dim=-1)/torch.linalg.norm(y,dim=-1) print(\"l2rerror.shape = %s\"%str(tuple(l2rerror.shape))) assert torch.allclose(fgp.post_cov(x,z),pcov_future) assert torch.allclose(fgp.post_var(x),pvar_future) assert torch.allclose(fgp.post_cubature_var(),pcvar_future) <pre>torch.Size([2, 3, 4, 16])\ntorch.Size([2, 3, 4, 8])\ntorch.Size([2, 3, 4, 4])\ntorch.Size([2, 3, 4, 2])\ntorch.Size([2, 3, 4, 1])\nl2rerror.shape = (2, 3, 4, 5)\n</pre> In\u00a0[12]: Copied! <pre>data = fgp.fit(iterations=5,verbose=False)\nl2rerror = torch.linalg.norm(y-fgp.post_mean(x),dim=-1)/torch.linalg.norm(y,dim=-1)\nprint(\"l2rerror.shape = %s\"%str(tuple(l2rerror.shape)))\n</pre> data = fgp.fit(iterations=5,verbose=False) l2rerror = torch.linalg.norm(y-fgp.post_mean(x),dim=-1)/torch.linalg.norm(y,dim=-1) print(\"l2rerror.shape = %s\"%str(tuple(l2rerror.shape))) <pre>l2rerror.shape = (2, 3, 4, 5)\n</pre> In\u00a0[13]: Copied! <pre>n_new = fgp.n*2\npcov_new = fgp.post_cov(x,z,n=n_new)\npvar_new = fgp.post_var(x,n=n_new)\npcvar_new = fgp.post_cubature_var(n=n_new)\nx_next = fgp.get_x_next(n_new)\ny_next = [f(l,x_next[l]) for l in range(num_tasks)]\nfgp.add_y_next(y_next)\nassert torch.allclose(fgp.post_cov(x,z),pcov_new)\nassert torch.allclose(fgp.post_var(x),pvar_new)\nassert torch.allclose(fgp.post_cubature_var(),pcvar_new)\n</pre> n_new = fgp.n*2 pcov_new = fgp.post_cov(x,z,n=n_new) pvar_new = fgp.post_var(x,n=n_new) pcvar_new = fgp.post_cubature_var(n=n_new) x_next = fgp.get_x_next(n_new) y_next = [f(l,x_next[l]) for l in range(num_tasks)] fgp.add_y_next(y_next) assert torch.allclose(fgp.post_cov(x,z),pcov_new) assert torch.allclose(fgp.post_var(x),pvar_new) assert torch.allclose(fgp.post_cubature_var(),pcvar_new) In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/batch_multitask/standard_gp/#standard-batch-multitask-gp","title":"Standard Batch Multitask GP\u00b6","text":""},{"location":"examples/batch_multitask/standard_gp/#true-function","title":"True Function\u00b6","text":""},{"location":"examples/batch_multitask/standard_gp/#construct-fast-gp","title":"Construct Fast GP\u00b6","text":""},{"location":"examples/batch_multitask/standard_gp/#project-and-increase-sample-size","title":"Project and Increase Sample Size\u00b6","text":""},{"location":"examples/derivative_informed/compare_gps_plot/","title":"Compare GPs","text":"In\u00a0[1]: Copied! <pre>import fastgps\nimport qmcpy as qp\nimport numpy as np\nimport torch\nimport pandas as pd\nfrom matplotlib import pyplot\n</pre> import fastgps import qmcpy as qp import numpy as np import torch import pandas as pd from matplotlib import pyplot In\u00a0[2]: Copied! <pre>device = \"cpu\"\nif device!=\"mps\":\n    torch.set_default_dtype(torch.float64)\n</pre> device = \"cpu\" if device!=\"mps\":     torch.set_default_dtype(torch.float64) In\u00a0[3]: Copied! <pre>colors = [\"xkcd:\"+color[:-1] for color in pd.read_csv(\"../../../xkcd_colors.txt\",comment=\"#\").iloc[:,0].tolist()][::-1]\n_alpha = 0.25\nWIDTH = 2*(500/72)\nLINEWIDTH = 3\nMARKERSIZE = 100\n</pre> colors = [\"xkcd:\"+color[:-1] for color in pd.read_csv(\"../../../xkcd_colors.txt\",comment=\"#\").iloc[:,0].tolist()][::-1] _alpha = 0.25 WIDTH = 2*(500/72) LINEWIDTH = 3 MARKERSIZE = 100 In\u00a0[4]: Copied! <pre>d = 1 \nf_smooth = lambda x: 15*(x[:,0]-1/2)**2*torch.sin(2*torch.pi*x[:,0])\ndef f_ackley(x, a=20, b=0.2, c=2*np.pi, scaling=32.768): # https://www.sfu.ca/~ssurjano/ackley.html\n    x = 2*scaling*x-scaling\n    t1 = a*torch.exp(-b*torch.sqrt(torch.mean(x**2,1)))\n    t2 = torch.exp(torch.mean(torch.cos(c*x),1))\n    t3 = a+np.exp(1)\n    y = -t1-t2+t3\n    return y\nf = f_smooth\ndef fp(x):\n    xg = x.clone().requires_grad_(True)\n    yg = f(xg)\n    yp = torch.autograd.grad(yg,xg,grad_outputs=torch.ones_like(yg))[0]\n    return yp[:,0].detach()\n</pre> d = 1  f_smooth = lambda x: 15*(x[:,0]-1/2)**2*torch.sin(2*torch.pi*x[:,0]) def f_ackley(x, a=20, b=0.2, c=2*np.pi, scaling=32.768): # https://www.sfu.ca/~ssurjano/ackley.html     x = 2*scaling*x-scaling     t1 = a*torch.exp(-b*torch.sqrt(torch.mean(x**2,1)))     t2 = torch.exp(torch.mean(torch.cos(c*x),1))     t3 = a+np.exp(1)     y = -t1-t2+t3     return y f = f_smooth def fp(x):     xg = x.clone().requires_grad_(True)     yg = f(xg)     yp = torch.autograd.grad(yg,xg,grad_outputs=torch.ones_like(yg))[0]     return yp[:,0].detach() In\u00a0[5]: Copied! <pre>xticks = torch.linspace(0,1,252,device=device)[1:-1,None]\nyticks = f(xticks)\nypticks = fp(xticks)\nprint(\"xticks.shape = %s\"%str(tuple(xticks.shape)))\nprint(\"yticks.shape = %s\"%str(tuple(yticks.shape)))\nprint(\"ypticks.shape = %s\"%str(tuple(ypticks.shape)))\n</pre> xticks = torch.linspace(0,1,252,device=device)[1:-1,None] yticks = f(xticks) ypticks = fp(xticks) print(\"xticks.shape = %s\"%str(tuple(xticks.shape))) print(\"yticks.shape = %s\"%str(tuple(yticks.shape))) print(\"ypticks.shape = %s\"%str(tuple(ypticks.shape))) <pre>xticks.shape = (250, 1)\nyticks.shape = (250,)\nypticks.shape = (250,)\n</pre> In\u00a0[6]: Copied! <pre>gps = [\n    fastgps.StandardGP(qp.KernelGaussian(1,torchify=True,device=device),seqs=qp.DigitalNetB2(1,seed=11,randomize=\"DS\")),\n    fastgps.FastGPDigitalNetB2(qp.KernelDigShiftInvar(1,torchify=True,device=device),seqs=qp.DigitalNetB2(1,seed=7,randomize=\"DS\")),\n    fastgps.FastGPLattice(qp.KernelShiftInvar(1,torchify=True,device=device),seqs=qp.Lattice(1,seed=7)),\n]\n</pre> gps = [     fastgps.StandardGP(qp.KernelGaussian(1,torchify=True,device=device),seqs=qp.DigitalNetB2(1,seed=11,randomize=\"DS\")),     fastgps.FastGPDigitalNetB2(qp.KernelDigShiftInvar(1,torchify=True,device=device),seqs=qp.DigitalNetB2(1,seed=7,randomize=\"DS\")),     fastgps.FastGPLattice(qp.KernelShiftInvar(1,torchify=True,device=device),seqs=qp.Lattice(1,seed=7)), ] In\u00a0[7]: Copied! <pre>gps_grad = [\n    fastgps.StandardGP(\n        qp.KernelMultiTaskDerivs(qp.KernelGaussian(1,torchify=True,device=device),num_tasks=2),\n        seqs = [qp.DigitalNetB2(1,seed=7,randomize=\"DS\"),qp.DigitalNetB2(1,seed=11,randomize=\"DS\")],\n        derivatives = [torch.tensor([0],device=device),torch.tensor([1],device=device)],\n    ),\n    fastgps.FastGPDigitalNetB2(\n        qp.KernelMultiTaskDerivs(qp.KernelDigShiftInvar(1,torchify=True,alpha=4,device=device),num_tasks=2),\n        seqs = [qp.DigitalNetB2(1,seed=7,randomize=\"DS\"),qp.DigitalNetB2(1,seed=11,randomize=\"DS\")],\n        derivatives = [torch.tensor([0],device=device),torch.tensor([1],device=device)],\n    ),\n    fastgps.FastGPLattice(\n        qp.KernelMultiTaskDerivs(qp.KernelShiftInvar(1,torchify=True,alpha=4,device=device),num_tasks=2),\n        seqs = [qp.Lattice(1,seed=7),qp.Lattice(1,seed=11)],\n        derivatives = [torch.tensor([0],device=device),torch.tensor([1],device=device)],\n    ),\n]\n</pre> gps_grad = [     fastgps.StandardGP(         qp.KernelMultiTaskDerivs(qp.KernelGaussian(1,torchify=True,device=device),num_tasks=2),         seqs = [qp.DigitalNetB2(1,seed=7,randomize=\"DS\"),qp.DigitalNetB2(1,seed=11,randomize=\"DS\")],         derivatives = [torch.tensor([0],device=device),torch.tensor([1],device=device)],     ),     fastgps.FastGPDigitalNetB2(         qp.KernelMultiTaskDerivs(qp.KernelDigShiftInvar(1,torchify=True,alpha=4,device=device),num_tasks=2),         seqs = [qp.DigitalNetB2(1,seed=7,randomize=\"DS\"),qp.DigitalNetB2(1,seed=11,randomize=\"DS\")],         derivatives = [torch.tensor([0],device=device),torch.tensor([1],device=device)],     ),     fastgps.FastGPLattice(         qp.KernelMultiTaskDerivs(qp.KernelShiftInvar(1,torchify=True,alpha=4,device=device),num_tasks=2),         seqs = [qp.Lattice(1,seed=7),qp.Lattice(1,seed=11)],         derivatives = [torch.tensor([0],device=device),torch.tensor([1],device=device)],     ), ] In\u00a0[8]: Copied! <pre>pmeans = [None]*len(gps)\npci_lows = [None]*len(gps)\npci_highs = [None]*len(gps)\nfor i,gp in enumerate(gps):\n    print(type(gp).__name__)\n    x_next = gp.get_x_next(n=2**2)\n    gp.add_y_next(f(x_next))\n    gp.fit()\n    pmeans[i],_,_,pci_lows[i],pci_highs[i] = gp.post_ci(xticks,confidence=0.95)\n    print(\"\\tl2 relative error = %.1e\"%(torch.linalg.norm(yticks-pmeans[i])/torch.linalg.norm(yticks)))\n</pre> pmeans = [None]*len(gps) pci_lows = [None]*len(gps) pci_highs = [None]*len(gps) for i,gp in enumerate(gps):     print(type(gp).__name__)     x_next = gp.get_x_next(n=2**2)     gp.add_y_next(f(x_next))     gp.fit()     pmeans[i],_,_,pci_lows[i],pci_highs[i] = gp.post_ci(xticks,confidence=0.95)     print(\"\\tl2 relative error = %.1e\"%(torch.linalg.norm(yticks-pmeans[i])/torch.linalg.norm(yticks)))  <pre>StandardGP\n     iter of 5.0e+03 | best loss  | loss      \n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n            0.00e+00 | 4.70e+02   | 4.70e+02  \n            5.00e+00 | 8.50e+00   | 8.50e+00  \n            1.00e+01 | 4.53e+00   | 4.53e+00  \n            1.50e+01 | 3.89e+00   | 4.03e+00  \n            2.00e+01 | 3.89e+00   | 3.90e+00  \n            2.40e+01 | 3.89e+00   | 3.89e+00  \n\tl2 relative error = 5.2e-01\nFastGPDigitalNetB2\n     iter of 5.0e+03 | best loss  | loss      \n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n            0.00e+00 | 5.76e+00   | 5.76e+00  \n            5.00e+00 | 5.17e+00   | 5.17e+00  \n            1.00e+01 | 5.09e+00   | 5.09e+00  \n            1.50e+01 | 4.91e+00   | 4.91e+00  \n            2.00e+01 | 4.74e+00   | 4.74e+00  \n            2.50e+01 | 4.43e+00   | 4.43e+00  \n            3.00e+01 | 4.30e+00   | 4.34e+00  \n            3.50e+01 | 4.26e+00   | 4.26e+00  \n            3.70e+01 | 4.26e+00   | 4.26e+00  \n\tl2 relative error = 4.9e-01\nFastGPLattice\n     iter of 5.0e+03 | best loss  | loss      \n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n            0.00e+00 | 6.23e+00   | 6.23e+00  \n            5.00e+00 | 5.74e+00   | 5.74e+00  \n            1.00e+01 | 5.60e+00   | 5.60e+00  \n            1.50e+01 | 5.33e+00   | 5.33e+00  \n            2.00e+01 | 4.66e+00   | 4.66e+00  \n            2.50e+01 | 4.05e+00   | 4.14e+00  \n            3.00e+01 | 3.90e+00   | 3.90e+00  \n            3.50e+01 | 3.61e+00   | 3.61e+00  \n            4.00e+01 | 3.38e+00   | 3.39e+00  \n            4.50e+01 | 3.32e+00   | 3.32e+00  \n            5.00e+01 | 3.31e+00   | 3.31e+00  \n            5.30e+01 | 3.31e+00   | 3.31e+00  \n\tl2 relative error = 4.9e-01\n</pre> In\u00a0[9]: Copied! <pre>pmeans_grad = [None]*len(gps_grad)\npci_lows_grad = [None]*len(gps_grad)\npci_highs_grad = [None]*len(gps_grad)\nfor i,gp in enumerate(gps_grad):\n    print(type(gp).__name__)\n    x_next = gp.get_x_next(n=[2**2,2**2])\n    gp.add_y_next([f(x_next[0]),fp(x_next[1])])\n    gp.fit()\n    pmeans_grad[i],_,_,pci_lows_grad[i],pci_highs_grad[i] = gp.post_ci(xticks,confidence=0.95)\n    print(\"\\tl2 relative error = %.1e\"%(torch.linalg.norm(yticks-pmeans_grad[i][0])/torch.linalg.norm(yticks)))\n</pre> pmeans_grad = [None]*len(gps_grad) pci_lows_grad = [None]*len(gps_grad) pci_highs_grad = [None]*len(gps_grad) for i,gp in enumerate(gps_grad):     print(type(gp).__name__)     x_next = gp.get_x_next(n=[2**2,2**2])     gp.add_y_next([f(x_next[0]),fp(x_next[1])])     gp.fit()     pmeans_grad[i],_,_,pci_lows_grad[i],pci_highs_grad[i] = gp.post_ci(xticks,confidence=0.95)     print(\"\\tl2 relative error = %.1e\"%(torch.linalg.norm(yticks-pmeans_grad[i][0])/torch.linalg.norm(yticks))) <pre>StandardGP\n     iter of 5.0e+03 | best loss  | loss      \n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n            0.00e+00 | 2.04e+05   | 2.04e+05  \n            5.00e+00 | 1.03e+03   | 1.03e+03  \n            1.00e+01 | 1.99e+01   | 2.17e+01  \n            1.50e+01 | 1.99e+01   | 1.99e+01  \n            1.80e+01 | 1.99e+01   | 1.99e+01  \n\tl2 relative error = 2.2e-01\nFastGPDigitalNetB2\n     iter of 5.0e+03 | best loss  | loss      \n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n            0.00e+00 | 2.74e+02   | 2.74e+02  \n            5.00e+00 | 1.22e+02   | 1.22e+02  \n            1.00e+01 | 3.70e+01   | 3.70e+01  \n            1.50e+01 | 2.68e+01   | 2.68e+01  \n            2.00e+01 | 2.51e+01   | 2.97e+01  \n            2.50e+01 | 2.51e+01   | 2.97e+01  \n            2.60e+01 | 2.51e+01   | 2.97e+01  \n\tl2 relative error = 6.0e+01\nFastGPLattice\n     iter of 5.0e+03 | best loss  | loss      \n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n            0.00e+00 | 2.43e+02   | 2.43e+02  \n            5.00e+00 | 6.49e+01   | 6.49e+01  \n            1.00e+01 | 2.52e+01   | 2.56e+01  \n            1.50e+01 | 2.51e+01   | 2.51e+01  \n            2.00e+01 | 2.49e+01   | 2.49e+01  \n            2.50e+01 | 2.47e+01   | 2.47e+01  \n            3.00e+01 | 2.42e+01   | 2.42e+01  \n            3.50e+01 | 2.29e+01   | 2.29e+01  \n            4.00e+01 | 2.15e+01   | 2.21e+01  \n            4.50e+01 | 2.11e+01   | 2.13e+01  \n            5.00e+01 | 2.11e+01   | 2.11e+01  \n            5.30e+01 | 2.11e+01   | 2.11e+01  \n\tl2 relative error = 1.2e-01\n</pre> In\u00a0[10]: Copied! <pre>fig,ax = pyplot.subplots(nrows=len(gps),ncols=3,sharex=True,sharey=False,figsize=(WIDTH*1.5,WIDTH/len(gps)*3))\nax = ax.reshape((len(gps),3))\nfor i,gp in enumerate(gps):\n    ax[i,0].set_ylabel(type(gp).__name__,fontsize=\"xx-large\")\n    ax[i,0].plot(xticks[:,0].cpu(),yticks.cpu(),color=\"k\",linewidth=LINEWIDTH)\n    ax[i,0].scatter(gp.x[:,0].cpu(),gp.y.cpu(),color=\"k\",s=MARKERSIZE)\n    ax[i,0].plot(xticks[:,0].cpu(),pmeans[i].cpu(),color=colors[i],linewidth=LINEWIDTH)\n    ax[i,0].fill_between(xticks[:,0].cpu(),pci_lows[i].cpu(),pci_highs[i].cpu(),color=colors[i],alpha=_alpha)\nfor i,gp in enumerate(gps_grad):\n    ax[i,1].plot(xticks[:,0].cpu(),yticks.cpu(),color=\"k\",linewidth=LINEWIDTH)\n    ax[i,2].plot(xticks[:,0].cpu(),ypticks.cpu(),color=\"k\",linewidth=LINEWIDTH)\n    ax[i,1].scatter(gp.x[0][:,0].cpu(),gp.y[0].cpu(),color=\"k\",s=MARKERSIZE)\n    ax[i,2].scatter(gp.x[1][:,0].cpu(),gp.y[1].cpu(),color=\"k\",s=MARKERSIZE)\n    ax[i,1].plot(xticks[:,0].cpu(),pmeans_grad[i][0].cpu(),color=colors[i],linewidth=LINEWIDTH)\n    ax[i,2].plot(xticks[:,0].cpu(),pmeans_grad[i][1].cpu(),color=colors[i],linewidth=LINEWIDTH)\n    ax[i,1].fill_between(xticks[:,0].cpu(),pci_lows_grad[i][0].cpu(),pci_highs_grad[i][0].cpu(),color=colors[i],alpha=_alpha)\n    ax[i,2].fill_between(xticks[:,0].cpu(),pci_lows_grad[i][1].cpu(),pci_highs_grad[i][1].cpu(),color=colors[i],alpha=_alpha)\nax[0,0].set_title(r\"$f$ no grad\",fontsize=\"xx-large\")\nax[0,1].set_title(r\"$f$ with grad\",fontsize=\"xx-large\")\nax[0,2].set_title(r\"$\\mathrm{d} f / \\mathrm{d} x$ with grad\",fontsize=\"xx-large\")\nfor j in range(3):\n    ax[-1,j].set_xlabel(r\"$x$\",fontsize=\"xx-large\")\nfig.savefig(\"./gps_deriv.pdf\",bbox_inches=\"tight\")\n</pre> fig,ax = pyplot.subplots(nrows=len(gps),ncols=3,sharex=True,sharey=False,figsize=(WIDTH*1.5,WIDTH/len(gps)*3)) ax = ax.reshape((len(gps),3)) for i,gp in enumerate(gps):     ax[i,0].set_ylabel(type(gp).__name__,fontsize=\"xx-large\")     ax[i,0].plot(xticks[:,0].cpu(),yticks.cpu(),color=\"k\",linewidth=LINEWIDTH)     ax[i,0].scatter(gp.x[:,0].cpu(),gp.y.cpu(),color=\"k\",s=MARKERSIZE)     ax[i,0].plot(xticks[:,0].cpu(),pmeans[i].cpu(),color=colors[i],linewidth=LINEWIDTH)     ax[i,0].fill_between(xticks[:,0].cpu(),pci_lows[i].cpu(),pci_highs[i].cpu(),color=colors[i],alpha=_alpha) for i,gp in enumerate(gps_grad):     ax[i,1].plot(xticks[:,0].cpu(),yticks.cpu(),color=\"k\",linewidth=LINEWIDTH)     ax[i,2].plot(xticks[:,0].cpu(),ypticks.cpu(),color=\"k\",linewidth=LINEWIDTH)     ax[i,1].scatter(gp.x[0][:,0].cpu(),gp.y[0].cpu(),color=\"k\",s=MARKERSIZE)     ax[i,2].scatter(gp.x[1][:,0].cpu(),gp.y[1].cpu(),color=\"k\",s=MARKERSIZE)     ax[i,1].plot(xticks[:,0].cpu(),pmeans_grad[i][0].cpu(),color=colors[i],linewidth=LINEWIDTH)     ax[i,2].plot(xticks[:,0].cpu(),pmeans_grad[i][1].cpu(),color=colors[i],linewidth=LINEWIDTH)     ax[i,1].fill_between(xticks[:,0].cpu(),pci_lows_grad[i][0].cpu(),pci_highs_grad[i][0].cpu(),color=colors[i],alpha=_alpha)     ax[i,2].fill_between(xticks[:,0].cpu(),pci_lows_grad[i][1].cpu(),pci_highs_grad[i][1].cpu(),color=colors[i],alpha=_alpha) ax[0,0].set_title(r\"$f$ no grad\",fontsize=\"xx-large\") ax[0,1].set_title(r\"$f$ with grad\",fontsize=\"xx-large\") ax[0,2].set_title(r\"$\\mathrm{d} f / \\mathrm{d} x$ with grad\",fontsize=\"xx-large\") for j in range(3):     ax[-1,j].set_xlabel(r\"$x$\",fontsize=\"xx-large\") fig.savefig(\"./gps_deriv.pdf\",bbox_inches=\"tight\") In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/derivative_informed/compare_gps_plot/#compare-gps-plot","title":"Compare GPs + Plot\u00b6","text":""},{"location":"examples/derivative_informed/compare_gps_plot/#true-function","title":"True Function\u00b6","text":""},{"location":"examples/derivative_informed/compare_gps_plot/#fast-gp-construction","title":"Fast GP Construction\u00b6","text":""},{"location":"examples/derivative_informed/compare_gps_plot/#gp-fitting","title":"GP Fitting\u00b6","text":""},{"location":"examples/derivative_informed/compare_gps_plot/#plot","title":"Plot\u00b6","text":""},{"location":"examples/derivative_informed/fgp_dnb2/","title":"Fast GP Net","text":"In\u00a0[1]: Copied! <pre>import fastgps\nimport qmcpy as qp\nimport torch\nimport numpy as np\n</pre> import fastgps import qmcpy as qp import torch import numpy as np In\u00a0[2]: Copied! <pre>device = \"cpu\"\nif device!=\"mps\":\n    torch.set_default_dtype(torch.float64)\n</pre> device = \"cpu\" if device!=\"mps\":     torch.set_default_dtype(torch.float64) In\u00a0[3]: Copied! <pre>d = 2\nf = lambda x: x[:,1]*torch.sin(x[:,0])+x[:,0]*torch.cos(x[:,1])\nf0 = lambda x: x[:,1]*torch.cos(x[:,0])+torch.cos(x[:,1])\nf1 = lambda x: torch.sin(x[:,0])-x[:,0]*torch.sin(x[:,1])\nderivatives = [\n    torch.tensor([0,0],device=device),\n    torch.tensor([1,0],device=device),\n    torch.tensor([0,1],device=device),\n]\nrng = torch.Generator().manual_seed(17)\nx = torch.rand((2**7,d),generator=rng).to(device) # random testing locations\ny = torch.cat([f(x)[None,:],f0(x)[None,:],f1(x)[None,:]],dim=0) # true values at random testing locations\nz = torch.rand((2**8,d),generator=rng).to(device) # other random locations at which to evaluate covariance\nprint(\"x.shape = %s\"%str(tuple(x.shape)))\nprint(\"y.shape = %s\"%str(tuple(y.shape)))\nprint(\"z.shape = %s\"%str(tuple(z.shape)))\n</pre> d = 2 f = lambda x: x[:,1]*torch.sin(x[:,0])+x[:,0]*torch.cos(x[:,1]) f0 = lambda x: x[:,1]*torch.cos(x[:,0])+torch.cos(x[:,1]) f1 = lambda x: torch.sin(x[:,0])-x[:,0]*torch.sin(x[:,1]) derivatives = [     torch.tensor([0,0],device=device),     torch.tensor([1,0],device=device),     torch.tensor([0,1],device=device), ] rng = torch.Generator().manual_seed(17) x = torch.rand((2**7,d),generator=rng).to(device) # random testing locations y = torch.cat([f(x)[None,:],f0(x)[None,:],f1(x)[None,:]],dim=0) # true values at random testing locations z = torch.rand((2**8,d),generator=rng).to(device) # other random locations at which to evaluate covariance print(\"x.shape = %s\"%str(tuple(x.shape))) print(\"y.shape = %s\"%str(tuple(y.shape))) print(\"z.shape = %s\"%str(tuple(z.shape))) <pre>x.shape = (128, 2)\ny.shape = (3, 128)\nz.shape = (256, 2)\n</pre> In\u00a0[4]: Copied! <pre>fgp = fastgps.FastGPDigitalNetB2(\n    qp.KernelMultiTaskDerivs(\n        qp.KernelDigShiftInvar(d,torchify=True,alpha=4,device=device),\n        num_tasks = len(derivatives)),\n    seqs=7,\n    derivatives=derivatives)\nx_next = fgp.get_x_next(n=[2**6,2**3,2**8])\ny_next = [f(x_next[0]),f0(x_next[1]),f1(x_next[2])]\nfgp.add_y_next(y_next)\nassert len(x_next)==len(y_next)\nfor i in range(len(x_next)):\n    print(\"i = %d\"%i)\n    print(\"\\tx_next[%d].shape = %s\"%(i,str(tuple(x_next[i].shape))))\n    print(\"\\ty_next[%d].shape = %s\"%(i,str(tuple(y_next[i].shape))))\n</pre> fgp = fastgps.FastGPDigitalNetB2(     qp.KernelMultiTaskDerivs(         qp.KernelDigShiftInvar(d,torchify=True,alpha=4,device=device),         num_tasks = len(derivatives)),     seqs=7,     derivatives=derivatives) x_next = fgp.get_x_next(n=[2**6,2**3,2**8]) y_next = [f(x_next[0]),f0(x_next[1]),f1(x_next[2])] fgp.add_y_next(y_next) assert len(x_next)==len(y_next) for i in range(len(x_next)):     print(\"i = %d\"%i)     print(\"\\tx_next[%d].shape = %s\"%(i,str(tuple(x_next[i].shape))))     print(\"\\ty_next[%d].shape = %s\"%(i,str(tuple(y_next[i].shape)))) <pre>i = 0\n\tx_next[0].shape = (64, 2)\n\ty_next[0].shape = (64,)\ni = 1\n\tx_next[1].shape = (8, 2)\n\ty_next[1].shape = (8,)\ni = 2\n\tx_next[2].shape = (256, 2)\n\ty_next[2].shape = (256,)\n</pre> In\u00a0[5]: Copied! <pre>pmean = fgp.post_mean(x)\nprint(\"pmean.shape = %s\"%str(tuple(pmean.shape)))\nprint(\"l2 relative error =\",(torch.linalg.norm(y-pmean,dim=1)/torch.linalg.norm(y,dim=1)))\n</pre> pmean = fgp.post_mean(x) print(\"pmean.shape = %s\"%str(tuple(pmean.shape))) print(\"l2 relative error =\",(torch.linalg.norm(y-pmean,dim=1)/torch.linalg.norm(y,dim=1))) <pre>pmean.shape = (3, 128)\nl2 relative error = tensor([0.1149, 0.5073, 0.1743])\n</pre> In\u00a0[6]: Copied! <pre>data = fgp.fit()\nlist(data.keys())\n</pre> data = fgp.fit() list(data.keys()) <pre>     iter of 5.0e+03 | best loss  | loss      \n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n            0.00e+00 | 3.04e+02   | 3.04e+02  \n            5.00e+00 | -3.77e+00  | -3.77e+00 \n            1.00e+01 | -3.50e+02  | -3.50e+02 \n            1.50e+01 | -5.77e+02  | -5.77e+02 \n            2.00e+01 | -5.77e+02  | -5.75e+02 \n            2.50e+01 | -5.80e+02  | -5.80e+02 \n            3.00e+01 | -5.82e+02  | -5.82e+02 \n            3.50e+01 | -5.82e+02  | -5.82e+02 \n            4.00e+01 | -5.82e+02  | -5.82e+02 \n            4.50e+01 | -5.83e+02  | -5.83e+02 \n            5.00e+01 | -5.83e+02  | -5.83e+02 \n            5.20e+01 | -5.83e+02  | -5.83e+02 \n</pre> Out[6]: <pre>[]</pre> In\u00a0[7]: Copied! <pre>pmean,pvar,q,ci_low,ci_high = fgp.post_ci(x,confidence=0.99)\nprint(\"pmean.shape = %s\"%str(tuple(pmean.shape)))\nprint(\"pvar.shape = %s\"%str(tuple(pvar.shape)))\nprint(\"q = %.2f\"%q)\nprint(\"ci_low.shape = %s\"%str(tuple(ci_low.shape)))\nprint(\"ci_high.shape = %s\"%str(tuple(ci_high.shape)))\nprint(\"l2 relative error =\",(torch.linalg.norm(y-pmean,dim=1)/torch.linalg.norm(y,dim=1)))\npcov = fgp.post_cov(x,x)\nprint(\"pcov.shape = %s\"%str(tuple(pcov.shape)))\n_range0,_rangen1 = torch.arange(pcov.size(0)),torch.arange(pcov.size(-1))\npcov2 = fgp.post_cov(x,z)\nprint(\"pcov2.shape = %s\"%str(tuple(pcov2.shape)))\nprint(\"\\npcov diag matches pvar: %s\"%torch.allclose(pcov[_range0,_range0][:,_rangen1,_rangen1],pvar))\nprint(\"non-negative pvar: %s\"%(pvar&gt;=0).all().item())\n</pre> pmean,pvar,q,ci_low,ci_high = fgp.post_ci(x,confidence=0.99) print(\"pmean.shape = %s\"%str(tuple(pmean.shape))) print(\"pvar.shape = %s\"%str(tuple(pvar.shape))) print(\"q = %.2f\"%q) print(\"ci_low.shape = %s\"%str(tuple(ci_low.shape))) print(\"ci_high.shape = %s\"%str(tuple(ci_high.shape))) print(\"l2 relative error =\",(torch.linalg.norm(y-pmean,dim=1)/torch.linalg.norm(y,dim=1))) pcov = fgp.post_cov(x,x) print(\"pcov.shape = %s\"%str(tuple(pcov.shape))) _range0,_rangen1 = torch.arange(pcov.size(0)),torch.arange(pcov.size(-1)) pcov2 = fgp.post_cov(x,z) print(\"pcov2.shape = %s\"%str(tuple(pcov2.shape))) print(\"\\npcov diag matches pvar: %s\"%torch.allclose(pcov[_range0,_range0][:,_rangen1,_rangen1],pvar)) print(\"non-negative pvar: %s\"%(pvar&gt;=0).all().item()) <pre>pmean.shape = (3, 128)\npvar.shape = (3, 128)\nq = 2.58\nci_low.shape = (3, 128)\nci_high.shape = (3, 128)\nl2 relative error = tensor([0.2048, 0.4079, 0.0216])\npcov.shape = (3, 3, 128, 128)\npcov2.shape = (3, 3, 128, 256)\n\npcov diag matches pvar: True\nnon-negative pvar: True\n</pre> In\u00a0[8]: Copied! <pre>pcmean,pcvar,q,cci_low,cci_high = fgp.post_cubature_ci(confidence=0.99)\nprint(\"pcmean =\",pcmean)\nprint(\"pcvar =\",pcvar)\nprint(\"cci_low =\",cci_low)\nprint(\"cci_high\",cci_high)\n</pre> pcmean,pcvar,q,cci_low,cci_high = fgp.post_cubature_ci(confidence=0.99) print(\"pcmean =\",pcmean) print(\"pcvar =\",pcvar) print(\"cci_low =\",cci_low) print(\"cci_high\",cci_high) <pre>pcmean = tensor([67.4858, 67.4858, 67.4858])\npcvar = tensor([0., 0., 0.])\ncci_low = tensor([67.4858, 67.4858, 67.4858])\ncci_high tensor([67.4858, 67.4858, 67.4858])\n</pre> In\u00a0[9]: Copied! <pre>n_new = fgp.n*torch.tensor([4,2,8],device=device)\npcov_future = fgp.post_cov(x,z,n=n_new)\npvar_future = fgp.post_var(x,n=n_new)\npcvar_future = fgp.post_cubature_var(n=n_new)\n</pre> n_new = fgp.n*torch.tensor([4,2,8],device=device) pcov_future = fgp.post_cov(x,z,n=n_new) pvar_future = fgp.post_var(x,n=n_new) pcvar_future = fgp.post_cubature_var(n=n_new) In\u00a0[10]: Copied! <pre>x_next = fgp.get_x_next(n_new)\ny_next = [f(x_next[0]),f0(x_next[1]),f1(x_next[2])]\nfor _y in y_next:\n    print(_y.shape)\nfgp.add_y_next(y_next)\nprint(\"l2 relative error =\",(torch.linalg.norm(y-fgp.post_mean(x),dim=1)/torch.linalg.norm(y,dim=1)))\nassert torch.allclose(fgp.post_cov(x,z),pcov_future)\nassert torch.allclose(fgp.post_var(x),pvar_future)\nassert torch.allclose(fgp.post_cubature_var(),pcvar_future)\n</pre> x_next = fgp.get_x_next(n_new) y_next = [f(x_next[0]),f0(x_next[1]),f1(x_next[2])] for _y in y_next:     print(_y.shape) fgp.add_y_next(y_next) print(\"l2 relative error =\",(torch.linalg.norm(y-fgp.post_mean(x),dim=1)/torch.linalg.norm(y,dim=1))) assert torch.allclose(fgp.post_cov(x,z),pcov_future) assert torch.allclose(fgp.post_var(x),pvar_future) assert torch.allclose(fgp.post_cubature_var(),pcvar_future) <pre>torch.Size([192])\ntorch.Size([8])\ntorch.Size([1792])\nl2 relative error = tensor([0.1800, 0.4430, 0.0213])\n</pre> In\u00a0[11]: Copied! <pre>data = fgp.fit(verbose=False)\nprint(\"l2 relative error =\",(torch.linalg.norm(y-fgp.post_mean(x),dim=1)/torch.linalg.norm(y,dim=1)))\n</pre> data = fgp.fit(verbose=False) print(\"l2 relative error =\",(torch.linalg.norm(y-fgp.post_mean(x),dim=1)/torch.linalg.norm(y,dim=1))) <pre>l2 relative error = tensor([0.1799, 0.5473, 0.0064])\n</pre> In\u00a0[12]: Copied! <pre>n_new = fgp.n*torch.tensor([4,8,2],device=device)\npcov_new = fgp.post_cov(x,z,n=n_new)\npvar_new = fgp.post_var(x,n=n_new)\npcvar_new = fgp.post_cubature_var(n=n_new)\nx_next = fgp.get_x_next(n_new)\ny_next = [f(x_next[0]),f0(x_next[1]),f1(x_next[2])]\nfgp.add_y_next(y_next)\nassert torch.allclose(fgp.post_cov(x,z),pcov_new)\nassert torch.allclose(fgp.post_var(x),pvar_new)\nassert torch.allclose(fgp.post_cubature_var(),pcvar_new)\n</pre> n_new = fgp.n*torch.tensor([4,8,2],device=device) pcov_new = fgp.post_cov(x,z,n=n_new) pvar_new = fgp.post_var(x,n=n_new) pcvar_new = fgp.post_cubature_var(n=n_new) x_next = fgp.get_x_next(n_new) y_next = [f(x_next[0]),f0(x_next[1]),f1(x_next[2])] fgp.add_y_next(y_next) assert torch.allclose(fgp.post_cov(x,z),pcov_new) assert torch.allclose(fgp.post_var(x),pvar_new) assert torch.allclose(fgp.post_cubature_var(),pcvar_new) In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/derivative_informed/fgp_dnb2/#fast-net-gp","title":"Fast Net GP\u00b6","text":""},{"location":"examples/derivative_informed/fgp_dnb2/#true-function","title":"True Function\u00b6","text":""},{"location":"examples/derivative_informed/fgp_dnb2/#construct-fast-gp","title":"Construct Fast GP\u00b6","text":""},{"location":"examples/derivative_informed/fgp_dnb2/#project-and-increase-sample-size","title":"Project and Increase Sample Size\u00b6","text":""},{"location":"examples/derivative_informed/fgp_lattice/","title":"Fast GP Lattice","text":"In\u00a0[1]: Copied! <pre>import fastgps\nimport qmcpy as qp\nimport torch\nimport numpy as np\n</pre> import fastgps import qmcpy as qp import torch import numpy as np In\u00a0[2]: Copied! <pre>device = \"cpu\"\nif device!=\"mps\":\n    torch.set_default_dtype(torch.float64)\n</pre> device = \"cpu\" if device!=\"mps\":     torch.set_default_dtype(torch.float64) In\u00a0[3]: Copied! <pre>d = 2\nf = lambda x: x[:,1]*torch.sin(x[:,0])+x[:,0]*torch.cos(x[:,1])\nf0 = lambda x: x[:,1]*torch.cos(x[:,0])+torch.cos(x[:,1])\nf1 = lambda x: torch.sin(x[:,0])-x[:,0]*torch.sin(x[:,1])\nderivatives = [\n    torch.tensor([0,0],device=device),\n    torch.tensor([1,0],device=device),\n    torch.tensor([0,1],device=device),\n]\nrng = torch.Generator().manual_seed(17)\nx = torch.rand((2**7,d),generator=rng).to(device) # random testing locations\ny = torch.cat([f(x)[None,:],f0(x)[None,:],f1(x)[None,:]],dim=0) # true values at random testing locations\nz = torch.rand((2**8,d),generator=rng).to(device) # other random locations at which to evaluate covariance\nprint(\"x.shape = %s\"%str(tuple(x.shape)))\nprint(\"y.shape = %s\"%str(tuple(y.shape)))\nprint(\"z.shape = %s\"%str(tuple(z.shape)))\n</pre> d = 2 f = lambda x: x[:,1]*torch.sin(x[:,0])+x[:,0]*torch.cos(x[:,1]) f0 = lambda x: x[:,1]*torch.cos(x[:,0])+torch.cos(x[:,1]) f1 = lambda x: torch.sin(x[:,0])-x[:,0]*torch.sin(x[:,1]) derivatives = [     torch.tensor([0,0],device=device),     torch.tensor([1,0],device=device),     torch.tensor([0,1],device=device), ] rng = torch.Generator().manual_seed(17) x = torch.rand((2**7,d),generator=rng).to(device) # random testing locations y = torch.cat([f(x)[None,:],f0(x)[None,:],f1(x)[None,:]],dim=0) # true values at random testing locations z = torch.rand((2**8,d),generator=rng).to(device) # other random locations at which to evaluate covariance print(\"x.shape = %s\"%str(tuple(x.shape))) print(\"y.shape = %s\"%str(tuple(y.shape))) print(\"z.shape = %s\"%str(tuple(z.shape))) <pre>x.shape = (128, 2)\ny.shape = (3, 128)\nz.shape = (256, 2)\n</pre> In\u00a0[4]: Copied! <pre>fgp = fastgps.FastGPLattice(\n    qp.KernelMultiTaskDerivs(\n        qp.KernelShiftInvar(d,torchify=True,alpha=2,device=device),\n        num_tasks=len(derivatives),\n        ),\n    seqs = 7,\n    derivatives = derivatives,)\nx_next = fgp.get_x_next(n=[2**6,2**3,2**8])\ny_next = [f(x_next[0]),f0(x_next[1]),f1(x_next[2])]\nfgp.add_y_next(y_next)\nassert len(x_next)==len(y_next)\nfor i in range(len(x_next)):\n    print(\"i = %d\"%i)\n    print(\"\\tx_next[%d].shape = %s\"%(i,str(tuple(x_next[i].shape))))\n    print(\"\\ty_next[%d].shape = %s\"%(i,str(tuple(y_next[i].shape))))\n</pre> fgp = fastgps.FastGPLattice(     qp.KernelMultiTaskDerivs(         qp.KernelShiftInvar(d,torchify=True,alpha=2,device=device),         num_tasks=len(derivatives),         ),     seqs = 7,     derivatives = derivatives,) x_next = fgp.get_x_next(n=[2**6,2**3,2**8]) y_next = [f(x_next[0]),f0(x_next[1]),f1(x_next[2])] fgp.add_y_next(y_next) assert len(x_next)==len(y_next) for i in range(len(x_next)):     print(\"i = %d\"%i)     print(\"\\tx_next[%d].shape = %s\"%(i,str(tuple(x_next[i].shape))))     print(\"\\ty_next[%d].shape = %s\"%(i,str(tuple(y_next[i].shape)))) <pre>i = 0\n\tx_next[0].shape = (64, 2)\n\ty_next[0].shape = (64,)\ni = 1\n\tx_next[1].shape = (8, 2)\n\ty_next[1].shape = (8,)\ni = 2\n\tx_next[2].shape = (256, 2)\n\ty_next[2].shape = (256,)\n</pre> In\u00a0[5]: Copied! <pre>pmean = fgp.post_mean(x)\nprint(\"pmean.shape = %s\"%str(tuple(pmean.shape)))\nprint(\"l2 relative error =\",(torch.linalg.norm(y-pmean,dim=1)/torch.linalg.norm(y,dim=1)))\n</pre> pmean = fgp.post_mean(x) print(\"pmean.shape = %s\"%str(tuple(pmean.shape))) print(\"l2 relative error =\",(torch.linalg.norm(y-pmean,dim=1)/torch.linalg.norm(y,dim=1))) <pre>pmean.shape = (3, 128)\nl2 relative error = tensor([0.2317, 4.4932, 4.7753])\n</pre> In\u00a0[6]: Copied! <pre>data = fgp.fit()\nlist(data.keys())\n</pre> data = fgp.fit() list(data.keys()) <pre>     iter of 5.0e+03 | best loss  | loss      \n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n            0.00e+00 | 7.68e+02   | 7.68e+02  \n            5.00e+00 | 5.49e+02   | 5.49e+02  \n            1.00e+01 | 5.42e+02   | 5.42e+02  \n            1.50e+01 | 5.22e+02   | 5.22e+02  \n            2.00e+01 | 4.79e+02   | 4.79e+02  \n            2.50e+01 | 4.07e+02   | 4.07e+02  \n            3.00e+01 | 2.37e+02   | 2.37e+02  \n            3.50e+01 | 2.11e+02   | 2.11e+02  \n            4.00e+01 | 2.11e+02   | 2.12e+02  \n            4.50e+01 | 2.11e+02   | 2.11e+02  \n            5.00e+01 | 2.11e+02   | 2.11e+02  \n            5.50e+01 | 2.11e+02   | 2.11e+02  \n            6.00e+01 | 2.11e+02   | 2.11e+02  \n            6.40e+01 | 2.11e+02   | 2.11e+02  \n</pre> Out[6]: <pre>[]</pre> In\u00a0[7]: Copied! <pre>pmean,pvar,q,ci_low,ci_high = fgp.post_ci(x,confidence=0.99)\nprint(\"pmean.shape = %s\"%str(tuple(pmean.shape)))\nprint(\"pvar.shape = %s\"%str(tuple(pvar.shape)))\nprint(\"q = %.2f\"%q)\nprint(\"ci_low.shape = %s\"%str(tuple(ci_low.shape)))\nprint(\"ci_high.shape = %s\"%str(tuple(ci_high.shape)))\nprint(\"l2 relative error =\",(torch.linalg.norm(y-pmean,dim=1)/torch.linalg.norm(y,dim=1)))\npcov = fgp.post_cov(x,x)\nprint(\"pcov.shape = %s\"%str(tuple(pcov.shape)))\n_range0,_rangen1 = torch.arange(pcov.size(0)),torch.arange(pcov.size(-1))\npcov2 = fgp.post_cov(x,z)\nprint(\"pcov2.shape = %s\"%str(tuple(pcov2.shape)))\nprint(\"\\npcov diag matches pvar: %s\"%torch.allclose(pcov[_range0,_range0][:,_rangen1,_rangen1],pvar))\nprint(\"non-negative pvar: %s\"%(pvar&gt;=0).all().item())\n</pre> pmean,pvar,q,ci_low,ci_high = fgp.post_ci(x,confidence=0.99) print(\"pmean.shape = %s\"%str(tuple(pmean.shape))) print(\"pvar.shape = %s\"%str(tuple(pvar.shape))) print(\"q = %.2f\"%q) print(\"ci_low.shape = %s\"%str(tuple(ci_low.shape))) print(\"ci_high.shape = %s\"%str(tuple(ci_high.shape))) print(\"l2 relative error =\",(torch.linalg.norm(y-pmean,dim=1)/torch.linalg.norm(y,dim=1))) pcov = fgp.post_cov(x,x) print(\"pcov.shape = %s\"%str(tuple(pcov.shape))) _range0,_rangen1 = torch.arange(pcov.size(0)),torch.arange(pcov.size(-1)) pcov2 = fgp.post_cov(x,z) print(\"pcov2.shape = %s\"%str(tuple(pcov2.shape))) print(\"\\npcov diag matches pvar: %s\"%torch.allclose(pcov[_range0,_range0][:,_rangen1,_rangen1],pvar)) print(\"non-negative pvar: %s\"%(pvar&gt;=0).all().item()) <pre>pmean.shape = (3, 128)\npvar.shape = (3, 128)\nq = 2.58\nci_low.shape = (3, 128)\nci_high.shape = (3, 128)\nl2 relative error = tensor([ 0.1363, 12.1602,  1.0216])\npcov.shape = (3, 3, 128, 128)\npcov2.shape = (3, 3, 128, 256)\n\npcov diag matches pvar: True\nnon-negative pvar: True\n</pre> In\u00a0[8]: Copied! <pre>pcmean,pcvar,q,cci_low,cci_high = fgp.post_cubature_ci(confidence=0.99)\nprint(\"pcmean =\",pcmean)\nprint(\"pcvar =\",pcvar)\nprint(\"cci_low =\",cci_low)\nprint(\"cci_high\",cci_high)\n</pre> pcmean,pcvar,q,cci_low,cci_high = fgp.post_cubature_ci(confidence=0.99) print(\"pcmean =\",pcmean) print(\"pcvar =\",pcvar) print(\"cci_low =\",cci_low) print(\"cci_high\",cci_high) <pre>pcmean = tensor([240.3550, 240.3550, 240.3550])\npcvar = tensor([0., 0., 0.])\ncci_low = tensor([240.3550, 240.3550, 240.3550])\ncci_high tensor([240.3550, 240.3550, 240.3550])\n</pre> In\u00a0[9]: Copied! <pre>n_new = fgp.n*torch.tensor([4,2,8],device=device)\npcov_future = fgp.post_cov(x,z,n=n_new)\npvar_future = fgp.post_var(x,n=n_new)\npcvar_future = fgp.post_cubature_var(n=n_new)\n</pre> n_new = fgp.n*torch.tensor([4,2,8],device=device) pcov_future = fgp.post_cov(x,z,n=n_new) pvar_future = fgp.post_var(x,n=n_new) pcvar_future = fgp.post_cubature_var(n=n_new) In\u00a0[10]: Copied! <pre>x_next = fgp.get_x_next(n_new)\ny_next = [f(x_next[0]),f0(x_next[1]),f1(x_next[2])]\nfor _y in y_next:\n    print(_y.shape)\nfgp.add_y_next(y_next)\nprint(\"l2 relative error =\",(torch.linalg.norm(y-fgp.post_mean(x),dim=1)/torch.linalg.norm(y,dim=1)))\nassert torch.allclose(fgp.post_cov(x,z),pcov_future)\nassert torch.allclose(fgp.post_var(x),pvar_future)\nassert torch.allclose(fgp.post_cubature_var(),pcvar_future)\n</pre> x_next = fgp.get_x_next(n_new) y_next = [f(x_next[0]),f0(x_next[1]),f1(x_next[2])] for _y in y_next:     print(_y.shape) fgp.add_y_next(y_next) print(\"l2 relative error =\",(torch.linalg.norm(y-fgp.post_mean(x),dim=1)/torch.linalg.norm(y,dim=1))) assert torch.allclose(fgp.post_cov(x,z),pcov_future) assert torch.allclose(fgp.post_var(x),pvar_future) assert torch.allclose(fgp.post_cubature_var(),pcvar_future) <pre>torch.Size([192])\ntorch.Size([8])\ntorch.Size([1792])\nl2 relative error = tensor([ 0.1199, 23.1693,  1.0979])\n</pre> In\u00a0[11]: Copied! <pre>data = fgp.fit(verbose=False)\nprint(\"l2 relative error =\",(torch.linalg.norm(y-fgp.post_mean(x),dim=1)/torch.linalg.norm(y,dim=1)))\n</pre> data = fgp.fit(verbose=False) print(\"l2 relative error =\",(torch.linalg.norm(y-fgp.post_mean(x),dim=1)/torch.linalg.norm(y,dim=1))) <pre>l2 relative error = tensor([ 0.1294, 27.2264,  1.0124])\n</pre> In\u00a0[12]: Copied! <pre>n_new = fgp.n*torch.tensor([4,8,2],device=device)\npcov_new = fgp.post_cov(x,z,n=n_new)\npvar_new = fgp.post_var(x,n=n_new)\npcvar_new = fgp.post_cubature_var(n=n_new)\nx_next = fgp.get_x_next(n_new)\ny_next = [f(x_next[0]),f0(x_next[1]),f1(x_next[2])]\nfgp.add_y_next(y_next)\nassert torch.allclose(fgp.post_cov(x,z),pcov_new)\nassert torch.allclose(fgp.post_var(x),pvar_new)\nassert torch.allclose(fgp.post_cubature_var(),pcvar_new)\n</pre> n_new = fgp.n*torch.tensor([4,8,2],device=device) pcov_new = fgp.post_cov(x,z,n=n_new) pvar_new = fgp.post_var(x,n=n_new) pcvar_new = fgp.post_cubature_var(n=n_new) x_next = fgp.get_x_next(n_new) y_next = [f(x_next[0]),f0(x_next[1]),f1(x_next[2])] fgp.add_y_next(y_next) assert torch.allclose(fgp.post_cov(x,z),pcov_new) assert torch.allclose(fgp.post_var(x),pvar_new) assert torch.allclose(fgp.post_cubature_var(),pcvar_new) In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/derivative_informed/fgp_lattice/#fast-lattice-gp","title":"Fast Lattice GP\u00b6","text":""},{"location":"examples/derivative_informed/fgp_lattice/#true-function","title":"True Function\u00b6","text":""},{"location":"examples/derivative_informed/fgp_lattice/#construct-fast-gp","title":"Construct Fast GP\u00b6","text":""},{"location":"examples/derivative_informed/fgp_lattice/#project-and-increase-sample-size","title":"Project and Increase Sample Size\u00b6","text":""},{"location":"examples/derivative_informed/standard_gp/","title":"Standard GP","text":"In\u00a0[1]: Copied! <pre>import fastgps\nimport qmcpy as qp\nimport torch\nimport numpy as np\n</pre> import fastgps import qmcpy as qp import torch import numpy as np In\u00a0[2]: Copied! <pre>device = \"cpu\"\nif device!=\"mps\":\n    torch.set_default_dtype(torch.float64)\n</pre> device = \"cpu\" if device!=\"mps\":     torch.set_default_dtype(torch.float64) In\u00a0[3]: Copied! <pre>d = 2\nf = lambda x: x[:,1]*torch.sin(x[:,0])+x[:,0]*torch.cos(x[:,1])\nf0 = lambda x: x[:,1]*torch.cos(x[:,0])+torch.cos(x[:,1])\nf1 = lambda x: torch.sin(x[:,0])-x[:,0]*torch.sin(x[:,1])\nderivatives = [\n    torch.tensor([0,0],device=device),\n    torch.tensor([1,0],device=device),\n    torch.tensor([0,1],device=device),\n]\nrng = torch.Generator().manual_seed(17)\nx = torch.rand((2**7,d),generator=rng).to(device) # random testing locations\ny = torch.cat([f(x)[None,:],f0(x)[None,:],f1(x)[None,:]],dim=0) # true values at random testing locations\nz = torch.rand((2**8,d),generator=rng).to(device) # other random locations at which to evaluate covariance\nprint(\"x.shape = %s\"%str(tuple(x.shape)))\nprint(\"y.shape = %s\"%str(tuple(y.shape)))\nprint(\"z.shape = %s\"%str(tuple(z.shape)))\n</pre> d = 2 f = lambda x: x[:,1]*torch.sin(x[:,0])+x[:,0]*torch.cos(x[:,1]) f0 = lambda x: x[:,1]*torch.cos(x[:,0])+torch.cos(x[:,1]) f1 = lambda x: torch.sin(x[:,0])-x[:,0]*torch.sin(x[:,1]) derivatives = [     torch.tensor([0,0],device=device),     torch.tensor([1,0],device=device),     torch.tensor([0,1],device=device), ] rng = torch.Generator().manual_seed(17) x = torch.rand((2**7,d),generator=rng).to(device) # random testing locations y = torch.cat([f(x)[None,:],f0(x)[None,:],f1(x)[None,:]],dim=0) # true values at random testing locations z = torch.rand((2**8,d),generator=rng).to(device) # other random locations at which to evaluate covariance print(\"x.shape = %s\"%str(tuple(x.shape))) print(\"y.shape = %s\"%str(tuple(y.shape))) print(\"z.shape = %s\"%str(tuple(z.shape))) <pre>x.shape = (128, 2)\ny.shape = (3, 128)\nz.shape = (256, 2)\n</pre> In\u00a0[4]: Copied! <pre>fgp = fastgps.StandardGP(\n   qp.KernelMultiTaskDerivs(\n        qp.KernelGaussian(d,torchify=True,device=device),\n        num_tasks = len(derivatives)),\n    seqs=7,\n    derivatives=derivatives)\nx_next = fgp.get_x_next(n=[2**4,2**2,2**3])\ny_next = [f(x_next[0]),f0(x_next[1]),f1(x_next[2])]\nfgp.add_y_next(y_next)\nassert len(x_next)==len(y_next)\nfor i in range(len(x_next)):\n    print(\"i = %d\"%i)\n    print(\"\\tx_next[%d].shape = %s\"%(i,str(tuple(x_next[i].shape))))\n    print(\"\\ty_next[%d].shape = %s\"%(i,str(tuple(y_next[i].shape))))\n</pre> fgp = fastgps.StandardGP(    qp.KernelMultiTaskDerivs(         qp.KernelGaussian(d,torchify=True,device=device),         num_tasks = len(derivatives)),     seqs=7,     derivatives=derivatives) x_next = fgp.get_x_next(n=[2**4,2**2,2**3]) y_next = [f(x_next[0]),f0(x_next[1]),f1(x_next[2])] fgp.add_y_next(y_next) assert len(x_next)==len(y_next) for i in range(len(x_next)):     print(\"i = %d\"%i)     print(\"\\tx_next[%d].shape = %s\"%(i,str(tuple(x_next[i].shape))))     print(\"\\ty_next[%d].shape = %s\"%(i,str(tuple(y_next[i].shape)))) <pre>i = 0\n\tx_next[0].shape = (16, 2)\n\ty_next[0].shape = (16,)\ni = 1\n\tx_next[1].shape = (4, 2)\n\ty_next[1].shape = (4,)\ni = 2\n\tx_next[2].shape = (8, 2)\n\ty_next[2].shape = (8,)\n</pre> In\u00a0[5]: Copied! <pre>pmean = fgp.post_mean(x)\nprint(\"pmean.shape = %s\"%str(tuple(pmean.shape)))\nprint(\"l2 relative error =\",(torch.linalg.norm(y-pmean,dim=1)/torch.linalg.norm(y,dim=1)))\n</pre> pmean = fgp.post_mean(x) print(\"pmean.shape = %s\"%str(tuple(pmean.shape))) print(\"l2 relative error =\",(torch.linalg.norm(y-pmean,dim=1)/torch.linalg.norm(y,dim=1))) <pre>pmean.shape = (3, 128)\nl2 relative error = tensor([0.0012, 0.0075, 0.0216])\n</pre> In\u00a0[6]: Copied! <pre>data = fgp.fit()\nlist(data.keys())\n</pre> data = fgp.fit() list(data.keys()) <pre>     iter of 5.0e+03 | best loss  | loss      \n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n            0.00e+00 | -5.55e+01  | -5.55e+01 \n            5.00e+00 | -6.42e+01  | -6.42e+01 \n            1.00e+01 | -6.43e+01  | -6.43e+01 \n            1.50e+01 | -6.46e+01  | -6.46e+01 \n            2.00e+01 | -6.50e+01  | -6.50e+01 \n            2.50e+01 | -6.51e+01  | -6.51e+01 \n            3.00e+01 | -6.51e+01  | -6.51e+01 \n            3.50e+01 | -6.51e+01  | -6.51e+01 \n</pre> Out[6]: <pre>[]</pre> In\u00a0[7]: Copied! <pre>pmean,pvar,q,ci_low,ci_high = fgp.post_ci(x,confidence=0.99)\nprint(\"pmean.shape = %s\"%str(tuple(pmean.shape)))\nprint(\"pvar.shape = %s\"%str(tuple(pvar.shape)))\nprint(\"q = %.2f\"%q)\nprint(\"ci_low.shape = %s\"%str(tuple(ci_low.shape)))\nprint(\"ci_high.shape = %s\"%str(tuple(ci_high.shape)))\nprint(\"l2 relative error =\",(torch.linalg.norm(y-pmean,dim=1)/torch.linalg.norm(y,dim=1)))\npcov = fgp.post_cov(x,x)\nprint(\"pcov.shape = %s\"%str(tuple(pcov.shape)))\n_range0,_rangen1 = torch.arange(pcov.size(0)),torch.arange(pcov.size(-1))\npcov2 = fgp.post_cov(x,z)\nprint(\"pcov2.shape = %s\"%str(tuple(pcov2.shape)))\nprint(\"\\npcov diag matches pvar: %s\"%torch.allclose(pcov[_range0,_range0][:,_rangen1,_rangen1],pvar))\nprint(\"non-negative pvar: %s\"%(pvar&gt;=0).all().item())\n</pre> pmean,pvar,q,ci_low,ci_high = fgp.post_ci(x,confidence=0.99) print(\"pmean.shape = %s\"%str(tuple(pmean.shape))) print(\"pvar.shape = %s\"%str(tuple(pvar.shape))) print(\"q = %.2f\"%q) print(\"ci_low.shape = %s\"%str(tuple(ci_low.shape))) print(\"ci_high.shape = %s\"%str(tuple(ci_high.shape))) print(\"l2 relative error =\",(torch.linalg.norm(y-pmean,dim=1)/torch.linalg.norm(y,dim=1))) pcov = fgp.post_cov(x,x) print(\"pcov.shape = %s\"%str(tuple(pcov.shape))) _range0,_rangen1 = torch.arange(pcov.size(0)),torch.arange(pcov.size(-1)) pcov2 = fgp.post_cov(x,z) print(\"pcov2.shape = %s\"%str(tuple(pcov2.shape))) print(\"\\npcov diag matches pvar: %s\"%torch.allclose(pcov[_range0,_range0][:,_rangen1,_rangen1],pvar)) print(\"non-negative pvar: %s\"%(pvar&gt;=0).all().item()) <pre>pmean.shape = (3, 128)\npvar.shape = (3, 128)\nq = 2.58\nci_low.shape = (3, 128)\nci_high.shape = (3, 128)\nl2 relative error = tensor([0.0024, 0.0110, 0.0262])\npcov.shape = (3, 3, 128, 128)\npcov2.shape = (3, 3, 128, 256)\n\npcov diag matches pvar: True\nnon-negative pvar: True\n</pre> In\u00a0[8]: Copied! <pre>pcmean,pcvar,q,cci_low,cci_high = fgp.post_cubature_ci(confidence=0.99)\nprint(\"pcmean =\",pcmean)\nprint(\"pcvar =\",pcvar)\nprint(\"cci_low =\",cci_low)\nprint(\"cci_high\",cci_high)\n</pre> pcmean,pcvar,q,cci_low,cci_high = fgp.post_cubature_ci(confidence=0.99) print(\"pcmean =\",pcmean) print(\"pcvar =\",pcvar) print(\"cci_low =\",cci_low) print(\"cci_high\",cci_high) <pre>pcmean = tensor([-58.4215, -58.4215, -58.4215])\npcvar = tensor([0., 0., 0.])\ncci_low = tensor([-58.4215, -58.4215, -58.4215])\ncci_high tensor([-58.4215, -58.4215, -58.4215])\n</pre> In\u00a0[9]: Copied! <pre>n_new = fgp.n*torch.tensor([4,2,8],device=device)\npcov_future = fgp.post_cov(x,z,n=n_new)\npvar_future = fgp.post_var(x,n=n_new)\npcvar_future = fgp.post_cubature_var(n=n_new)\n</pre> n_new = fgp.n*torch.tensor([4,2,8],device=device) pcov_future = fgp.post_cov(x,z,n=n_new) pvar_future = fgp.post_var(x,n=n_new) pcvar_future = fgp.post_cubature_var(n=n_new) In\u00a0[10]: Copied! <pre>x_next = fgp.get_x_next(n_new)\ny_next = [f(x_next[0]),f0(x_next[1]),f1(x_next[2])]\nfor _y in y_next:\n    print(_y.shape)\nfgp.add_y_next(y_next)\nprint(\"l2 relative error =\",(torch.linalg.norm(y-fgp.post_mean(x),dim=1)/torch.linalg.norm(y,dim=1)))\nassert torch.allclose(fgp.post_cov(x,z),pcov_future)\nassert torch.allclose(fgp.post_var(x),pvar_future)\nassert torch.allclose(fgp.post_cubature_var(),pcvar_future)\n</pre> x_next = fgp.get_x_next(n_new) y_next = [f(x_next[0]),f0(x_next[1]),f1(x_next[2])] for _y in y_next:     print(_y.shape) fgp.add_y_next(y_next) print(\"l2 relative error =\",(torch.linalg.norm(y-fgp.post_mean(x),dim=1)/torch.linalg.norm(y,dim=1))) assert torch.allclose(fgp.post_cov(x,z),pcov_future) assert torch.allclose(fgp.post_var(x),pvar_future) assert torch.allclose(fgp.post_cubature_var(),pcvar_future) <pre>torch.Size([48])\ntorch.Size([4])\ntorch.Size([56])\nl2 relative error = tensor([0.0002, 0.0009, 0.0030])\n</pre> In\u00a0[11]: Copied! <pre>data = fgp.fit(verbose=False)\nprint(\"l2 relative error =\",(torch.linalg.norm(y-fgp.post_mean(x),dim=1)/torch.linalg.norm(y,dim=1)))\n</pre> data = fgp.fit(verbose=False) print(\"l2 relative error =\",(torch.linalg.norm(y-fgp.post_mean(x),dim=1)/torch.linalg.norm(y,dim=1))) <pre>l2 relative error = tensor([0.0002, 0.0005, 0.0028])\n</pre> In\u00a0[12]: Copied! <pre>n_new = fgp.n*torch.tensor([4,8,2],device=device)\npcov_new = fgp.post_cov(x,z,n=n_new)\npvar_new = fgp.post_var(x,n=n_new)\npcvar_new = fgp.post_cubature_var(n=n_new)\nx_next = fgp.get_x_next(n_new)\ny_next = [f(x_next[0]),f0(x_next[1]),f1(x_next[2])]\nfgp.add_y_next(y_next)\nassert torch.allclose(fgp.post_cov(x,z),pcov_new)\nassert torch.allclose(fgp.post_var(x),pvar_new)\nassert torch.allclose(fgp.post_cubature_var(),pcvar_new)\n</pre> n_new = fgp.n*torch.tensor([4,8,2],device=device) pcov_new = fgp.post_cov(x,z,n=n_new) pvar_new = fgp.post_var(x,n=n_new) pcvar_new = fgp.post_cubature_var(n=n_new) x_next = fgp.get_x_next(n_new) y_next = [f(x_next[0]),f0(x_next[1]),f1(x_next[2])] fgp.add_y_next(y_next) assert torch.allclose(fgp.post_cov(x,z),pcov_new) assert torch.allclose(fgp.post_var(x),pvar_new) assert torch.allclose(fgp.post_cubature_var(),pcvar_new) In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/derivative_informed/standard_gp/#standard-gp","title":"Standard GP\u00b6","text":""},{"location":"examples/derivative_informed/standard_gp/#true-function","title":"True Function\u00b6","text":""},{"location":"examples/derivative_informed/standard_gp/#construct-gp","title":"Construct GP\u00b6","text":""},{"location":"examples/derivative_informed/standard_gp/#project-and-increase-sample-size","title":"Project and Increase Sample Size\u00b6","text":""},{"location":"examples/misc/kiss_gp_dnets/","title":"KISS Net","text":"In\u00a0[7]: Copied! <pre>import torch\ntorch.set_default_dtype(torch.float64)\nimport numpy as np \nimport scipy.stats\nimport qmcpy as qp\nimport matplotlib\nfrom matplotlib import pyplot\npyplot.style.use(\"seaborn-v0_8-whitegrid\")\nimport pandas as pd\nCOLORS = [\"xkcd:\"+color[:-1] for color in pd.read_csv(\"../../../xkcd_colors.txt\",comment=\"#\",header=None).iloc[:,0].tolist()][::-1]\n# pyplot.rcParams['axes.prop_cycle'] = matplotlib.cycler(color=COLORS)\nLINESTYLES = ['solid','dotted','dashed','dashdot',(0, (1, 1))]\nDEFAULTFONTSIZE = 30\npyplot.rcParams['xtick.labelsize'] = DEFAULTFONTSIZE\npyplot.rcParams['ytick.labelsize'] = DEFAULTFONTSIZE\npyplot.rcParams['ytick.labelsize'] = DEFAULTFONTSIZE\npyplot.rcParams['axes.titlesize'] = DEFAULTFONTSIZE\npyplot.rcParams['figure.titlesize'] = DEFAULTFONTSIZE\npyplot.rcParams[\"axes.labelsize\"] = DEFAULTFONTSIZE\npyplot.rcParams['legend.fontsize'] = DEFAULTFONTSIZE\npyplot.rcParams['font.size'] = DEFAULTFONTSIZE\npyplot.rcParams['lines.linewidth'] = 5\npyplot.rcParams['lines.markersize'] = 15\nPW = 30 # inches\n</pre> import torch torch.set_default_dtype(torch.float64) import numpy as np  import scipy.stats import qmcpy as qp import matplotlib from matplotlib import pyplot pyplot.style.use(\"seaborn-v0_8-whitegrid\") import pandas as pd COLORS = [\"xkcd:\"+color[:-1] for color in pd.read_csv(\"../../../xkcd_colors.txt\",comment=\"#\",header=None).iloc[:,0].tolist()][::-1] # pyplot.rcParams['axes.prop_cycle'] = matplotlib.cycler(color=COLORS) LINESTYLES = ['solid','dotted','dashed','dashdot',(0, (1, 1))] DEFAULTFONTSIZE = 30 pyplot.rcParams['xtick.labelsize'] = DEFAULTFONTSIZE pyplot.rcParams['ytick.labelsize'] = DEFAULTFONTSIZE pyplot.rcParams['ytick.labelsize'] = DEFAULTFONTSIZE pyplot.rcParams['axes.titlesize'] = DEFAULTFONTSIZE pyplot.rcParams['figure.titlesize'] = DEFAULTFONTSIZE pyplot.rcParams[\"axes.labelsize\"] = DEFAULTFONTSIZE pyplot.rcParams['legend.fontsize'] = DEFAULTFONTSIZE pyplot.rcParams['font.size'] = DEFAULTFONTSIZE pyplot.rcParams['lines.linewidth'] = 5 pyplot.rcParams['lines.markersize'] = 15 PW = 30 # inches In\u00a0[\u00a0]: Copied! <pre>d = 2\nrng = torch.Generator().manual_seed(11)\nn = 4000\nm = 12\nassert m%2==0, \"m must be even\"\nx = torch.rand(n,d,generator=rng)\nprint(\"x.shape = %s\"%str(tuple(x.shape)))\ndnb2 = qp.DigitalNetB2(d,seed=11) # with d=2 this is a (0,m,d) sequence\nu = torch.from_numpy(dnb2(2**m))\nprint(\"u.shape = %s\"%str(tuple(u.shape)))\nfig,ax = pyplot.subplots(nrows=1,ncols=2,figsize=(PW/2,PW/4))\nax[0].scatter(x[:,0],x[:,1],s=50,color=COLORS[0],marker='o')\nax[0].set_title(r\"$\\mathsf{X}$\")\nax[1].scatter(u[:,0],u[:,1],s=50,color=COLORS[1],marker='s')\nax[1].set_title(r\"$\\mathsf{U}$\")\nfor i in range(2):\n    ax[i].set_xlim([0,1])\n    ax[i].set_ylim([0,1])\n    ax[i].set_aspect(1)\n    ax[i].set_xticks(torch.arange(2**(m/2)+1)/2**(m/2))\n    ax[i].set_yticks(torch.arange(2**(m/2)+1)/2**(m/2))\n    ax[i].set_xticklabels([r\"$\\frac{%d}{%d}$\"%(i,2**(m//2)) if i/2**(m//2) in [0,1/4,1/2,3/4,1] else \"\" for i in range(2**(m//2)+1)])\n    ax[i].set_yticklabels([r\"$%d/%d$\"%(i,2**(m//2)) if i/2**(m//2) in [0,1/4,1/2,3/4,1] else \"\" for i in range(2**(m//2)+1)])\nfig.tight_layout()\n</pre> d = 2 rng = torch.Generator().manual_seed(11) n = 4000 m = 12 assert m%2==0, \"m must be even\" x = torch.rand(n,d,generator=rng) print(\"x.shape = %s\"%str(tuple(x.shape))) dnb2 = qp.DigitalNetB2(d,seed=11) # with d=2 this is a (0,m,d) sequence u = torch.from_numpy(dnb2(2**m)) print(\"u.shape = %s\"%str(tuple(u.shape))) fig,ax = pyplot.subplots(nrows=1,ncols=2,figsize=(PW/2,PW/4)) ax[0].scatter(x[:,0],x[:,1],s=50,color=COLORS[0],marker='o') ax[0].set_title(r\"$\\mathsf{X}$\") ax[1].scatter(u[:,0],u[:,1],s=50,color=COLORS[1],marker='s') ax[1].set_title(r\"$\\mathsf{U}$\") for i in range(2):     ax[i].set_xlim([0,1])     ax[i].set_ylim([0,1])     ax[i].set_aspect(1)     ax[i].set_xticks(torch.arange(2**(m/2)+1)/2**(m/2))     ax[i].set_yticks(torch.arange(2**(m/2)+1)/2**(m/2))     ax[i].set_xticklabels([r\"$\\frac{%d}{%d}$\"%(i,2**(m//2)) if i/2**(m//2) in [0,1/4,1/2,3/4,1] else \"\" for i in range(2**(m//2)+1)])     ax[i].set_yticklabels([r\"$%d/%d$\"%(i,2**(m//2)) if i/2**(m//2) in [0,1/4,1/2,3/4,1] else \"\" for i in range(2**(m//2)+1)]) fig.tight_layout() <pre>x.shape = (4000, 2)\nu.shape = (4096, 2)\n</pre> In\u00a0[10]: Copied! <pre>kernel = qp.KernelDigShiftInvar(d,torchify=True,t=dnb2.t)\nk_xx = kernel(x[:,None,:],x[None,:,:]).detach()\nprint(\"k_xx.shape = %s\"%str(tuple(k_xx.shape)))\nk_xu = kernel(x[:,None,:],u[None,:,:]).detach()\nprint(\"k_xu.shape = %s\"%str(tuple(k_xu.shape)))\nk_uu = kernel(u[:,None,:],u[None,:,:]).detach()\nprint(\"k_uu.shape = %s\"%str(tuple(k_uu.shape)))\nfig,ax = pyplot.subplots(nrows=1,ncols=3,figsize=(PW,PW/3))\nax[0].imshow(k_xx,cmap=\"gnuplot2\")\nax[0].set_title(r\"$\\mathsf{K}_{\\mathsf{X},\\mathsf{X}}$\")\nax[1].imshow(k_xu,cmap=\"gnuplot2\")\nax[1].set_title(r\"$\\mathsf{K}_{\\mathsf{X},\\mathsf{U}}$\")\nax[2].imshow(k_uu,cmap=\"gnuplot2\")\nax[2].set_title(r\"$\\mathsf{K}_{\\mathsf{U},\\mathsf{U}}$\")\nfig.tight_layout()\n</pre> kernel = qp.KernelDigShiftInvar(d,torchify=True,t=dnb2.t) k_xx = kernel(x[:,None,:],x[None,:,:]).detach() print(\"k_xx.shape = %s\"%str(tuple(k_xx.shape))) k_xu = kernel(x[:,None,:],u[None,:,:]).detach() print(\"k_xu.shape = %s\"%str(tuple(k_xu.shape))) k_uu = kernel(u[:,None,:],u[None,:,:]).detach() print(\"k_uu.shape = %s\"%str(tuple(k_uu.shape))) fig,ax = pyplot.subplots(nrows=1,ncols=3,figsize=(PW,PW/3)) ax[0].imshow(k_xx,cmap=\"gnuplot2\") ax[0].set_title(r\"$\\mathsf{K}_{\\mathsf{X},\\mathsf{X}}$\") ax[1].imshow(k_xu,cmap=\"gnuplot2\") ax[1].set_title(r\"$\\mathsf{K}_{\\mathsf{X},\\mathsf{U}}$\") ax[2].imshow(k_uu,cmap=\"gnuplot2\") ax[2].set_title(r\"$\\mathsf{K}_{\\mathsf{U},\\mathsf{U}}$\") fig.tight_layout() <pre>k_xx.shape = (4000, 4000)\nk_xu.shape = (4000, 4096)\nk_uu.shape = (4096, 4096)\n</pre> In\u00a0[11]: Copied! <pre>map_u = {(int(j1*2**(m//2)),int(j2*2**(m//2))): i for i,(j1,j2) in enumerate(u)}\nw = torch.tensor([map_u[(int(j1*2**(m//2)),int(j2*2**(m//2)))] for j1,j2 in x],dtype=int)\nfig,ax = pyplot.subplots(nrows=1,ncols=1,figsize=(PW/4,PW/4))\nax.scatter(x[:,0],x[:,1],s=100,color=COLORS[0],marker='o',label=r\"$\\mathsf{X}$\")\nax.scatter(u[:,0],u[:,1],s=100,color=COLORS[1],marker='s',label=r\"$\\mathsf{U}$\")\nfor i in range(n):\n    u_i = u[w[i]]\n    ax.plot([x[i,0],u_i[0]],[x[i,1],u_i[1]],color=\"k\",alpha=0.25)\nax.set_xlim([0,1])\nax.set_ylim([0,1])\nax.set_aspect(1)\nax.set_xticks(torch.arange(2**(m/2)+1)/2**(m/2))\nax.set_yticks(torch.arange(2**(m/2)+1)/2**(m/2))\nax.set_xticklabels([r\"$\\frac{%d}{%d}$\"%(i,2**(m//2)) if i/2**(m//2) in [0,1/4,1/2,3/4,1] else \"\" for i in range(2**(m//2)+1)])\nax.set_yticklabels([r\"$%d/%d$\"%(i,2**(m//2)) if i/2**(m//2) in [0,1/4,1/2,3/4,1] else \"\" for i in range(2**(m//2)+1)])\nfig.legend(frameon=False,bbox_to_anchor=(0.8,1.05),ncols=2)\nfig.tight_layout()\n</pre> map_u = {(int(j1*2**(m//2)),int(j2*2**(m//2))): i for i,(j1,j2) in enumerate(u)} w = torch.tensor([map_u[(int(j1*2**(m//2)),int(j2*2**(m//2)))] for j1,j2 in x],dtype=int) fig,ax = pyplot.subplots(nrows=1,ncols=1,figsize=(PW/4,PW/4)) ax.scatter(x[:,0],x[:,1],s=100,color=COLORS[0],marker='o',label=r\"$\\mathsf{X}$\") ax.scatter(u[:,0],u[:,1],s=100,color=COLORS[1],marker='s',label=r\"$\\mathsf{U}$\") for i in range(n):     u_i = u[w[i]]     ax.plot([x[i,0],u_i[0]],[x[i,1],u_i[1]],color=\"k\",alpha=0.25) ax.set_xlim([0,1]) ax.set_ylim([0,1]) ax.set_aspect(1) ax.set_xticks(torch.arange(2**(m/2)+1)/2**(m/2)) ax.set_yticks(torch.arange(2**(m/2)+1)/2**(m/2)) ax.set_xticklabels([r\"$\\frac{%d}{%d}$\"%(i,2**(m//2)) if i/2**(m//2) in [0,1/4,1/2,3/4,1] else \"\" for i in range(2**(m//2)+1)]) ax.set_yticklabels([r\"$%d/%d$\"%(i,2**(m//2)) if i/2**(m//2) in [0,1/4,1/2,3/4,1] else \"\" for i in range(2**(m//2)+1)]) fig.legend(frameon=False,bbox_to_anchor=(0.8,1.05),ncols=2) fig.tight_layout() In\u00a0[12]: Copied! <pre>norm_k_xx = torch.linalg.norm(k_xx)\ng_kiss1 = k_xu@torch.linalg.solve(k_uu,k_xu.T) # SOR\ng_kiss1[torch.arange(n),torch.arange(n)] = k_xx[torch.arange(n),torch.arange(n)] # uncomment for FITC \neps1 = torch.linalg.norm(k_xx-g_kiss1)/norm_k_xx\nprint(\"eps1 = %.2e\"%eps1)\ng_kiss2 = k_uu[w,:][:,w]\ng_kiss2[torch.arange(n),torch.arange(n)] = k_xx[torch.arange(n),torch.arange(n)] # uncomment for FITC \neps2 = torch.linalg.norm(k_xx-g_kiss2)/norm_k_xx\nprint(\"eps2 = %.2e\"%eps2)\n</pre> norm_k_xx = torch.linalg.norm(k_xx) g_kiss1 = k_xu@torch.linalg.solve(k_uu,k_xu.T) # SOR g_kiss1[torch.arange(n),torch.arange(n)] = k_xx[torch.arange(n),torch.arange(n)] # uncomment for FITC  eps1 = torch.linalg.norm(k_xx-g_kiss1)/norm_k_xx print(\"eps1 = %.2e\"%eps1) g_kiss2 = k_uu[w,:][:,w] g_kiss2[torch.arange(n),torch.arange(n)] = k_xx[torch.arange(n),torch.arange(n)] # uncomment for FITC  eps2 = torch.linalg.norm(k_xx-g_kiss2)/norm_k_xx print(\"eps2 = %.2e\"%eps2) <pre>eps1 = 7.33e-04\neps2 = 8.90e-03\n</pre> In\u00a0[13]: Copied! <pre>def f(x):\n    assert x.size(-1)==2\n    x = 6*x-3 \n    x0,x1 = x[...,0],x[...,1]\n    y = 3*(1-x0)**2*torch.exp(-x0**2-(x1+1)**2)-10*(x0/5-x0**3-x1**5)*torch.exp(-x0**2-x1**2)-1/3*torch.exp(-(x0+1)**2-x1**2)\n    return y\nxticks_1d = torch.linspace(0,1,101)[1:-1]\nx0mesh,x1mesh = torch.meshgrid(xticks_1d,xticks_1d,indexing=\"ij\")\nxticks = torch.stack([x0mesh.flatten(),x1mesh.flatten()],axis=1)\nyticks = f(xticks) \ny = f(x)\neps = 1e0\nk_xticks_x = kernel(xticks[:,None,:],x).detach()\nalpha_gp = torch.linalg.solve(k_xx+eps*torch.eye(n),y[:,None])[:,0].detach()\nyhatticks_gp = k_xticks_x@alpha_gp\nl2rerror_gp = torch.linalg.norm(yticks-yhatticks_gp)/torch.linalg.norm(yticks)\nalpha_kiss1 = torch.linalg.solve(g_kiss1+eps*torch.eye(n),y[:,None])[:,0].detach()\nyhatticks_kiss1 = k_xticks_x@alpha_kiss1\nl2rerror_kiss1 = torch.linalg.norm(yticks-yhatticks_kiss1)/torch.linalg.norm(yticks)\nalpha_kiss2 = torch.linalg.solve(g_kiss2+eps*torch.eye(n),y[:,None])[:,0].detach()\nyhatticks_kiss2 = k_xticks_x@alpha_kiss2\nl2rerror_kiss2 = torch.linalg.norm(yticks-yhatticks_kiss2)/torch.linalg.norm(yticks)\nfig,ax = pyplot.subplots(1,4,subplot_kw={'projection':'3d'},figsize=(PW,PW/4))\nax[0].plot_surface(x0mesh,x1mesh,yticks.reshape(x0mesh.shape),cmap=\"gnuplot2\");\nax[1].plot_surface(x0mesh,x1mesh,yhatticks_gp.reshape(x0mesh.shape),cmap=\"gnuplot2\");\nax[2].plot_surface(x0mesh,x1mesh,yhatticks_kiss1.reshape(x0mesh.shape),cmap=\"gnuplot2\");\nax[3].plot_surface(x0mesh,x1mesh,yhatticks_kiss2.reshape(x0mesh.shape),cmap=\"gnuplot2\");\nax[1].set_title(r\"error = %.2e\"%l2rerror_gp)\nax[2].set_title(r\"error = %.2e\"%l2rerror_kiss1)\nax[3].set_title(r\"error = %.2e\"%l2rerror_kiss2);\n</pre> def f(x):     assert x.size(-1)==2     x = 6*x-3      x0,x1 = x[...,0],x[...,1]     y = 3*(1-x0)**2*torch.exp(-x0**2-(x1+1)**2)-10*(x0/5-x0**3-x1**5)*torch.exp(-x0**2-x1**2)-1/3*torch.exp(-(x0+1)**2-x1**2)     return y xticks_1d = torch.linspace(0,1,101)[1:-1] x0mesh,x1mesh = torch.meshgrid(xticks_1d,xticks_1d,indexing=\"ij\") xticks = torch.stack([x0mesh.flatten(),x1mesh.flatten()],axis=1) yticks = f(xticks)  y = f(x) eps = 1e0 k_xticks_x = kernel(xticks[:,None,:],x).detach() alpha_gp = torch.linalg.solve(k_xx+eps*torch.eye(n),y[:,None])[:,0].detach() yhatticks_gp = k_xticks_x@alpha_gp l2rerror_gp = torch.linalg.norm(yticks-yhatticks_gp)/torch.linalg.norm(yticks) alpha_kiss1 = torch.linalg.solve(g_kiss1+eps*torch.eye(n),y[:,None])[:,0].detach() yhatticks_kiss1 = k_xticks_x@alpha_kiss1 l2rerror_kiss1 = torch.linalg.norm(yticks-yhatticks_kiss1)/torch.linalg.norm(yticks) alpha_kiss2 = torch.linalg.solve(g_kiss2+eps*torch.eye(n),y[:,None])[:,0].detach() yhatticks_kiss2 = k_xticks_x@alpha_kiss2 l2rerror_kiss2 = torch.linalg.norm(yticks-yhatticks_kiss2)/torch.linalg.norm(yticks) fig,ax = pyplot.subplots(1,4,subplot_kw={'projection':'3d'},figsize=(PW,PW/4)) ax[0].plot_surface(x0mesh,x1mesh,yticks.reshape(x0mesh.shape),cmap=\"gnuplot2\"); ax[1].plot_surface(x0mesh,x1mesh,yhatticks_gp.reshape(x0mesh.shape),cmap=\"gnuplot2\"); ax[2].plot_surface(x0mesh,x1mesh,yhatticks_kiss1.reshape(x0mesh.shape),cmap=\"gnuplot2\"); ax[3].plot_surface(x0mesh,x1mesh,yhatticks_kiss2.reshape(x0mesh.shape),cmap=\"gnuplot2\"); ax[1].set_title(r\"error = %.2e\"%l2rerror_gp) ax[2].set_title(r\"error = %.2e\"%l2rerror_kiss1) ax[3].set_title(r\"error = %.2e\"%l2rerror_kiss2); In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/misc/kiss_gp_dnets/#kiss-gps-with-digital-sequences","title":"KISS GPs with Digital Sequences\u00b6","text":"<p>https://arxiv.org/pdf/1503.01057</p> <p>$$\\mathsf{K}_{\\mathsf{X},\\mathsf{X}} \\approx \\mathsf{K}_{\\mathsf{X},\\mathsf{U}} \\mathsf{K}_{\\mathsf{U},\\mathsf{U}}^{-1} \\mathsf{K}_{\\mathsf{U},\\mathsf{X}} \\approx \\mathsf{W} \\mathsf{K}_{\\mathsf{U},\\mathsf{U}} \\mathsf{K}_{\\mathsf{U},\\mathsf{U}}^{-1} \\mathsf{K}_{\\mathsf{U},\\mathsf{X}} \\mathsf{W}^\\intercal = \\mathsf{W} \\mathsf{K}_{\\mathsf{U},\\mathsf{U}} \\mathsf{W}^\\intercal$$</p> <p>$$\\varepsilon_1 = \\frac{\\lVert \\mathsf{K}_{\\mathsf{X},\\mathsf{X}} - \\mathsf{K}_{\\mathsf{X},\\mathsf{U}} \\mathsf{K}_{\\mathsf{U},\\mathsf{U}}^{-1} \\mathsf{K}_{\\mathsf{U},\\mathsf{X}} \\rVert_F}{\\lVert \\mathsf{K}_{\\mathsf{X},\\mathsf{X}} \\rVert_F}$$</p> <p>$$\\varepsilon_2 = \\frac{\\lVert \\mathsf{K}_{\\mathsf{X},\\mathsf{X}} - \\mathsf{W} \\mathsf{K}_{\\mathsf{U},\\mathsf{U}} \\mathsf{W}^\\intercal \\rVert_F}{\\lVert \\mathsf{K}_{\\mathsf{X},\\mathsf{X}} \\rVert_F}$$</p>"},{"location":"examples/misc/kiss_gp_dnets/#setup","title":"Setup\u00b6","text":""},{"location":"examples/misc/kiss_gp_dnets/#data-points-and-inducing-points","title":"Data points and inducing points\u00b6","text":""},{"location":"examples/misc/kiss_gp_dnets/#kernel-matrices","title":"Kernel matrices\u00b6","text":""},{"location":"examples/misc/kiss_gp_dnets/#nearest-neighbor","title":"Nearest neighbor\u00b6","text":""},{"location":"examples/misc/kiss_gp_dnets/#approximate-gram-matrices","title":"Approximate Gram matrices\u00b6","text":""},{"location":"examples/misc/kiss_gp_dnets/#function-approximation-with-gps","title":"Function approximation with GPs\u00b6","text":""},{"location":"examples/misc/periodization/","title":"Periodization","text":"In\u00a0[1]: Copied! <pre>import fastgps \nimport qmcpy as qp \nimport torch \ntorch.set_default_dtype(torch.float64)\nimport numpy as np\nfrom scipy.sparse import diags\nfrom scipy.sparse.linalg import spsolve\nfrom scipy.stats import norm\nimport itertools\nimport time\nimport warnings\nimport pandas as pd\nimport matplotlib\nfrom matplotlib import pyplot\npyplot.style.use(\"seaborn-v0_8-whitegrid\")\nCOLORS = [\"xkcd:\"+color[:-1] for color in pd.read_csv(\"../../../xkcd_colors.txt\",comment=\"#\",header=None).iloc[:,0].tolist()][::-1]\npyplot.rcParams['axes.prop_cycle'] = matplotlib.cycler(color=COLORS)\nLINESTYLES = ['solid','dotted','dashed','dashdot',(0, (1, 1))]\nDEFAULTFONTSIZE = 30\npyplot.rcParams['xtick.labelsize'] = DEFAULTFONTSIZE\npyplot.rcParams['ytick.labelsize'] = DEFAULTFONTSIZE\npyplot.rcParams['ytick.labelsize'] = DEFAULTFONTSIZE\npyplot.rcParams['axes.titlesize'] = DEFAULTFONTSIZE\npyplot.rcParams['figure.titlesize'] = DEFAULTFONTSIZE\npyplot.rcParams[\"axes.labelsize\"] = DEFAULTFONTSIZE\npyplot.rcParams['legend.fontsize'] = DEFAULTFONTSIZE\npyplot.rcParams['font.size'] = DEFAULTFONTSIZE\npyplot.rcParams['lines.linewidth'] = 5\npyplot.rcParams['lines.markersize'] = 15\nPW = 30 # inches\n</pre> import fastgps  import qmcpy as qp  import torch  torch.set_default_dtype(torch.float64) import numpy as np from scipy.sparse import diags from scipy.sparse.linalg import spsolve from scipy.stats import norm import itertools import time import warnings import pandas as pd import matplotlib from matplotlib import pyplot pyplot.style.use(\"seaborn-v0_8-whitegrid\") COLORS = [\"xkcd:\"+color[:-1] for color in pd.read_csv(\"../../../xkcd_colors.txt\",comment=\"#\",header=None).iloc[:,0].tolist()][::-1] pyplot.rcParams['axes.prop_cycle'] = matplotlib.cycler(color=COLORS) LINESTYLES = ['solid','dotted','dashed','dashdot',(0, (1, 1))] DEFAULTFONTSIZE = 30 pyplot.rcParams['xtick.labelsize'] = DEFAULTFONTSIZE pyplot.rcParams['ytick.labelsize'] = DEFAULTFONTSIZE pyplot.rcParams['ytick.labelsize'] = DEFAULTFONTSIZE pyplot.rcParams['axes.titlesize'] = DEFAULTFONTSIZE pyplot.rcParams['figure.titlesize'] = DEFAULTFONTSIZE pyplot.rcParams[\"axes.labelsize\"] = DEFAULTFONTSIZE pyplot.rcParams['legend.fontsize'] = DEFAULTFONTSIZE pyplot.rcParams['font.size'] = DEFAULTFONTSIZE pyplot.rcParams['lines.linewidth'] = 5 pyplot.rcParams['lines.markersize'] = 15 PW = 30 # inches In\u00a0[2]: Copied! <pre>ticks = torch.linspace(0,1,5001)[1:-1,None]\nBETA = 0.95\nZ = norm.ppf((BETA+1)/2)\nticks.min(),ticks.max()\n</pre> ticks = torch.linspace(0,1,5001)[1:-1,None] BETA = 0.95 Z = norm.ppf((BETA+1)/2) ticks.min(),ticks.max() Out[2]: <pre>(tensor(0.0002), tensor(0.9998))</pre> In\u00a0[3]: Copied! <pre>nalphas = 4\nfig,ax = pyplot.subplots(nrows=1,ncols=nalphas,figsize=(PW,PW/nalphas),sharex=True,sharey=True)\nfor i in range(nalphas):\n    alpha = i+1\n    kernel = qp.KernelShiftInvar(d=1,alpha=alpha,torchify=True)\n    kticks = kernel(torch.zeros(1),ticks[:,None]).detach()\n    ax[i].plot(ticks,kticks)\n    ax[i].set_title(r\"$\\alpha = %d$ SI Kernel\"%alpha)\n</pre> nalphas = 4 fig,ax = pyplot.subplots(nrows=1,ncols=nalphas,figsize=(PW,PW/nalphas),sharex=True,sharey=True) for i in range(nalphas):     alpha = i+1     kernel = qp.KernelShiftInvar(d=1,alpha=alpha,torchify=True)     kticks = kernel(torch.zeros(1),ticks[:,None]).detach()     ax[i].plot(ticks,kticks)     ax[i].set_title(r\"$\\alpha = %d$ SI Kernel\"%alpha) In\u00a0[53]: Copied! <pre>from torch import pi as PI \nfrom torch import sin as SIN \nfrom torch import cos as COS \nfrom torch import asin as ASIN \nfrom torch import acos as ACOS\n\nclass PTransform:\n    def _patch01(self, x, threshold=1e-12):\n        assert (-threshold&lt;=x).all() and (x&lt;=(1+threshold)).all(), \"x.min() = %s, x.max()-1 = %s\"%(x.min(),x.max()-1)\n        x[x&lt;0] = 0\n        x[x&gt;1] = 1\n    def tf(self, x):\n        xt = self._tf(x)\n        self._patch01(xt)\n        return xt\n    def inv(self, xt):\n        x = self._inv(xt)\n        self._patch01(x) \n        return x\n    def logweight(self, x):\n        w_full = self._weight_full(x) \n        return torch.log(w_full).sum(-1)\n    def weight(self, x):\n        return torch.exp(self.logweight(x))\n\nclass TentPT(PTransform):\n    def _tf(self, x):\n        return 1-2*torch.abs(x-1/2)\n    def _weight_full(self, x):\n        return torch.ones_like(x)\n    \nclass IdentityPT(PTransform):\n    def _tf(self, x):\n        return x\n    def _weight_full(self, x):\n        return torch.ones_like(x)\n    def _inv(self, xt):\n        return xt\n\nclass Monotone01PTTransform(PTransform):\n    def _inv(self, xt, errortol=1e-8):\n        x = 1/2*torch.ones_like(xt)\n        error = 1/2\n        t = 2\n        while error&gt;errortol:\n            xthat = self.tf(x)\n            flaglt = xthat&lt;xt\n            flaggt = xthat&gt;=xt\n            x[flaglt] = x[flaglt]+2**(-t)\n            x[flaggt] = x[flaggt]-2**(-t)\n            t += 1 \n            error /= 2\n        return x\n\nclass Poly0PT(Monotone01PTTransform):\n    def _tf(self, x):\n        return 3*x**2-2*x**3\n    def _weight_full(self, x):\n        return 6*x*(1-x)\n\nclass Poly1PT(Monotone01PTTransform):\n    def _tf(self, x):\n        return x**3*(10-15*x+6*x**2)\n    def _weight_full(self, x):\n        return 30*x**2*(1-x)**2\n    \nclass Sidi0PT(Monotone01PTTransform):\n    def _tf(self, x):\n        return SIN(PI*x/2)**2\n    def _weight_full(self, x):\n        return 1/2*PI*SIN(PI*x)\n    def _inv(self, y):\n        return ASIN(torch.sqrt(y))*2/PI\n\nclass Sidi1PT(Monotone01PTTransform):\n    def _tf(self, x):\n        return x-SIN(2*PI*x)/(2*PI)\n    def _weight_full(self, x):\n        return 2*SIN(PI*x)**2\n\nclass Sidi2PT(Monotone01PTTransform):\n    def _tf(self, x):\n        return (2+COS(PI*x))*SIN(PI*x/2)**4\n    def _weight_full(self, x):\n        return 3/4*PI*SIN(PI*x)**3\n\nclass Sidi3PT(Monotone01PTTransform):\n    def _tf(self, x):\n        return (12*PI*x-8*SIN(2*PI*x)+SIN(4*PI*x))/(12*PI)\n    def _weight_full(self, x):\n        return 8/3*SIN(PI*x)**4\n\nclass Sidi4PT(Monotone01PTTransform):\n    def _tf(self, x):\n        return 1/4*(19+18*COS(PI*x)+3*COS(2*PI*x))*SIN(PI*x/2)**6\n    def _weight_full(self, x):\n        return 15/16*PI*SIN(PI*x)**5\n\nimport scipy.stats \nclass TestPT(PTransform):\n    def _tf(self, x):\n        return 1/2*(COS(PI*x)+1)\n    def _weight_full(self, x):\n        return 1/2*PI*SIN(PI*x)\n</pre> from torch import pi as PI  from torch import sin as SIN  from torch import cos as COS  from torch import asin as ASIN  from torch import acos as ACOS  class PTransform:     def _patch01(self, x, threshold=1e-12):         assert (-threshold&lt;=x).all() and (x&lt;=(1+threshold)).all(), \"x.min() = %s, x.max()-1 = %s\"%(x.min(),x.max()-1)         x[x&lt;0] = 0         x[x&gt;1] = 1     def tf(self, x):         xt = self._tf(x)         self._patch01(xt)         return xt     def inv(self, xt):         x = self._inv(xt)         self._patch01(x)          return x     def logweight(self, x):         w_full = self._weight_full(x)          return torch.log(w_full).sum(-1)     def weight(self, x):         return torch.exp(self.logweight(x))  class TentPT(PTransform):     def _tf(self, x):         return 1-2*torch.abs(x-1/2)     def _weight_full(self, x):         return torch.ones_like(x)      class IdentityPT(PTransform):     def _tf(self, x):         return x     def _weight_full(self, x):         return torch.ones_like(x)     def _inv(self, xt):         return xt  class Monotone01PTTransform(PTransform):     def _inv(self, xt, errortol=1e-8):         x = 1/2*torch.ones_like(xt)         error = 1/2         t = 2         while error&gt;errortol:             xthat = self.tf(x)             flaglt = xthat=xt             x[flaglt] = x[flaglt]+2**(-t)             x[flaggt] = x[flaggt]-2**(-t)             t += 1              error /= 2         return x  class Poly0PT(Monotone01PTTransform):     def _tf(self, x):         return 3*x**2-2*x**3     def _weight_full(self, x):         return 6*x*(1-x)  class Poly1PT(Monotone01PTTransform):     def _tf(self, x):         return x**3*(10-15*x+6*x**2)     def _weight_full(self, x):         return 30*x**2*(1-x)**2      class Sidi0PT(Monotone01PTTransform):     def _tf(self, x):         return SIN(PI*x/2)**2     def _weight_full(self, x):         return 1/2*PI*SIN(PI*x)     def _inv(self, y):         return ASIN(torch.sqrt(y))*2/PI  class Sidi1PT(Monotone01PTTransform):     def _tf(self, x):         return x-SIN(2*PI*x)/(2*PI)     def _weight_full(self, x):         return 2*SIN(PI*x)**2  class Sidi2PT(Monotone01PTTransform):     def _tf(self, x):         return (2+COS(PI*x))*SIN(PI*x/2)**4     def _weight_full(self, x):         return 3/4*PI*SIN(PI*x)**3  class Sidi3PT(Monotone01PTTransform):     def _tf(self, x):         return (12*PI*x-8*SIN(2*PI*x)+SIN(4*PI*x))/(12*PI)     def _weight_full(self, x):         return 8/3*SIN(PI*x)**4  class Sidi4PT(Monotone01PTTransform):     def _tf(self, x):         return 1/4*(19+18*COS(PI*x)+3*COS(2*PI*x))*SIN(PI*x/2)**6     def _weight_full(self, x):         return 15/16*PI*SIN(PI*x)**5  import scipy.stats  class TestPT(PTransform):     def _tf(self, x):         return 1/2*(COS(PI*x)+1)     def _weight_full(self, x):         return 1/2*PI*SIN(PI*x) In\u00a0[56]: Copied! <pre>def f(x):\n    assert x.ndim==2 and x.shape[-1]==1\n    x0 = x[:,0]\n    # y = x0*torch.sin(2*PI*x0)\n    # y = x0*torch.exp(x0)\n    y = x0*torch.exp(-3*x0)\n    return y\nptransforms = [\n    IdentityPT(),\n    TentPT(),\n    TestPT(),\n    Sidi0PT(),\n    Sidi1PT(),\n    Sidi2PT(),\n    # Sidi3PT(),\n    # Sidi4PT(),\n    Poly0PT(),\n    Poly1PT(),\n]\nyticks = f(ticks)\nnrows = 5\nfig,ax = pyplot.subplots(nrows=nrows,ncols=len(ptransforms),figsize=(PW,PW/len(ptransforms)*nrows),sharex=True,sharey=\"row\")\nax = np.atleast_2d(ax).reshape((nrows,len(ptransforms)))\nfor i,pt in enumerate(ptransforms):\n    pt = ptransforms[i]\n    ptname = type(pt).__name__\n    z = pt.tf(ticks)\n    tickweight = pt.weight(ticks)\n    hticks = tickweight*f(z)\n    if hasattr(pt,\"_inv\"):\n        xtickshat = pt.inv(z)\n        absdiffs = torch.abs(ticks-xtickshat)\n        if not torch.allclose(ticks,xtickshat):\n            maxabsdiffs = absdiffs.max()\n            where_maxabsdiff = torch.where(absdiffs==maxabsdiffs)\n            #warnings.warn(\"max abs difference = %.2e at %s\"%(maxabsdiffs,where_maxabsdiff))\n        ax[1,i].plot(z,absdiffs,label=ptname)\n        ax[1,i].set_yscale(\"log\",base=10)\n    fgp = fastgps.FastGPLattice(\n        kernel = qp.KernelShiftInvar(\n            d = 1,\n            alpha = 3,\n            torchify = True, \n        ),\n        seqs = qp.Lattice(seed=7),\n    )\n    zquery = fgp.get_x_next(2**2)\n    xquery = pt.tf(zquery)\n    yquery = f(xquery)\n    weight = pt.weight(zquery)\n    hquery = weight*yquery\n    fgp.add_y_next(hquery)\n    fgp.fit(verbose=0)\n    htick_pmean,htick_pvar,_,htick_ci_low,htick_ci_high = fgp.post_ci(ticks)\n    ax[0,i].plot(ticks,z[:,0])\n    ax[2,i].plot(ticks,tickweight)\n    ax[3,i].plot(ticks,hticks)\n    ax[3,i].plot(ticks,htick_pmean)\n    ax[3,i].scatter(zquery,hquery)\n    ax[3,i].fill_between(ticks[:,0],htick_ci_low,htick_ci_high,alpha=.1)\n    ax[0,i].set_title(ptname)\n    if hasattr(pt,\"_inv\"):\n        zticks = pt.inv(ticks)\n        ztickweights = pt.weight(zticks)\n        ytick_pmean = fgp.post_mean(zticks)/ztickweights\n        ytick_pstd = torch.sqrt(fgp.post_var(zticks)/ztickweights**2)\n        ytick_ci_low = ytick_pmean-Z*ytick_pstd\n        ytick_ci_high = ytick_pmean+Z*ytick_pstd\n        l2rerror_h = torch.linalg.norm(htick_pmean-hticks)/torch.linalg.norm(hticks)\n        l2rerror_y = torch.linalg.norm((ytick_pmean-yticks))/torch.linalg.norm(yticks)\n        ax[4,i].plot(ticks,yticks)\n        ax[4,i].plot(ticks,ytick_pmean)\n        ax[4,i].scatter(xquery,yquery)\n        ax[4,i].fill_between(ticks[:,0],ytick_ci_low,ytick_ci_high,alpha=.1)\n    else:\n        l2rerror_h = torch.nan\n        l2rerror_y = torch.nan\n    print(\"%15s:\\t pcmean = %-10.5f pcstd = %-10.1e l2rerror_h = %-10.1e l2rerror_y = %-10.1e\"%(ptname,fgp.post_cubature_mean(),torch.sqrt(fgp.post_cubature_var()),l2rerror_h,l2rerror_y))\nax[0,0].set_ylabel(r\"$\\Phi(x)$\")\nax[1,0].set_ylabel(r\"$| \\Phi^{-1}(\\Phi(x))-x |$\")\nax[2,0].set_ylabel(r\"$\\Phi'(z)$\")\nax[3,0].set_ylabel(r\"$h(z) = f(\\Phi(z)) \\Phi'(z)$\")\nax[4,0].set_ylabel(r\"$f(x) = h(\\Phi^{-1}(x)) / \\Phi'(\\Phi^{-1}(x))$\")\nfig.tight_layout()\n</pre> def f(x):     assert x.ndim==2 and x.shape[-1]==1     x0 = x[:,0]     # y = x0*torch.sin(2*PI*x0)     # y = x0*torch.exp(x0)     y = x0*torch.exp(-3*x0)     return y ptransforms = [     IdentityPT(),     TentPT(),     TestPT(),     Sidi0PT(),     Sidi1PT(),     Sidi2PT(),     # Sidi3PT(),     # Sidi4PT(),     Poly0PT(),     Poly1PT(), ] yticks = f(ticks) nrows = 5 fig,ax = pyplot.subplots(nrows=nrows,ncols=len(ptransforms),figsize=(PW,PW/len(ptransforms)*nrows),sharex=True,sharey=\"row\") ax = np.atleast_2d(ax).reshape((nrows,len(ptransforms))) for i,pt in enumerate(ptransforms):     pt = ptransforms[i]     ptname = type(pt).__name__     z = pt.tf(ticks)     tickweight = pt.weight(ticks)     hticks = tickweight*f(z)     if hasattr(pt,\"_inv\"):         xtickshat = pt.inv(z)         absdiffs = torch.abs(ticks-xtickshat)         if not torch.allclose(ticks,xtickshat):             maxabsdiffs = absdiffs.max()             where_maxabsdiff = torch.where(absdiffs==maxabsdiffs)             #warnings.warn(\"max abs difference = %.2e at %s\"%(maxabsdiffs,where_maxabsdiff))         ax[1,i].plot(z,absdiffs,label=ptname)         ax[1,i].set_yscale(\"log\",base=10)     fgp = fastgps.FastGPLattice(         kernel = qp.KernelShiftInvar(             d = 1,             alpha = 3,             torchify = True,          ),         seqs = qp.Lattice(seed=7),     )     zquery = fgp.get_x_next(2**2)     xquery = pt.tf(zquery)     yquery = f(xquery)     weight = pt.weight(zquery)     hquery = weight*yquery     fgp.add_y_next(hquery)     fgp.fit(verbose=0)     htick_pmean,htick_pvar,_,htick_ci_low,htick_ci_high = fgp.post_ci(ticks)     ax[0,i].plot(ticks,z[:,0])     ax[2,i].plot(ticks,tickweight)     ax[3,i].plot(ticks,hticks)     ax[3,i].plot(ticks,htick_pmean)     ax[3,i].scatter(zquery,hquery)     ax[3,i].fill_between(ticks[:,0],htick_ci_low,htick_ci_high,alpha=.1)     ax[0,i].set_title(ptname)     if hasattr(pt,\"_inv\"):         zticks = pt.inv(ticks)         ztickweights = pt.weight(zticks)         ytick_pmean = fgp.post_mean(zticks)/ztickweights         ytick_pstd = torch.sqrt(fgp.post_var(zticks)/ztickweights**2)         ytick_ci_low = ytick_pmean-Z*ytick_pstd         ytick_ci_high = ytick_pmean+Z*ytick_pstd         l2rerror_h = torch.linalg.norm(htick_pmean-hticks)/torch.linalg.norm(hticks)         l2rerror_y = torch.linalg.norm((ytick_pmean-yticks))/torch.linalg.norm(yticks)         ax[4,i].plot(ticks,yticks)         ax[4,i].plot(ticks,ytick_pmean)         ax[4,i].scatter(xquery,yquery)         ax[4,i].fill_between(ticks[:,0],ytick_ci_low,ytick_ci_high,alpha=.1)     else:         l2rerror_h = torch.nan         l2rerror_y = torch.nan     print(\"%15s:\\t pcmean = %-10.5f pcstd = %-10.1e l2rerror_h = %-10.1e l2rerror_y = %-10.1e\"%(ptname,fgp.post_cubature_mean(),torch.sqrt(fgp.post_cubature_var()),l2rerror_h,l2rerror_y)) ax[0,0].set_ylabel(r\"$\\Phi(x)$\") ax[1,0].set_ylabel(r\"$| \\Phi^{-1}(\\Phi(x))-x |$\") ax[2,0].set_ylabel(r\"$\\Phi'(z)$\") ax[3,0].set_ylabel(r\"$h(z) = f(\\Phi(z)) \\Phi'(z)$\") ax[4,0].set_ylabel(r\"$f(x) = h(\\Phi^{-1}(x)) / \\Phi'(\\Phi^{-1}(x))$\") fig.tight_layout() <pre>/var/folders/xk/w1s5c54x0zv90dgqk3vpmbsw004hmz/T/ipykernel_62409/3395727716.py:38: UserWarning: Data has no positive values, and therefore cannot be log-scaled.\n  ax[1,i].set_yscale(\"log\",base=10)\n</pre> <pre>     IdentityPT:\t pcmean = 0.08497    pcstd = 4.6e-05    l2rerror_h = 1.2e-01    l2rerror_y = 1.2e-01   \n         TentPT:\t pcmean = 0.08675    pcstd = 2.0e-04    l2rerror_h = nan        l2rerror_y = nan       \n         TestPT:\t pcmean = 0.08908    pcstd = 1.8e-04    l2rerror_h = nan        l2rerror_y = nan       \n        Sidi0PT:\t pcmean = 0.08904    pcstd = 7.6e-05    l2rerror_h = 8.8e-02    l2rerror_y = 1.4e-01   \n        Sidi1PT:\t pcmean = 0.08889    pcstd = 2.4e-04    l2rerror_h = 1.8e-01    l2rerror_y = 2.9e-01   \n        Sidi2PT:\t pcmean = 0.08592    pcstd = 1.9e-04    l2rerror_h = 2.4e-01    l2rerror_y = 4.2e-01   \n        Poly0PT:\t pcmean = 0.08875    pcstd = 1.5e-04    l2rerror_h = 7.3e-02    l2rerror_y = 1.2e-01   \n        Poly1PT:\t pcmean = 0.08965    pcstd = 1.7e-04    l2rerror_h = 1.6e-01    l2rerror_y = 2.5e-01   \n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/misc/periodization/#scratch-work","title":"Scratch Work\u00b6","text":"<p>Sidi transforms paper, see equation (1.8)</p> <p>mathematica code</p> <pre>k = 3;\nSimplify[Integrate[Sin[Pi*u]^(k+1),{u,0,x}]/Integrate[Sin[Pi*u]^(k+1),{u,0,1}]]\nSimplify[D[Integrate[Sin[Pi*u]^(k+1),{u,0,x}]/Integrate[Sin[Pi*u]^(k+1),{u,0,1}],x]]\n</pre>"},{"location":"examples/misc/random_fourier_features/","title":"Random Fourier features","text":"In\u00a0[1]: Copied! <pre>import torch\ntorch.set_default_dtype(torch.float64)\nimport numpy as np \nimport scipy.stats\nimport matplotlib\nfrom matplotlib import pyplot\npyplot.style.use(\"seaborn-v0_8-whitegrid\")\n# COLORS = [\"xkcd:\"+color[:-1] for color in pd.read_csv(\"../../../xkcd_colors.txt\",comment=\"#\",header=None).iloc[:,0].tolist()][::-1]\n# pyplot.rcParams['axes.prop_cycle'] = matplotlib.cycler(color=COLORS)\nLINESTYLES = ['solid','dotted','dashed','dashdot',(0, (1, 1))]\nDEFAULTFONTSIZE = 30\npyplot.rcParams['xtick.labelsize'] = DEFAULTFONTSIZE\npyplot.rcParams['ytick.labelsize'] = DEFAULTFONTSIZE\npyplot.rcParams['ytick.labelsize'] = DEFAULTFONTSIZE\npyplot.rcParams['axes.titlesize'] = DEFAULTFONTSIZE\npyplot.rcParams['figure.titlesize'] = DEFAULTFONTSIZE\npyplot.rcParams[\"axes.labelsize\"] = DEFAULTFONTSIZE\npyplot.rcParams['legend.fontsize'] = DEFAULTFONTSIZE\npyplot.rcParams['font.size'] = DEFAULTFONTSIZE\npyplot.rcParams['lines.linewidth'] = 5\npyplot.rcParams['lines.markersize'] = 15\nPW = 30 # inches\n</pre> import torch torch.set_default_dtype(torch.float64) import numpy as np  import scipy.stats import matplotlib from matplotlib import pyplot pyplot.style.use(\"seaborn-v0_8-whitegrid\") # COLORS = [\"xkcd:\"+color[:-1] for color in pd.read_csv(\"../../../xkcd_colors.txt\",comment=\"#\",header=None).iloc[:,0].tolist()][::-1] # pyplot.rcParams['axes.prop_cycle'] = matplotlib.cycler(color=COLORS) LINESTYLES = ['solid','dotted','dashed','dashdot',(0, (1, 1))] DEFAULTFONTSIZE = 30 pyplot.rcParams['xtick.labelsize'] = DEFAULTFONTSIZE pyplot.rcParams['ytick.labelsize'] = DEFAULTFONTSIZE pyplot.rcParams['ytick.labelsize'] = DEFAULTFONTSIZE pyplot.rcParams['axes.titlesize'] = DEFAULTFONTSIZE pyplot.rcParams['figure.titlesize'] = DEFAULTFONTSIZE pyplot.rcParams[\"axes.labelsize\"] = DEFAULTFONTSIZE pyplot.rcParams['legend.fontsize'] = DEFAULTFONTSIZE pyplot.rcParams['font.size'] = DEFAULTFONTSIZE pyplot.rcParams['lines.linewidth'] = 5 pyplot.rcParams['lines.markersize'] = 15 PW = 30 # inches In\u00a0[2]: Copied! <pre>n = 10\nd = 1 \nrng = torch.Generator().manual_seed(11)\nx = torch.linspace(0,1,n)[:,None]\nf = lambda x: torch.sum(10*x*torch.sin(10*x),dim=-1)\ny = f(x)\nprint(\"x.shape = %s\"%str(tuple(x.shape)))\nprint(\"y.shape = %s\"%str(tuple(y.shape)))\nfig,ax = pyplot.subplots(nrows=1,ncols=1,figsize=(PW/5,PW/5))\nax.scatter(x,y,color=\"black\");\n</pre> n = 10 d = 1  rng = torch.Generator().manual_seed(11) x = torch.linspace(0,1,n)[:,None] f = lambda x: torch.sum(10*x*torch.sin(10*x),dim=-1) y = f(x) print(\"x.shape = %s\"%str(tuple(x.shape))) print(\"y.shape = %s\"%str(tuple(y.shape))) fig,ax = pyplot.subplots(nrows=1,ncols=1,figsize=(PW/5,PW/5)) ax.scatter(x,y,color=\"black\"); <pre>x.shape = (10, 1)\ny.shape = (10,)\n</pre> In\u00a0[3]: Copied! <pre>sigma = .25\ndef kernel_se(x, y):\n    return torch.exp(-torch.linalg.norm(x-y,dim=-1)**2/(2*sigma**2))\nN = 10000\neps = 1e-8\nepsI = eps*torch.eye(n)\nw = torch.randn(N,d,generator=rng)/sigma\nprint(\"w.shape = %s\"%str(tuple(w.shape)))\nz = 1/np.sqrt(N)*torch.cat([\n    torch.cos(torch.einsum(\"ij,...kj-&gt;...ki\",w,x)),\n    torch.sin(torch.einsum(\"ij,...kj-&gt;...ki\",w,x))\n    ],dim=-1)\nprint(\"z.shape = %s\"%str(tuple(z.shape)))\nkmat = kernel_se(x[:,None,:],x[None,:,:])+epsI\nprint(\"kmat.shape = %s\"%str(tuple(kmat.shape)))\nkmat_rff = z@z.T+epsI\nprint(\"kmat_rff.shape = %s\"%str(tuple(kmat_rff.shape)))\nfig,ax = pyplot.subplots(nrows=1,ncols=3,figsize=(3*PW/3,PW/3),sharex=True,sharey=True)\nax[0].set_title(r\"$\\mathsf{K}$\")\n_ck = ax[0].imshow(kmat,cmap=\"gnuplot2\")\nfig.colorbar(_ck)\nax[1].set_title(r\"$\\mathsf{K}_\\mathrm{RFF}$\")\n_ckhat = ax[1].imshow(kmat_rff,cmap=\"gnuplot2\")\nfig.colorbar(_ckhat)\nax[2].set_title(r\"$| \\mathsf{K} - \\mathsf{K}_\\mathrm{RFF} |$\")\n_cerror = ax[2].imshow((kmat-kmat_rff).abs(),cmap=\"gnuplot2\")\nfig.colorbar(_cerror);\n</pre> sigma = .25 def kernel_se(x, y):     return torch.exp(-torch.linalg.norm(x-y,dim=-1)**2/(2*sigma**2)) N = 10000 eps = 1e-8 epsI = eps*torch.eye(n) w = torch.randn(N,d,generator=rng)/sigma print(\"w.shape = %s\"%str(tuple(w.shape))) z = 1/np.sqrt(N)*torch.cat([     torch.cos(torch.einsum(\"ij,...kj-&gt;...ki\",w,x)),     torch.sin(torch.einsum(\"ij,...kj-&gt;...ki\",w,x))     ],dim=-1) print(\"z.shape = %s\"%str(tuple(z.shape))) kmat = kernel_se(x[:,None,:],x[None,:,:])+epsI print(\"kmat.shape = %s\"%str(tuple(kmat.shape))) kmat_rff = z@z.T+epsI print(\"kmat_rff.shape = %s\"%str(tuple(kmat_rff.shape))) fig,ax = pyplot.subplots(nrows=1,ncols=3,figsize=(3*PW/3,PW/3),sharex=True,sharey=True) ax[0].set_title(r\"$\\mathsf{K}$\") _ck = ax[0].imshow(kmat,cmap=\"gnuplot2\") fig.colorbar(_ck) ax[1].set_title(r\"$\\mathsf{K}_\\mathrm{RFF}$\") _ckhat = ax[1].imshow(kmat_rff,cmap=\"gnuplot2\") fig.colorbar(_ckhat) ax[2].set_title(r\"$| \\mathsf{K} - \\mathsf{K}_\\mathrm{RFF} |$\") _cerror = ax[2].imshow((kmat-kmat_rff).abs(),cmap=\"gnuplot2\") fig.colorbar(_cerror); <pre>w.shape = (10000, 1)\nz.shape = (10, 20000)\nkmat.shape = (10, 10)\nkmat_rff.shape = (10, 10)\n</pre> In\u00a0[7]: Copied! <pre>alpha = torch.cholesky_solve(y[:,None],torch.linalg.cholesky(kmat))[:,0]\nprint(\"alpha.shape = %s\"%str(tuple(alpha.shape)))\nalpha_rff = torch.cholesky_solve(y[:,None],torch.linalg.cholesky(kmat_rff))[:,0]\nprint(\"alpha_rff.shape = %s\"%str(tuple(alpha_rff.shape)))\nnquery = 100\nxquery = torch.linspace(0,1,nquery)[:,None]\nyquery = f(xquery)\nprint(\"yquery.shape = %s\"%str(tuple(yquery.shape)))\nkcross = kernel_se(xquery[:,None,:],x[None,:,:])\nprint(\"kcross.shape = %s\"%str(tuple(kcross.shape)))\nyhat = kcross@alpha \nyhat_rff = kcross@alpha_rff\nl2rerror = torch.linalg.norm(yhat-yquery)/torch.linalg.norm(yquery)\nl2rerror_rff = torch.linalg.norm(yhat_rff-yquery)/torch.linalg.norm(yquery)\nprint(\"    L2 relative error yhat: %.1e\"%l2rerror)\nprint(\"L2 relative error yhat_rff: %.1e\"%l2rerror_rff)\nfig,ax = pyplot.subplots(nrows=1,ncols=1,figsize=(PW/5,PW/5))\nax.scatter(x,y,color=\"black\")\nax.plot(xquery,yquery,color=\"black\",label=r\"$f$\")\nax.plot(xquery,yhat,label=r\"$\\widehat{f}$\")\nax.plot(xquery,yhat_rff,label=r\"$\\widehat{f}_\\mathrm{RFF}$\")\nax.legend();\n</pre> alpha = torch.cholesky_solve(y[:,None],torch.linalg.cholesky(kmat))[:,0] print(\"alpha.shape = %s\"%str(tuple(alpha.shape))) alpha_rff = torch.cholesky_solve(y[:,None],torch.linalg.cholesky(kmat_rff))[:,0] print(\"alpha_rff.shape = %s\"%str(tuple(alpha_rff.shape))) nquery = 100 xquery = torch.linspace(0,1,nquery)[:,None] yquery = f(xquery) print(\"yquery.shape = %s\"%str(tuple(yquery.shape))) kcross = kernel_se(xquery[:,None,:],x[None,:,:]) print(\"kcross.shape = %s\"%str(tuple(kcross.shape))) yhat = kcross@alpha  yhat_rff = kcross@alpha_rff l2rerror = torch.linalg.norm(yhat-yquery)/torch.linalg.norm(yquery) l2rerror_rff = torch.linalg.norm(yhat_rff-yquery)/torch.linalg.norm(yquery) print(\"    L2 relative error yhat: %.1e\"%l2rerror) print(\"L2 relative error yhat_rff: %.1e\"%l2rerror_rff) fig,ax = pyplot.subplots(nrows=1,ncols=1,figsize=(PW/5,PW/5)) ax.scatter(x,y,color=\"black\") ax.plot(xquery,yquery,color=\"black\",label=r\"$f$\") ax.plot(xquery,yhat,label=r\"$\\widehat{f}$\") ax.plot(xquery,yhat_rff,label=r\"$\\widehat{f}_\\mathrm{RFF}$\") ax.legend(); <pre>alpha.shape = (10,)\nalpha_rff.shape = (10,)\nyquery.shape = (100,)\nkcross.shape = (100, 10)\n    L2 relative error yhat: 2.5e-03\nL2 relative error yhat_rff: 1.1e-01\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/mlqmc/comparison/","title":"Comparison of GPs","text":"In\u00a0[1]: Copied! <pre>import fastgps\nimport qmcpy as qp\nimport numpy as np\nimport torch\nimport pandas as pd\nfrom matplotlib import pyplot\n</pre> import fastgps import qmcpy as qp import numpy as np import torch import pandas as pd from matplotlib import pyplot In\u00a0[2]: Copied! <pre>device = \"cpu\"\nif device!=\"mps\":\n    torch.set_default_dtype(torch.float64)\n</pre> device = \"cpu\" if device!=\"mps\":     torch.set_default_dtype(torch.float64) In\u00a0[3]: Copied! <pre>colors = [\"xkcd:\"+color[:-1] for color in pd.read_csv(\"../../../xkcd_colors.txt\",comment=\"#\").iloc[:,0].tolist()][::-1]\n_alpha = 0.25\nWIDTH = 2*(500/72)\nLINEWIDTH = 3\nMARKERSIZE = 100\n</pre> colors = [\"xkcd:\"+color[:-1] for color in pd.read_csv(\"../../../xkcd_colors.txt\",comment=\"#\").iloc[:,0].tolist()][::-1] _alpha = 0.25 WIDTH = 2*(500/72) LINEWIDTH = 3 MARKERSIZE = 100 In\u00a0[4]: Copied! <pre>levels = 4\nn = np.array([2**14,2**12,2**10,2**8])\nr = 8\nn_r = n//r\nd_coarsest = 2\nf = qp.FinancialOption(\n    sampler = qp.IIDStdUniform(dimension=d_coarsest),\n    option = \"ASIAN\",\n    call_put = \"CALL\",\n    asian_mean = \"GEOMETRIC\",\n    asian_mean_quadrature_rule = \"RIGHT\",\n    volatility = .5,\n    start_price = 39,\n    strike_price = 40,\n    interest_rate = 0.05,\n    t_final = 1, \n    level = 0,\n    d_coarsest = d_coarsest,\n)\nfs = [f]+f.spawn([l for l in range(1,levels)])\nd = [fs[l].d for l in range(levels)]\nexact_value_inf_dim = f.get_exact_value_inf_dim()\nexact_values = np.array([0]+[fs[l].get_exact_value() for l in range(levels)])\nexact_diffs = exact_values[1:]-exact_values[:-1]\nfor l in range(levels):\n    print(\"l = %-7d d[l] = %-7d n[l] = %-7d n_r[l] = %-7d exact_diffs[l] = %-10.2e exact_values[l+1] = %-10.4f exact_value_inf_dim = %-10.4f\"%(l,d[l],n[l],n_r[l],exact_diffs[l],exact_values[l+1],exact_value_inf_dim))\n</pre> levels = 4 n = np.array([2**14,2**12,2**10,2**8]) r = 8 n_r = n//r d_coarsest = 2 f = qp.FinancialOption(     sampler = qp.IIDStdUniform(dimension=d_coarsest),     option = \"ASIAN\",     call_put = \"CALL\",     asian_mean = \"GEOMETRIC\",     asian_mean_quadrature_rule = \"RIGHT\",     volatility = .5,     start_price = 39,     strike_price = 40,     interest_rate = 0.05,     t_final = 1,      level = 0,     d_coarsest = d_coarsest, ) fs = [f]+f.spawn([l for l in range(1,levels)]) d = [fs[l].d for l in range(levels)] exact_value_inf_dim = f.get_exact_value_inf_dim() exact_values = np.array([0]+[fs[l].get_exact_value() for l in range(levels)]) exact_diffs = exact_values[1:]-exact_values[:-1] for l in range(levels):     print(\"l = %-7d d[l] = %-7d n[l] = %-7d n_r[l] = %-7d exact_diffs[l] = %-10.2e exact_values[l+1] = %-10.4f exact_value_inf_dim = %-10.4f\"%(l,d[l],n[l],n_r[l],exact_diffs[l],exact_values[l+1],exact_value_inf_dim)) <pre>l = 0       d[l] = 2       n[l] = 16384   n_r[l] = 2048    exact_diffs[l] = 5.89e+00   exact_values[l+1] = 5.8860     exact_value_inf_dim = 3.9344    \nl = 1       d[l] = 4       n[l] = 4096    n_r[l] = 512     exact_diffs[l] = -1.00e+00  exact_values[l+1] = 4.8832     exact_value_inf_dim = 3.9344    \nl = 2       d[l] = 8       n[l] = 1024    n_r[l] = 128     exact_diffs[l] = -4.80e-01  exact_values[l+1] = 4.4027     exact_value_inf_dim = 3.9344    \nl = 3       d[l] = 16      n[l] = 256     n_r[l] = 32      exact_diffs[l] = -2.36e-01  exact_values[l+1] = 4.1671     exact_value_inf_dim = 3.9344    \n</pre> In\u00a0[5]: Copied! <pre>print(\"MLMC-IID\")\nprint(\"~\"*25)\niid_samplers = qp.IIDStdUniform(d[0],seed=7)\niid_samplers = [iid_samplers] + iid_samplers.spawn(s=levels-1,dimensions=d[1:])\nmlmc_approx = 0\nmlmc_std_error = 0\nfor l in range(levels):\n    x = iid_samplers[l](n[l])\n    y = fs[l].f(x)\n    delta = y[1]-y[0]\n    mlmc_approx_l = np.mean(delta)\n    mlmc_approx += mlmc_approx_l\n    mlmc_std_error_l = delta.std(ddof=1)/np.sqrt(n[l])\n    mlmc_std_error = np.sqrt(mlmc_std_error**2+mlmc_std_error_l**2)\n    mlmc_true_error_l = np.abs(mlmc_approx_l-exact_diffs[l])\n    mlmc_true_error = np.abs(mlmc_approx-exact_values[-1])\n    print(\"error = %-10.2e std_error = %-10.2e approx = %-10.4f error_l = %-10.2e std_error_l = %-10.2e approx_l = %-10.4f x.shape = %-10s\\ty.shape = %-10s\\tdelta.shape = %-10s\"\\\n          %(mlmc_true_error,mlmc_std_error,mlmc_approx,mlmc_true_error_l,mlmc_std_error_l,mlmc_approx_l,x.shape,y.shape,delta.shape))\n</pre> print(\"MLMC-IID\") print(\"~\"*25) iid_samplers = qp.IIDStdUniform(d[0],seed=7) iid_samplers = [iid_samplers] + iid_samplers.spawn(s=levels-1,dimensions=d[1:]) mlmc_approx = 0 mlmc_std_error = 0 for l in range(levels):     x = iid_samplers[l](n[l])     y = fs[l].f(x)     delta = y[1]-y[0]     mlmc_approx_l = np.mean(delta)     mlmc_approx += mlmc_approx_l     mlmc_std_error_l = delta.std(ddof=1)/np.sqrt(n[l])     mlmc_std_error = np.sqrt(mlmc_std_error**2+mlmc_std_error_l**2)     mlmc_true_error_l = np.abs(mlmc_approx_l-exact_diffs[l])     mlmc_true_error = np.abs(mlmc_approx-exact_values[-1])     print(\"error = %-10.2e std_error = %-10.2e approx = %-10.4f error_l = %-10.2e std_error_l = %-10.2e approx_l = %-10.4f x.shape = %-10s\\ty.shape = %-10s\\tdelta.shape = %-10s\"\\           %(mlmc_true_error,mlmc_std_error,mlmc_approx,mlmc_true_error_l,mlmc_std_error_l,mlmc_approx_l,x.shape,y.shape,delta.shape)) <pre>MLMC-IID\n~~~~~~~~~~~~~~~~~~~~~~~~~\nerror = 1.86e+00   std_error = 8.87e-02   approx = 6.0298     error_l = 1.44e-01   std_error_l = 8.87e-02   approx_l = 6.0298     x.shape = (16384, 2)\ty.shape = (2, 16384)\tdelta.shape = (16384,)  \nerror = 9.94e-01   std_error = 1.00e-01   approx = 5.1615     error_l = 1.34e-01   std_error_l = 4.66e-02   approx_l = -0.8683    x.shape = (4096, 4) \ty.shape = (2, 4096) \tdelta.shape = (4096,)   \nerror = 5.10e-01   std_error = 1.10e-01   approx = 4.6774     error_l = 3.64e-03   std_error_l = 4.55e-02   approx_l = -0.4841    x.shape = (1024, 8) \ty.shape = (2, 1024) \tdelta.shape = (1024,)   \nerror = 2.21e-01   std_error = 1.21e-01   approx = 4.3877     error_l = 5.41e-02   std_error_l = 4.93e-02   approx_l = -0.2897    x.shape = (256, 16) \ty.shape = (2, 256)  \tdelta.shape = (256,)    \n</pre> In\u00a0[13]: Copied! <pre>print(\"MLQMC-Lattice\")\nprint(\"~\"*25)\nlattice_samplers_mlqmc = qp.Lattice(d[0],replications=r,seed=7)\nlattice_samplers_mlqmc = [lattice_samplers_mlqmc]+lattice_samplers_mlqmc.spawn(s=levels-1,dimensions=d[1:])\nmlqmc_lattice_approx = 0 \nmlqmc_lattice_std_error = 0 \nfor l in range(levels):\n    x = lattice_samplers_mlqmc[l](n_r[l])\n    y = fs[l].f(x)\n    delta = y[1]-y[0]\n    muhats = delta.mean(-1)\n    mlqmc_lattice_approx_l = muhats.mean()\n    mlqmc_lattice_approx += mlqmc_lattice_approx_l\n    mlqmc_lattice_std_error_l = muhats.std(ddof=1)/np.sqrt(r)\n    mlqmc_lattice_std_error = np.sqrt(mlqmc_lattice_std_error**2+mlqmc_lattice_std_error_l**2)\n    mlqmc_lattice_true_error_l = np.abs(mlqmc_lattice_approx_l-exact_diffs[l])\n    mlqmc_lattice_true_error = np.abs(mlqmc_lattice_approx-exact_values[-1])\n    print(\"error = %-10.2e std_error = %-10.2e approx = %-10.4f error_l = %-10.2e std_error_l = %-10.2e approx_l = %-10.4f x.shape = %-15s\\ty.shape = %-15s\\tdelta.shape = %s\"\\\n          %(mlqmc_lattice_true_error,mlqmc_lattice_std_error,mlqmc_lattice_approx,mlqmc_lattice_true_error_l,mlqmc_lattice_std_error_l,mlqmc_lattice_approx_l,x.shape,y.shape,delta.shape))\n</pre> print(\"MLQMC-Lattice\") print(\"~\"*25) lattice_samplers_mlqmc = qp.Lattice(d[0],replications=r,seed=7) lattice_samplers_mlqmc = [lattice_samplers_mlqmc]+lattice_samplers_mlqmc.spawn(s=levels-1,dimensions=d[1:]) mlqmc_lattice_approx = 0  mlqmc_lattice_std_error = 0  for l in range(levels):     x = lattice_samplers_mlqmc[l](n_r[l])     y = fs[l].f(x)     delta = y[1]-y[0]     muhats = delta.mean(-1)     mlqmc_lattice_approx_l = muhats.mean()     mlqmc_lattice_approx += mlqmc_lattice_approx_l     mlqmc_lattice_std_error_l = muhats.std(ddof=1)/np.sqrt(r)     mlqmc_lattice_std_error = np.sqrt(mlqmc_lattice_std_error**2+mlqmc_lattice_std_error_l**2)     mlqmc_lattice_true_error_l = np.abs(mlqmc_lattice_approx_l-exact_diffs[l])     mlqmc_lattice_true_error = np.abs(mlqmc_lattice_approx-exact_values[-1])     print(\"error = %-10.2e std_error = %-10.2e approx = %-10.4f error_l = %-10.2e std_error_l = %-10.2e approx_l = %-10.4f x.shape = %-15s\\ty.shape = %-15s\\tdelta.shape = %s\"\\           %(mlqmc_lattice_true_error,mlqmc_lattice_std_error,mlqmc_lattice_approx,mlqmc_lattice_true_error_l,mlqmc_lattice_std_error_l,mlqmc_lattice_approx_l,x.shape,y.shape,delta.shape)) <pre>MLQMC-Lattice\n~~~~~~~~~~~~~~~~~~~~~~~~~\nerror = 1.72e+00   std_error = 6.89e-03   approx = 5.8919     error_l = 5.95e-03   std_error_l = 6.89e-03   approx_l = 5.8919     x.shape = (8, 2048, 2)   \ty.shape = (2, 8, 2048)   \tdelta.shape = (8, 2048)\nerror = 7.24e-01   std_error = 1.19e-02   approx = 4.8914     error_l = 2.27e-03   std_error_l = 9.64e-03   approx_l = -1.0005    x.shape = (8, 512, 4)    \ty.shape = (2, 8, 512)    \tdelta.shape = (8, 512)\nerror = 2.83e-01   std_error = 2.29e-02   approx = 4.4500     error_l = 3.91e-02   std_error_l = 1.96e-02   approx_l = -0.4414    x.shape = (8, 128, 8)    \ty.shape = (2, 8, 128)    \tdelta.shape = (8, 128)\nerror = 5.32e-02   std_error = 3.57e-02   approx = 4.2203     error_l = 5.86e-03   std_error_l = 2.73e-02   approx_l = -0.2297    x.shape = (8, 32, 16)    \ty.shape = (2, 8, 32)     \tdelta.shape = (8, 32)\n</pre> In\u00a0[\u00a0]: Copied! <pre>print(\"MLQMC-Net\")\nprint(\"~\"*25)\nnet_samplers_mlqmc = qp.DigitalNetB2(d[0],replications=r,seed=7)\nnet_samplers_mlqmc = [net_samplers_mlqmc]+net_samplers_mlqmc.spawn(s=levels-1,dimensions=d[1:])\nmlqmc_net_approx = 0 \nmlqmc_net_std_error = 0 \nfor l in range(levels):\n    x = net_samplers_mlqmc[l](n_r[l])\n    y = fs[l].f(x)\n    delta = y[1]-y[0]\n    muhats = delta.mean(-1)\n    mlqmc_net_approx_l = muhats.mean()\n    mlqmc_net_approx += mlqmc_net_approx_l\n    mlqmc_net_std_error_l = muhats.std(ddof=1)/np.sqrt(r)\n    mlqmc_net_std_error = np.sqrt(mlqmc_net_std_error**2+mlqmc_net_std_error_l**2)\n    mlqmc_net_true_error_l = np.abs(mlqmc_net_approx_l-exact_diffs[l])\n    mlqmc_net_true_error = np.abs(mlqmc_net_approx-exact_values[-1])\n    print(\"error = %-10.2e std_error = %-10.2e approx = %-10.4f error_l = %-10.2e std_error_l = %-10.2e approx_l = %-10.4f x.shape = %-15s\\ty.shape = %-15s\\tdelta.shape = %s\"\\\n          %(mlqmc_net_true_error,mlqmc_net_std_error,mlqmc_net_approx,mlqmc_net_true_error_l,mlqmc_net_std_error_l,mlqmc_net_approx_l,x.shape,y.shape,delta.shape))\n</pre> print(\"MLQMC-Net\") print(\"~\"*25) net_samplers_mlqmc = qp.DigitalNetB2(d[0],replications=r,seed=7) net_samplers_mlqmc = [net_samplers_mlqmc]+net_samplers_mlqmc.spawn(s=levels-1,dimensions=d[1:]) mlqmc_net_approx = 0  mlqmc_net_std_error = 0  for l in range(levels):     x = net_samplers_mlqmc[l](n_r[l])     y = fs[l].f(x)     delta = y[1]-y[0]     muhats = delta.mean(-1)     mlqmc_net_approx_l = muhats.mean()     mlqmc_net_approx += mlqmc_net_approx_l     mlqmc_net_std_error_l = muhats.std(ddof=1)/np.sqrt(r)     mlqmc_net_std_error = np.sqrt(mlqmc_net_std_error**2+mlqmc_net_std_error_l**2)     mlqmc_net_true_error_l = np.abs(mlqmc_net_approx_l-exact_diffs[l])     mlqmc_net_true_error = np.abs(mlqmc_net_approx-exact_values[-1])     print(\"error = %-10.2e std_error = %-10.2e approx = %-10.4f error_l = %-10.2e std_error_l = %-10.2e approx_l = %-10.4f x.shape = %-15s\\ty.shape = %-15s\\tdelta.shape = %s\"\\           %(mlqmc_net_true_error,mlqmc_net_std_error,mlqmc_net_approx,mlqmc_net_true_error_l,mlqmc_net_std_error_l,mlqmc_net_approx_l,x.shape,y.shape,delta.shape)) <pre>MLQMC-Net\n~~~~~~~~~~~~~~~~~~~~~~~~~\nerror = 1.72e+00   std_error = 2.21e-03   approx = 5.8834     error_l = 2.60e-03   std_error_l = 2.21e-03   approx_l = 5.8834     x.shape = (8, 2048, 2)   \ty.shape = (2, 8, 2048)   \tdelta.shape = (8, 2048)\nerror = 7.12e-01   std_error = 4.87e-03   approx = 4.8790     error_l = 1.60e-03   std_error_l = 4.35e-03   approx_l = -1.0044    x.shape = (8, 512, 4)    \ty.shape = (2, 8, 512)    \tdelta.shape = (8, 512)\nerror = 2.27e-01   std_error = 8.07e-03   approx = 4.3941     error_l = 4.43e-03   std_error_l = 6.43e-03   approx_l = -0.4849    x.shape = (8, 128, 8)    \ty.shape = (2, 8, 128)    \tdelta.shape = (8, 128)\nerror = 2.93e-03   std_error = 3.42e-02   approx = 4.1642     error_l = 5.70e-03   std_error_l = 3.32e-02   approx_l = -0.2299    x.shape = (8, 32, 16)    \ty.shape = (2, 8, 32)     \tdelta.shape = (8, 32)\n</pre> In\u00a0[15]: Copied! <pre>print(\"FIGPs-Lattice\")\nprint(\"~\"*25)\nlattice_samplers_figps = qp.Lattice(d[0],seed=7)\nlattice_samplers_figps = [lattice_samplers_figps]+lattice_samplers_figps.spawn(s=levels-1,dimensions=d[1:])\nfigps_lattice = [\n    fastgps.FastGPLattice(\n        kernel = qp.KernelShiftInvar(\n            d = d[l],\n            torchify = True,\n            alpha = 2,\n        ),\n        seqs = lattice_samplers_figps[l],\n    )\n    for l in range(levels)\n]\nfigps_lattice_approx = 0 \nfigps_lattice_std_error = 0 \nfor l in range(levels):\n    x = figps_lattice[l].get_x_next(n=n[l]).numpy()\n    y = fs[l].f(x)\n    delta = y[1]-y[0]\n    figps_lattice[l].add_y_next(torch.from_numpy(delta))\n    fit_data = figps_lattice[l].fit(\n        loss_metric = \"MLL\",\n        stop_crit_improvement_threshold = 1e-3,\n        iterations = 100,\n        store_hists = True,\n        verbose = False,\n    )\n    fastgps.plot_fastgps_fit_data(fit_data)\n    figps_lattice_approx_l = figps_lattice[l].post_cubature_mean().numpy()\n    figps_lattice_approx += figps_lattice_approx_l\n    figps_lattice_std_error_l = np.sqrt(figps_lattice[l].post_cubature_var().numpy())\n    figps_lattice_std_error = np.sqrt(figps_lattice_std_error**2+figps_lattice_std_error_l**2)\n    figps_lattice_true_error_l = np.abs(figps_lattice_approx_l-exact_diffs[l])\n    figps_lattice_true_error = np.abs(figps_lattice_approx-exact_values[-1])\n    print(\"error = %-10.2e std_error = %-10.2e approx = %-10.4f error_l = %-10.2e std_error_l = %-10.2e approx_l = %-10.4f x.shape = %-15s\\ty.shape = %-15s\\tdelta.shape = %s\"\\\n          %(figps_lattice_true_error,figps_lattice_std_error,figps_lattice_approx,figps_lattice_true_error_l,figps_lattice_std_error_l,figps_lattice_approx_l,x.shape,y.shape,delta.shape))\n</pre> print(\"FIGPs-Lattice\") print(\"~\"*25) lattice_samplers_figps = qp.Lattice(d[0],seed=7) lattice_samplers_figps = [lattice_samplers_figps]+lattice_samplers_figps.spawn(s=levels-1,dimensions=d[1:]) figps_lattice = [     fastgps.FastGPLattice(         kernel = qp.KernelShiftInvar(             d = d[l],             torchify = True,             alpha = 2,         ),         seqs = lattice_samplers_figps[l],     )     for l in range(levels) ] figps_lattice_approx = 0  figps_lattice_std_error = 0  for l in range(levels):     x = figps_lattice[l].get_x_next(n=n[l]).numpy()     y = fs[l].f(x)     delta = y[1]-y[0]     figps_lattice[l].add_y_next(torch.from_numpy(delta))     fit_data = figps_lattice[l].fit(         loss_metric = \"MLL\",         stop_crit_improvement_threshold = 1e-3,         iterations = 100,         store_hists = True,         verbose = False,     )     fastgps.plot_fastgps_fit_data(fit_data)     figps_lattice_approx_l = figps_lattice[l].post_cubature_mean().numpy()     figps_lattice_approx += figps_lattice_approx_l     figps_lattice_std_error_l = np.sqrt(figps_lattice[l].post_cubature_var().numpy())     figps_lattice_std_error = np.sqrt(figps_lattice_std_error**2+figps_lattice_std_error_l**2)     figps_lattice_true_error_l = np.abs(figps_lattice_approx_l-exact_diffs[l])     figps_lattice_true_error = np.abs(figps_lattice_approx-exact_values[-1])     print(\"error = %-10.2e std_error = %-10.2e approx = %-10.4f error_l = %-10.2e std_error_l = %-10.2e approx_l = %-10.4f x.shape = %-15s\\ty.shape = %-15s\\tdelta.shape = %s\"\\           %(figps_lattice_true_error,figps_lattice_std_error,figps_lattice_approx,figps_lattice_true_error_l,figps_lattice_std_error_l,figps_lattice_approx_l,x.shape,y.shape,delta.shape)) <pre>FIGPs-Lattice\n~~~~~~~~~~~~~~~~~~~~~~~~~\nerror = 1.72e+00   std_error = 1.11e-03   approx = 5.8868     error_l = 8.17e-04   std_error_l = 1.11e-03   approx_l = 5.8868     x.shape = (16384, 2)     \ty.shape = (2, 16384)     \tdelta.shape = (16384,)\nerror = 7.14e-01   std_error = 3.51e-03   approx = 4.8815     error_l = 2.50e-03   std_error_l = 3.33e-03   approx_l = -1.0053    x.shape = (4096, 4)      \ty.shape = (2, 4096)      \tdelta.shape = (4096,)\nerror = 2.43e-01   std_error = 4.67e-02   approx = 4.4101     error_l = 9.10e-03   std_error_l = 4.66e-02   approx_l = -0.4714    x.shape = (1024, 8)      \ty.shape = (2, 1024)      \tdelta.shape = (1024,)\nerror = 4.89e-03   std_error = 1.02e-01   approx = 4.1720     error_l = 2.52e-03   std_error_l = 9.06e-02   approx_l = -0.2381    x.shape = (256, 16)      \ty.shape = (2, 256)       \tdelta.shape = (256,)\n</pre> In\u00a0[\u00a0]: Copied! <pre>print(\"FIGPs-Net\")\nprint(\"~\"*25)\nnet_samplers_figps = qp.DigitalNetB2(d[0],seed=7)\nnet_samplers_figps = [net_samplers_figps]+net_samplers_figps.spawn(s=levels-1,dimensions=d[1:])\nfigps_net = [\n    fastgps.FastGPDigitalNetB2(\n        kernel = qp.KernelDigShiftInvar(\n            d = d[l],\n            torchify = True,\n            alpha = 2\n        ),\n        seqs = net_samplers_figps[l],\n    )\n    for l in range(levels)\n]\nfigps_net_approx = 0 \nfigps_net_std_error = 0 \nfor l in range(levels):\n    x = figps_net[l].get_x_next(n=n[l]).numpy()\n    y = fs[l].f(x)\n    delta = y[1]-y[0]\n    figps_net[l].add_y_next(torch.from_numpy(delta))\n    fit_data = figps_net[l].fit(\n        loss_metric = \"MLL\",\n        stop_crit_improvement_threshold = 1e-3,\n        iterations = 100,\n        store_hists = True,\n        verbose = False,\n    )\n    fastgps.plot_fastgps_fit_data(fit_data)\n    figps_net_approx_l = figps_net[l].post_cubature_mean().numpy()\n    figps_net_approx += figps_net_approx_l\n    figps_net_std_error_l = np.sqrt(figps_net[l].post_cubature_var().numpy())\n    figps_net_std_error = np.sqrt(figps_net_std_error**2+figps_net_std_error_l**2)\n    figps_net_true_error_l = np.abs(figps_net_approx_l-exact_diffs[l])\n    figps_net_true_error = np.abs(figps_net_approx-exact_values[-1])\n    print(\"error = %-10.2e std_error = %-10.2e approx = %-10.4f error_l = %-10.2e std_error_l = %-10.2e approx_l = %-10.4f x.shape = %-15s\\ty.shape = %-15s\\tdelta.shape = %s\"\\\n          %(figps_net_true_error,figps_net_std_error,figps_net_approx,figps_net_true_error_l,figps_net_std_error_l,figps_net_approx_l,x.shape,y.shape,delta.shape))\n</pre> print(\"FIGPs-Net\") print(\"~\"*25) net_samplers_figps = qp.DigitalNetB2(d[0],seed=7) net_samplers_figps = [net_samplers_figps]+net_samplers_figps.spawn(s=levels-1,dimensions=d[1:]) figps_net = [     fastgps.FastGPDigitalNetB2(         kernel = qp.KernelDigShiftInvar(             d = d[l],             torchify = True,             alpha = 2         ),         seqs = net_samplers_figps[l],     )     for l in range(levels) ] figps_net_approx = 0  figps_net_std_error = 0  for l in range(levels):     x = figps_net[l].get_x_next(n=n[l]).numpy()     y = fs[l].f(x)     delta = y[1]-y[0]     figps_net[l].add_y_next(torch.from_numpy(delta))     fit_data = figps_net[l].fit(         loss_metric = \"MLL\",         stop_crit_improvement_threshold = 1e-3,         iterations = 100,         store_hists = True,         verbose = False,     )     fastgps.plot_fastgps_fit_data(fit_data)     figps_net_approx_l = figps_net[l].post_cubature_mean().numpy()     figps_net_approx += figps_net_approx_l     figps_net_std_error_l = np.sqrt(figps_net[l].post_cubature_var().numpy())     figps_net_std_error = np.sqrt(figps_net_std_error**2+figps_net_std_error_l**2)     figps_net_true_error_l = np.abs(figps_net_approx_l-exact_diffs[l])     figps_net_true_error = np.abs(figps_net_approx-exact_values[-1])     print(\"error = %-10.2e std_error = %-10.2e approx = %-10.4f error_l = %-10.2e std_error_l = %-10.2e approx_l = %-10.4f x.shape = %-15s\\ty.shape = %-15s\\tdelta.shape = %s\"\\           %(figps_net_true_error,figps_net_std_error,figps_net_approx,figps_net_true_error_l,figps_net_std_error_l,figps_net_approx_l,x.shape,y.shape,delta.shape)) <pre>FIGPs-Net\n~~~~~~~~~~~~~~~~~~~~~~~~~\nerror = 1.72e+00   std_error = 8.61e-04   approx = 5.8849     error_l = 1.04e-03   std_error_l = 8.61e-04   approx_l = 5.8849     x.shape = (16384, 2)     \ty.shape = (2, 16384)     \tdelta.shape = (16384,)\nerror = 7.16e-01   std_error = 2.02e-03   approx = 4.8827     error_l = 5.99e-04   std_error_l = 1.82e-03   approx_l = -1.0022    x.shape = (4096, 4)      \ty.shape = (2, 4096)      \tdelta.shape = (4096,)\nerror = 2.35e-01   std_error = 4.38e-03   approx = 4.4023     error_l = 6.96e-05   std_error_l = 3.89e-03   approx_l = -0.4804    x.shape = (1024, 8)      \ty.shape = (2, 1024)      \tdelta.shape = (1024,)\nerror = 3.42e-03   std_error = 1.26e-02   approx = 4.1637     error_l = 3.05e-03   std_error_l = 1.18e-02   approx_l = -0.2386    x.shape = (256, 16)      \ty.shape = (2, 256)       \tdelta.shape = (256,)\n</pre> In\u00a0[17]: Copied! <pre>print(\"FMTGPs-Diff-Lattice\")\nprint(\"~\"*25)\nlattice_samplers_mtgps = qp.Lattice(d[-1],seed=7)\nlattice_samplers_mtgps = [lattice_samplers_mtgps]+lattice_samplers_mtgps.spawn(s=levels-1)\nfmtgpds_lattice = fastgps.FastGPLattice(\n    kernel = qp.KernelMultiTask(\n        qp.KernelShiftInvar(\n            d = d[-1],\n            torchify = True,\n            alpha = 2,\n        ),\n        num_tasks = levels,\n        rank_factor = levels,\n    ),\n    seqs = lattice_samplers_mtgps,\n)\nfor l in range(levels):\n    x = fmtgpds_lattice.get_x_next(n=n[l],task=l).numpy()\n    y = fs[l].f(x[...,:d[l]])\n    delta = y[1]-y[0]\n    fmtgpds_lattice.add_y_next(torch.from_numpy(delta),task=l)\nfit_data = fmtgpds_lattice.fit(\n    loss_metric = \"MLL\",\n    stop_crit_improvement_threshold = 1e-3,\n    iterations = 100,\n    store_hists = True,\n    verbose = 10,\n)\nfastgps.plot_fastgps_fit_data(fit_data)\nfmtgpds_lattice_means = fmtgpds_lattice.post_cubature_mean().numpy()\nfmtgpds_lattice_covs = fmtgpds_lattice.post_cubature_cov().numpy()\nfmtgpds_lattice_weights = np.ones(levels) \nfmtgpds_lattice_optimal_weights_unscaled = np.linalg.solve(fmtgpds_lattice_covs+fmtgpds_lattice_means[:,None]*fmtgpds_lattice_means[None,:],fmtgpds_lattice_means)\nfmtgpds_lattice_optimal_weights = (fmtgpds_lattice_weights*fmtgpds_lattice_means).sum()*fmtgpds_lattice_optimal_weights_unscaled\nfmtgpds_lattice_approx = (fmtgpds_lattice_optimal_weights*fmtgpds_lattice_means).sum()\nfmtgpds_lattice_true_error = np.abs(fmtgpds_lattice_approx-exact_values[-1])\nfmtgpds_lattice_std_error = np.sqrt(fmtgpds_lattice_optimal_weights@fmtgpds_lattice_covs@fmtgpds_lattice_optimal_weights)\nfor l in range(levels):\n    fmtgpds_lattice_weights_l = (np.arange(levels)&lt;=l).astype(int)\n    fmtgpds_lattice_optimal_weights_l = (fmtgpds_lattice_weights_l*fmtgpds_lattice_means).sum()*fmtgpds_lattice_optimal_weights_unscaled\n    fmtgpds_lattice_approx_l = (fmtgpds_lattice_optimal_weights_l*fmtgpds_lattice_means).sum()\n    fmtgpds_lattice_std_error_l = np.sqrt(fmtgpds_lattice_optimal_weights_l@fmtgpds_lattice_covs@fmtgpds_lattice_optimal_weights_l)\n    fmtgpds_lattice_true_error_l = np.abs(fmtgpds_lattice_approx_l-exact_values[l+1])\n    print(\"error = %-10.2e std_error = %-10.2e approx = %-10.4f error_l = %-10.2e std_error_l = %-10.2e approx_l = %-10.4f\"\\\n          %(fmtgpds_lattice_true_error,fmtgpds_lattice_std_error,fmtgpds_lattice_approx,fmtgpds_lattice_true_error_l,fmtgpds_lattice_std_error_l,fmtgpds_lattice_approx_l))\n</pre> print(\"FMTGPs-Diff-Lattice\") print(\"~\"*25) lattice_samplers_mtgps = qp.Lattice(d[-1],seed=7) lattice_samplers_mtgps = [lattice_samplers_mtgps]+lattice_samplers_mtgps.spawn(s=levels-1) fmtgpds_lattice = fastgps.FastGPLattice(     kernel = qp.KernelMultiTask(         qp.KernelShiftInvar(             d = d[-1],             torchify = True,             alpha = 2,         ),         num_tasks = levels,         rank_factor = levels,     ),     seqs = lattice_samplers_mtgps, ) for l in range(levels):     x = fmtgpds_lattice.get_x_next(n=n[l],task=l).numpy()     y = fs[l].f(x[...,:d[l]])     delta = y[1]-y[0]     fmtgpds_lattice.add_y_next(torch.from_numpy(delta),task=l) fit_data = fmtgpds_lattice.fit(     loss_metric = \"MLL\",     stop_crit_improvement_threshold = 1e-3,     iterations = 100,     store_hists = True,     verbose = 10, ) fastgps.plot_fastgps_fit_data(fit_data) fmtgpds_lattice_means = fmtgpds_lattice.post_cubature_mean().numpy() fmtgpds_lattice_covs = fmtgpds_lattice.post_cubature_cov().numpy() fmtgpds_lattice_weights = np.ones(levels)  fmtgpds_lattice_optimal_weights_unscaled = np.linalg.solve(fmtgpds_lattice_covs+fmtgpds_lattice_means[:,None]*fmtgpds_lattice_means[None,:],fmtgpds_lattice_means) fmtgpds_lattice_optimal_weights = (fmtgpds_lattice_weights*fmtgpds_lattice_means).sum()*fmtgpds_lattice_optimal_weights_unscaled fmtgpds_lattice_approx = (fmtgpds_lattice_optimal_weights*fmtgpds_lattice_means).sum() fmtgpds_lattice_true_error = np.abs(fmtgpds_lattice_approx-exact_values[-1]) fmtgpds_lattice_std_error = np.sqrt(fmtgpds_lattice_optimal_weights@fmtgpds_lattice_covs@fmtgpds_lattice_optimal_weights) for l in range(levels):     fmtgpds_lattice_weights_l = (np.arange(levels)&lt;=l).astype(int)     fmtgpds_lattice_optimal_weights_l = (fmtgpds_lattice_weights_l*fmtgpds_lattice_means).sum()*fmtgpds_lattice_optimal_weights_unscaled     fmtgpds_lattice_approx_l = (fmtgpds_lattice_optimal_weights_l*fmtgpds_lattice_means).sum()     fmtgpds_lattice_std_error_l = np.sqrt(fmtgpds_lattice_optimal_weights_l@fmtgpds_lattice_covs@fmtgpds_lattice_optimal_weights_l)     fmtgpds_lattice_true_error_l = np.abs(fmtgpds_lattice_approx_l-exact_values[l+1])     print(\"error = %-10.2e std_error = %-10.2e approx = %-10.4f error_l = %-10.2e std_error_l = %-10.2e approx_l = %-10.4f\"\\           %(fmtgpds_lattice_true_error,fmtgpds_lattice_std_error,fmtgpds_lattice_approx,fmtgpds_lattice_true_error_l,fmtgpds_lattice_std_error_l,fmtgpds_lattice_approx_l)) <pre>FMTGPs-Diff-Lattice\n~~~~~~~~~~~~~~~~~~~~~~~~~\n     iter of 1.0e+02 | best loss  | loss      \n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n            0.00e+00 | 2.05e+05   | 2.05e+05  \n            1.00e+01 | 6.79e+04   | 6.79e+04  \n            2.00e+01 | 6.51e+04   | 6.52e+04  \n            3.00e+01 | 6.36e+04   | 6.36e+04  \n            4.00e+01 | 6.26e+04   | 6.26e+04  \n            5.00e+01 | 6.21e+04   | 6.21e+04  \n            6.00e+01 | 6.13e+04   | 6.13e+04  \n            7.00e+01 | 6.07e+04   | 6.07e+04  \n            8.00e+01 | 6.04e+04   | 6.04e+04  \n            9.00e+01 | 6.03e+04   | 6.03e+04  \n            1.00e+02 | 6.03e+04   | 6.03e+04  \nerror = 1.79e-03   std_error = 1.25e-02   approx = 4.1689     error_l = 7.65e-04   std_error_l = 1.76e-02   approx_l = 5.8867    \nerror = 1.79e-03   std_error = 1.25e-02   approx = 4.1689     error_l = 2.71e-03   std_error_l = 1.46e-02   approx_l = 4.8805    \nerror = 1.79e-03   std_error = 1.25e-02   approx = 4.1689     error_l = 6.65e-03   std_error_l = 1.32e-02   approx_l = 4.4093    \nerror = 1.79e-03   std_error = 1.25e-02   approx = 4.1689     error_l = 1.79e-03   std_error_l = 1.25e-02   approx_l = 4.1689    \n</pre> In\u00a0[18]: Copied! <pre>print(\"FMTGPs-Diff-Net\")\nprint(\"~\"*25)\nnet_samplers_mtgps = qp.DigitalNetB2(d[-1],randomize=\"DS\",seed=7)\nnet_samplers_mtgps = [net_samplers_mtgps]+net_samplers_mtgps.spawn(s=levels-1)\nfmtgpds_net = fastgps.FastGPDigitalNetB2(\n    kernel = qp.KernelMultiTask(\n        qp.KernelDigShiftInvar(\n            d = d[-1],\n            torchify = True,\n            alpha = 2,\n        ),\n        num_tasks = levels,\n        rank_factor = levels,\n    ),\n    seqs = net_samplers_mtgps,\n)\nfor l in range(levels):\n    x = fmtgpds_net.get_x_next(n=n[l],task=l).numpy()\n    y = fs[l].f(x[...,:d[l]])\n    delta = y[1]-y[0]\n    fmtgpds_net.add_y_next(torch.from_numpy(delta),task=l)\nfit_data = fmtgpds_net.fit(\n    loss_metric = \"MLL\",\n    stop_crit_improvement_threshold = 1e-3,\n    iterations = 100,\n    store_hists = True,\n    verbose = 10,\n)\nfastgps.plot_fastgps_fit_data(fit_data)\nfmtgpds_net_means = fmtgpds_net.post_cubature_mean().numpy()\nfmtgpds_net_covs = fmtgpds_net.post_cubature_cov().numpy()\nfmtgpds_net_weights = np.ones(levels) \nfmtgpds_net_optimal_weights_unscaled = np.linalg.solve(fmtgpds_net_covs+fmtgpds_net_means[:,None]*fmtgpds_net_means[None,:],fmtgpds_net_means)\nfmtgpds_net_optimal_weights = (fmtgpds_net_weights*fmtgpds_net_means).sum()*fmtgpds_net_optimal_weights_unscaled\nfmtgpds_net_approx = (fmtgpds_net_optimal_weights*fmtgpds_net_means).sum()\nfmtgpds_net_true_error = np.abs(fmtgpds_net_approx-exact_values[-1])\nfmtgpds_net_std_error = np.sqrt(fmtgpds_net_optimal_weights@fmtgpds_net_covs@fmtgpds_net_optimal_weights)\nfor l in range(levels):\n    fmtgpds_net_weights_l = (np.arange(levels)&lt;=l).astype(int)\n    fmtgpds_net_optimal_weights_l = (fmtgpds_net_weights_l*fmtgpds_net_means).sum()*fmtgpds_net_optimal_weights_unscaled\n    fmtgpds_net_approx_l = (fmtgpds_net_optimal_weights_l*fmtgpds_net_means).sum()\n    fmtgpds_net_std_error_l = np.sqrt(fmtgpds_net_optimal_weights_l@fmtgpds_net_covs@fmtgpds_net_optimal_weights_l)\n    fmtgpds_net_true_error_l = np.abs(fmtgpds_net_approx_l-exact_values[l+1])\n    print(\"error = %-10.2e std_error = %-10.2e approx = %-10.4f error_l = %-10.2e std_error_l = %-10.2e approx_l = %-10.4f\"\\\n          %(fmtgpds_net_true_error,fmtgpds_net_std_error,fmtgpds_net_approx,fmtgpds_net_true_error_l,fmtgpds_net_std_error_l,fmtgpds_net_approx_l))\n</pre> print(\"FMTGPs-Diff-Net\") print(\"~\"*25) net_samplers_mtgps = qp.DigitalNetB2(d[-1],randomize=\"DS\",seed=7) net_samplers_mtgps = [net_samplers_mtgps]+net_samplers_mtgps.spawn(s=levels-1) fmtgpds_net = fastgps.FastGPDigitalNetB2(     kernel = qp.KernelMultiTask(         qp.KernelDigShiftInvar(             d = d[-1],             torchify = True,             alpha = 2,         ),         num_tasks = levels,         rank_factor = levels,     ),     seqs = net_samplers_mtgps, ) for l in range(levels):     x = fmtgpds_net.get_x_next(n=n[l],task=l).numpy()     y = fs[l].f(x[...,:d[l]])     delta = y[1]-y[0]     fmtgpds_net.add_y_next(torch.from_numpy(delta),task=l) fit_data = fmtgpds_net.fit(     loss_metric = \"MLL\",     stop_crit_improvement_threshold = 1e-3,     iterations = 100,     store_hists = True,     verbose = 10, ) fastgps.plot_fastgps_fit_data(fit_data) fmtgpds_net_means = fmtgpds_net.post_cubature_mean().numpy() fmtgpds_net_covs = fmtgpds_net.post_cubature_cov().numpy() fmtgpds_net_weights = np.ones(levels)  fmtgpds_net_optimal_weights_unscaled = np.linalg.solve(fmtgpds_net_covs+fmtgpds_net_means[:,None]*fmtgpds_net_means[None,:],fmtgpds_net_means) fmtgpds_net_optimal_weights = (fmtgpds_net_weights*fmtgpds_net_means).sum()*fmtgpds_net_optimal_weights_unscaled fmtgpds_net_approx = (fmtgpds_net_optimal_weights*fmtgpds_net_means).sum() fmtgpds_net_true_error = np.abs(fmtgpds_net_approx-exact_values[-1]) fmtgpds_net_std_error = np.sqrt(fmtgpds_net_optimal_weights@fmtgpds_net_covs@fmtgpds_net_optimal_weights) for l in range(levels):     fmtgpds_net_weights_l = (np.arange(levels)&lt;=l).astype(int)     fmtgpds_net_optimal_weights_l = (fmtgpds_net_weights_l*fmtgpds_net_means).sum()*fmtgpds_net_optimal_weights_unscaled     fmtgpds_net_approx_l = (fmtgpds_net_optimal_weights_l*fmtgpds_net_means).sum()     fmtgpds_net_std_error_l = np.sqrt(fmtgpds_net_optimal_weights_l@fmtgpds_net_covs@fmtgpds_net_optimal_weights_l)     fmtgpds_net_true_error_l = np.abs(fmtgpds_net_approx_l-exact_values[l+1])     print(\"error = %-10.2e std_error = %-10.2e approx = %-10.4f error_l = %-10.2e std_error_l = %-10.2e approx_l = %-10.4f\"\\           %(fmtgpds_net_true_error,fmtgpds_net_std_error,fmtgpds_net_approx,fmtgpds_net_true_error_l,fmtgpds_net_std_error_l,fmtgpds_net_approx_l)) <pre>FMTGPs-Diff-Net\n~~~~~~~~~~~~~~~~~~~~~~~~~\n     iter of 1.0e+02 | best loss  | loss      \n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n            0.00e+00 | 6.24e+04   | 6.24e+04  \n            1.00e+01 | 3.51e+04   | 3.51e+04  \n            2.00e+01 | 1.99e+04   | 1.99e+04  \n            3.00e+01 | 1.72e+04   | 1.72e+04  \n            4.00e+01 | 1.63e+04   | 1.63e+04  \n            5.00e+01 | 1.58e+04   | 1.58e+04  \n            6.00e+01 | 1.56e+04   | 1.56e+04  \n            7.00e+01 | 1.55e+04   | 1.55e+04  \n            8.00e+01 | 1.54e+04   | 1.54e+04  \n            9.00e+01 | 1.54e+04   | 1.54e+04  \n            1.00e+02 | 1.53e+04   | 1.53e+04  \nerror = 1.45e-03   std_error = 4.16e-02   approx = 4.1686     error_l = 2.83e-04   std_error_l = 5.87e-02   approx_l = 5.8862    \nerror = 1.45e-03   std_error = 4.16e-02   approx = 4.1686     error_l = 7.04e-04   std_error_l = 4.87e-02   approx_l = 4.8839    \nerror = 1.45e-03   std_error = 4.16e-02   approx = 4.1686     error_l = 1.50e-03   std_error_l = 4.39e-02   approx_l = 4.4042    \nerror = 1.45e-03   std_error = 4.16e-02   approx = 4.1686     error_l = 1.45e-03   std_error_l = 4.16e-02   approx_l = 4.1686    \n</pre> In\u00a0[19]: Copied! <pre>print(\"FMTGPs-Func-Lattice\")\nprint(\"~\"*25)\nfmtgpfs_lattice = fastgps.FastGPLattice(\n    kernel = qp.KernelMultiTask(\n        qp.KernelShiftInvar(\n            d = d[-1],\n            torchify = True,\n            alpha = 2,\n        ),\n        num_tasks = levels,\n        rank_factor = levels,\n    ),\n    seqs = lattice_samplers_mtgps,\n)\nfor l in range(levels):\n    x = fmtgpfs_lattice.get_x_next(n=n[l],task=l).numpy()\n    y = fs[l].f(x[...,:d[l]])\n    delta = y[1]\n    fmtgpfs_lattice.add_y_next(torch.from_numpy(delta),task=l)\nfit_data = fmtgpfs_lattice.fit(\n    loss_metric = \"MLL\",\n    stop_crit_improvement_threshold = 1e-0,\n    iterations = 100,\n    store_hists = True,\n    verbose = 10,\n)\nfmtgpfs_lattice_means = fmtgpfs_lattice.post_cubature_mean().numpy()\nfmtgpfs_lattice_covs = fmtgpfs_lattice.post_cubature_cov().numpy()\nfmtgpfs_lattice_weights = (np.arange(levels)==(levels-1)).astype(int)\nfmtgpfs_lattice_optimal_weights_unscaled = np.linalg.solve(fmtgpfs_lattice_covs+fmtgpfs_lattice_means[:,None]*fmtgpfs_lattice_means[None,:],fmtgpfs_lattice_means)\nfmtgpfs_lattice_optimal_weights = (fmtgpfs_lattice_weights*fmtgpfs_lattice_means).sum()*fmtgpfs_lattice_optimal_weights_unscaled\nfmtgpfs_lattice_approx = (fmtgpfs_lattice_optimal_weights*fmtgpfs_lattice_means).sum()\nfmtgpfs_lattice_true_error = np.abs(fmtgpfs_lattice_approx-exact_values[-1])\nfmtgpfs_lattice_std_error = np.sqrt(fmtgpfs_lattice_optimal_weights@fmtgpfs_lattice_covs@fmtgpfs_lattice_optimal_weights)\nfor l in range(levels):\n    fmtgpfs_lattice_weights_l = (np.arange(levels)==l).astype(int)\n    fmtgpfs_lattice_optimal_weights_l = (fmtgpfs_lattice_weights_l*fmtgpfs_lattice_means).sum()*fmtgpfs_lattice_optimal_weights_unscaled\n    fmtgpfs_lattice_approx_l = (fmtgpfs_lattice_optimal_weights_l*fmtgpfs_lattice_means).sum()\n    fmtgpfs_lattice_std_error_l = np.sqrt(fmtgpfs_lattice_optimal_weights_l@fmtgpfs_lattice_covs@fmtgpfs_lattice_optimal_weights_l)\n    fmtgpfs_lattice_true_error_l = np.abs(fmtgpfs_lattice_approx_l-exact_values[l+1])\n    print(\"error = %-10.2e std_error = %-10.2e approx = %-10.4f error_l = %-10.2e std_error_l = %-10.2e approx_l = %-10.4f\"\\\n          %(fmtgpfs_lattice_true_error,fmtgpfs_lattice_std_error,fmtgpfs_lattice_approx,fmtgpfs_lattice_true_error_l,fmtgpfs_lattice_std_error_l,fmtgpfs_lattice_approx_l))\n</pre> print(\"FMTGPs-Func-Lattice\") print(\"~\"*25) fmtgpfs_lattice = fastgps.FastGPLattice(     kernel = qp.KernelMultiTask(         qp.KernelShiftInvar(             d = d[-1],             torchify = True,             alpha = 2,         ),         num_tasks = levels,         rank_factor = levels,     ),     seqs = lattice_samplers_mtgps, ) for l in range(levels):     x = fmtgpfs_lattice.get_x_next(n=n[l],task=l).numpy()     y = fs[l].f(x[...,:d[l]])     delta = y[1]     fmtgpfs_lattice.add_y_next(torch.from_numpy(delta),task=l) fit_data = fmtgpfs_lattice.fit(     loss_metric = \"MLL\",     stop_crit_improvement_threshold = 1e-0,     iterations = 100,     store_hists = True,     verbose = 10, ) fmtgpfs_lattice_means = fmtgpfs_lattice.post_cubature_mean().numpy() fmtgpfs_lattice_covs = fmtgpfs_lattice.post_cubature_cov().numpy() fmtgpfs_lattice_weights = (np.arange(levels)==(levels-1)).astype(int) fmtgpfs_lattice_optimal_weights_unscaled = np.linalg.solve(fmtgpfs_lattice_covs+fmtgpfs_lattice_means[:,None]*fmtgpfs_lattice_means[None,:],fmtgpfs_lattice_means) fmtgpfs_lattice_optimal_weights = (fmtgpfs_lattice_weights*fmtgpfs_lattice_means).sum()*fmtgpfs_lattice_optimal_weights_unscaled fmtgpfs_lattice_approx = (fmtgpfs_lattice_optimal_weights*fmtgpfs_lattice_means).sum() fmtgpfs_lattice_true_error = np.abs(fmtgpfs_lattice_approx-exact_values[-1]) fmtgpfs_lattice_std_error = np.sqrt(fmtgpfs_lattice_optimal_weights@fmtgpfs_lattice_covs@fmtgpfs_lattice_optimal_weights) for l in range(levels):     fmtgpfs_lattice_weights_l = (np.arange(levels)==l).astype(int)     fmtgpfs_lattice_optimal_weights_l = (fmtgpfs_lattice_weights_l*fmtgpfs_lattice_means).sum()*fmtgpfs_lattice_optimal_weights_unscaled     fmtgpfs_lattice_approx_l = (fmtgpfs_lattice_optimal_weights_l*fmtgpfs_lattice_means).sum()     fmtgpfs_lattice_std_error_l = np.sqrt(fmtgpfs_lattice_optimal_weights_l@fmtgpfs_lattice_covs@fmtgpfs_lattice_optimal_weights_l)     fmtgpfs_lattice_true_error_l = np.abs(fmtgpfs_lattice_approx_l-exact_values[l+1])     print(\"error = %-10.2e std_error = %-10.2e approx = %-10.4f error_l = %-10.2e std_error_l = %-10.2e approx_l = %-10.4f\"\\           %(fmtgpfs_lattice_true_error,fmtgpfs_lattice_std_error,fmtgpfs_lattice_approx,fmtgpfs_lattice_true_error_l,fmtgpfs_lattice_std_error_l,fmtgpfs_lattice_approx_l)) <pre>FMTGPs-Func-Lattice\n~~~~~~~~~~~~~~~~~~~~~~~~~\n     iter of 1.0e+02 | best loss  | loss      \n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n            0.00e+00 | 2.04e+05   | 2.04e+05  \n            1.00e+01 | 7.20e+04   | 7.20e+04  \n            2.00e+01 | 6.78e+04   | 6.78e+04  \n            3.00e+01 | 6.64e+04   | 6.64e+04  \n            4.00e+01 | 6.56e+04   | 6.56e+04  \n            5.00e+01 | 6.41e+04   | 6.41e+04  \n            6.00e+01 | 6.28e+04   | 6.28e+04  \n            7.00e+01 | 6.26e+04   | 6.26e+04  \n            8.00e+01 | 6.26e+04   | 6.26e+04  \n            9.00e+01 | 6.25e+04   | 6.25e+04  \n            1.00e+02 | 6.25e+04   | 6.25e+04  \nerror = 7.30e-02   std_error = nan        approx = 4.0941     error_l = 8.17e-04   std_error_l = nan        approx_l = 5.8868    \nerror = 7.30e-02   std_error = nan        approx = 4.0941     error_l = 3.81e-03   std_error_l = nan        approx_l = 4.8794    \nerror = 7.30e-02   std_error = nan        approx = 4.0941     error_l = 3.00e-02   std_error_l = nan        approx_l = 4.3727    \nerror = 7.30e-02   std_error = nan        approx = 4.0941     error_l = 7.30e-02   std_error_l = nan        approx_l = 4.0941    \n</pre> <pre>/var/folders/xk/w1s5c54x0zv90dgqk3vpmbsw004hmz/T/ipykernel_76766/3230475336.py:34: RuntimeWarning: invalid value encountered in sqrt\n  fmtgpfs_lattice_std_error = np.sqrt(fmtgpfs_lattice_optimal_weights@fmtgpfs_lattice_covs@fmtgpfs_lattice_optimal_weights)\n/var/folders/xk/w1s5c54x0zv90dgqk3vpmbsw004hmz/T/ipykernel_76766/3230475336.py:39: RuntimeWarning: invalid value encountered in sqrt\n  fmtgpfs_lattice_std_error_l = np.sqrt(fmtgpfs_lattice_optimal_weights_l@fmtgpfs_lattice_covs@fmtgpfs_lattice_optimal_weights_l)\n</pre> In\u00a0[20]: Copied! <pre>print(\"FMTGPs-Func-Net\")\nprint(\"~\"*25)\nfmtgpfs_net = fastgps.FastGPDigitalNetB2(\n    kernel = qp.KernelMultiTask(\n        qp.KernelDigShiftInvar(\n            d = d[-1],\n            torchify = True,\n            alpha = 2,\n        ),\n        num_tasks = levels,\n        rank_factor = levels,\n    ),\n    seqs = net_samplers_mtgps,\n)\nfor l in range(levels):\n    x = fmtgpfs_net.get_x_next(n=n[l],task=l).numpy()\n    y = fs[l].f(x[...,:d[l]])\n    delta = y[1]\n    fmtgpfs_net.add_y_next(torch.from_numpy(delta),task=l)\nfit_data = fmtgpfs_net.fit(\n    loss_metric = \"MLL\",\n    stop_crit_improvement_threshold = 1e-0,\n    iterations = 100,\n    store_hists = True,\n    verbose = 10,\n)\nfastgps.plot_fastgps_fit_data(fit_data)\nfmtgpfs_net_means = fmtgpfs_net.post_cubature_mean().numpy()\nfmtgpfs_net_covs = fmtgpfs_net.post_cubature_cov().numpy()\nfmtgpfs_net_weights = (np.arange(levels)==(levels-1)).astype(int)\nfmtgpfs_net_optimal_weights_unscaled = np.linalg.solve(fmtgpfs_net_covs+fmtgpfs_net_means[:,None]*fmtgpfs_net_means[None,:],fmtgpfs_net_means)\nfmtgpfs_net_optimal_weights = (fmtgpfs_net_weights*fmtgpfs_net_means).sum()*fmtgpfs_net_optimal_weights_unscaled\nfmtgpfs_net_approx = (fmtgpfs_net_optimal_weights*fmtgpfs_net_means).sum()\nfmtgpfs_net_true_error = np.abs(fmtgpfs_net_approx-exact_values[-1])\nfmtgpfs_net_std_error = np.sqrt(fmtgpfs_net_optimal_weights@fmtgpfs_net_covs@fmtgpfs_net_optimal_weights)\nfor l in range(levels):\n    fmtgpfs_net_weights_l = (np.arange(levels)==l).astype(int)\n    fmtgpfs_net_optimal_weights_l = (fmtgpfs_net_weights_l*fmtgpfs_net_means).sum()*fmtgpfs_net_optimal_weights_unscaled\n    fmtgpfs_net_approx_l = (fmtgpfs_net_optimal_weights_l*fmtgpfs_net_means).sum()\n    fmtgpfs_net_std_error_l = np.sqrt(fmtgpfs_net_optimal_weights_l@fmtgpfs_net_covs@fmtgpfs_net_optimal_weights_l)\n    fmtgpfs_net_true_error_l = np.abs(fmtgpfs_net_approx_l-exact_values[l+1])\n    print(\"error = %-10.2e std_error = %-10.2e approx = %-10.4f error_l = %-10.2e std_error_l = %-10.2e approx_l = %-10.4f\"\\\n          %(fmtgpfs_net_true_error,fmtgpfs_net_std_error,fmtgpfs_net_approx,fmtgpfs_net_true_error_l,fmtgpfs_net_std_error_l,fmtgpfs_net_approx_l))\n</pre> print(\"FMTGPs-Func-Net\") print(\"~\"*25) fmtgpfs_net = fastgps.FastGPDigitalNetB2(     kernel = qp.KernelMultiTask(         qp.KernelDigShiftInvar(             d = d[-1],             torchify = True,             alpha = 2,         ),         num_tasks = levels,         rank_factor = levels,     ),     seqs = net_samplers_mtgps, ) for l in range(levels):     x = fmtgpfs_net.get_x_next(n=n[l],task=l).numpy()     y = fs[l].f(x[...,:d[l]])     delta = y[1]     fmtgpfs_net.add_y_next(torch.from_numpy(delta),task=l) fit_data = fmtgpfs_net.fit(     loss_metric = \"MLL\",     stop_crit_improvement_threshold = 1e-0,     iterations = 100,     store_hists = True,     verbose = 10, ) fastgps.plot_fastgps_fit_data(fit_data) fmtgpfs_net_means = fmtgpfs_net.post_cubature_mean().numpy() fmtgpfs_net_covs = fmtgpfs_net.post_cubature_cov().numpy() fmtgpfs_net_weights = (np.arange(levels)==(levels-1)).astype(int) fmtgpfs_net_optimal_weights_unscaled = np.linalg.solve(fmtgpfs_net_covs+fmtgpfs_net_means[:,None]*fmtgpfs_net_means[None,:],fmtgpfs_net_means) fmtgpfs_net_optimal_weights = (fmtgpfs_net_weights*fmtgpfs_net_means).sum()*fmtgpfs_net_optimal_weights_unscaled fmtgpfs_net_approx = (fmtgpfs_net_optimal_weights*fmtgpfs_net_means).sum() fmtgpfs_net_true_error = np.abs(fmtgpfs_net_approx-exact_values[-1]) fmtgpfs_net_std_error = np.sqrt(fmtgpfs_net_optimal_weights@fmtgpfs_net_covs@fmtgpfs_net_optimal_weights) for l in range(levels):     fmtgpfs_net_weights_l = (np.arange(levels)==l).astype(int)     fmtgpfs_net_optimal_weights_l = (fmtgpfs_net_weights_l*fmtgpfs_net_means).sum()*fmtgpfs_net_optimal_weights_unscaled     fmtgpfs_net_approx_l = (fmtgpfs_net_optimal_weights_l*fmtgpfs_net_means).sum()     fmtgpfs_net_std_error_l = np.sqrt(fmtgpfs_net_optimal_weights_l@fmtgpfs_net_covs@fmtgpfs_net_optimal_weights_l)     fmtgpfs_net_true_error_l = np.abs(fmtgpfs_net_approx_l-exact_values[l+1])     print(\"error = %-10.2e std_error = %-10.2e approx = %-10.4f error_l = %-10.2e std_error_l = %-10.2e approx_l = %-10.4f\"\\           %(fmtgpfs_net_true_error,fmtgpfs_net_std_error,fmtgpfs_net_approx,fmtgpfs_net_true_error_l,fmtgpfs_net_std_error_l,fmtgpfs_net_approx_l)) <pre>FMTGPs-Func-Net\n~~~~~~~~~~~~~~~~~~~~~~~~~\n     iter of 1.0e+02 | best loss  | loss      \n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n            0.00e+00 | 5.08e+04   | 5.08e+04  \n            1.00e+01 | 2.19e+04   | 2.19e+04  \n            2.00e+01 | 9.04e+03   | 9.13e+03  \n            3.00e+01 | 8.86e+03   | 8.86e+03  \n            4.00e+01 | 8.80e+03   | 8.80e+03  \n            5.00e+01 | 8.70e+03   | 8.70e+03  \n            6.00e+01 | 8.54e+03   | 8.69e+03  \n            6.80e+01 | 8.54e+03   | 8.54e+03  \nerror = 7.54e-03   std_error = 1.04e-04   approx = 4.1747     error_l = 8.69e-04   std_error_l = 1.46e-04   approx_l = 5.8868    \nerror = 7.54e-03   std_error = 1.04e-04   approx = 4.1747     error_l = 2.43e-03   std_error_l = 1.21e-04   approx_l = 4.8856    \nerror = 7.54e-03   std_error = 1.04e-04   approx = 4.1747     error_l = 3.76e-03   std_error_l = 1.09e-04   approx_l = 4.3989    \nerror = 7.54e-03   std_error = 1.04e-04   approx = 4.1747     error_l = 7.54e-03   std_error_l = 1.04e-04   approx_l = 4.1747    \n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/mlqmc/comparison/#multilevel-quasi-monte-carlo-comparison","title":"Multilevel Quasi-Monte Carlo Comparison\u00b6","text":""},{"location":"examples/mlqmc/comparison/#setup","title":"Setup\u00b6","text":""},{"location":"examples/mlqmc/comparison/#problem","title":"Problem\u00b6","text":""},{"location":"examples/mlqmc/comparison/#iid-monte-carlo","title":"IID Monte Carlo\u00b6","text":""},{"location":"examples/mlqmc/comparison/#qmc-and-mlqmc","title":"QMC and MLQMC\u00b6","text":""},{"location":"examples/mlqmc/comparison/#lattice","title":"Lattice\u00b6","text":""},{"location":"examples/mlqmc/comparison/#net","title":"Net\u00b6","text":""},{"location":"examples/mlqmc/comparison/#fast-independent-gps","title":"Fast Independent GPs\u00b6","text":""},{"location":"examples/mlqmc/comparison/#lattice","title":"Lattice\u00b6","text":""},{"location":"examples/mlqmc/comparison/#net","title":"Net\u00b6","text":""},{"location":"examples/mlqmc/comparison/#fast-multitask-gps-modeling-differences","title":"Fast Multitask GPs modeling Differences\u00b6","text":""},{"location":"examples/mlqmc/comparison/#lattice","title":"Lattice\u00b6","text":""},{"location":"examples/mlqmc/comparison/#net","title":"Net\u00b6","text":""},{"location":"examples/mlqmc/comparison/#fast-multitask-gps-modeling-functions","title":"Fast Multitask GPs modeling Functions\u00b6","text":""},{"location":"examples/mlqmc/comparison/#lattice","title":"Lattice\u00b6","text":""},{"location":"examples/mlqmc/comparison/#net","title":"Net\u00b6","text":""},{"location":"examples/mlqmc/visual/","title":"Visualization","text":"In\u00a0[1]: Copied! <pre>import fastgps \nimport qmcpy as qp \nimport torch \ntorch.set_default_dtype(torch.float64)\nimport numpy as np\nfrom scipy.sparse import diags\nfrom scipy.sparse.linalg import spsolve\nfrom scipy.stats import norm\nimport itertools\nimport time\nimport pandas as pd\nimport matplotlib\nfrom matplotlib import pyplot\npyplot.style.use(\"seaborn-v0_8-whitegrid\")\nCOLORS = [\"xkcd:\"+color[:-1] for color in pd.read_csv(\"../../../xkcd_colors.txt\",comment=\"#\",header=None).iloc[:,0].tolist()][::-1]\npyplot.rcParams['axes.prop_cycle'] = matplotlib.cycler(color=COLORS)\nLINESTYLES = ['solid','dotted','dashed','dashdot',(0, (1, 1))]\nDEFAULTFONTSIZE = 30\npyplot.rcParams['xtick.labelsize'] = DEFAULTFONTSIZE\npyplot.rcParams['ytick.labelsize'] = DEFAULTFONTSIZE\npyplot.rcParams['ytick.labelsize'] = DEFAULTFONTSIZE\npyplot.rcParams['axes.titlesize'] = DEFAULTFONTSIZE\npyplot.rcParams['figure.titlesize'] = DEFAULTFONTSIZE\npyplot.rcParams[\"axes.labelsize\"] = DEFAULTFONTSIZE\npyplot.rcParams['legend.fontsize'] = DEFAULTFONTSIZE\npyplot.rcParams['font.size'] = DEFAULTFONTSIZE\npyplot.rcParams['lines.linewidth'] = 5\npyplot.rcParams['lines.markersize'] = 15\nPW = 30 # inches\n</pre> import fastgps  import qmcpy as qp  import torch  torch.set_default_dtype(torch.float64) import numpy as np from scipy.sparse import diags from scipy.sparse.linalg import spsolve from scipy.stats import norm import itertools import time import pandas as pd import matplotlib from matplotlib import pyplot pyplot.style.use(\"seaborn-v0_8-whitegrid\") COLORS = [\"xkcd:\"+color[:-1] for color in pd.read_csv(\"../../../xkcd_colors.txt\",comment=\"#\",header=None).iloc[:,0].tolist()][::-1] pyplot.rcParams['axes.prop_cycle'] = matplotlib.cycler(color=COLORS) LINESTYLES = ['solid','dotted','dashed','dashdot',(0, (1, 1))] DEFAULTFONTSIZE = 30 pyplot.rcParams['xtick.labelsize'] = DEFAULTFONTSIZE pyplot.rcParams['ytick.labelsize'] = DEFAULTFONTSIZE pyplot.rcParams['ytick.labelsize'] = DEFAULTFONTSIZE pyplot.rcParams['axes.titlesize'] = DEFAULTFONTSIZE pyplot.rcParams['figure.titlesize'] = DEFAULTFONTSIZE pyplot.rcParams[\"axes.labelsize\"] = DEFAULTFONTSIZE pyplot.rcParams['legend.fontsize'] = DEFAULTFONTSIZE pyplot.rcParams['font.size'] = DEFAULTFONTSIZE pyplot.rcParams['lines.linewidth'] = 5 pyplot.rcParams['lines.markersize'] = 15 PW = 30 # inches In\u00a0[\u00a0]: Copied! <pre>def diffs(l, x):\n    assert x.ndim==2 and x.size(1)==1\n    assert l in [0,1,2]\n    x = x[:,0]\n    if l==0:\n        return 500*torch.abs(x-1/2)*torch.sin(2*np.pi*x)+100\n    elif l==1:\n        return 10*(torch.abs(x-1/4)*torch.sin(8*np.pi*x)+torch.cos(16*np.pi*x)/50)+7\n    else:\n        return (torch.cos(16*np.pi*x)-5*torch.abs(x-3/8)+torch.cos(16*np.pi*x)/50)/2+3\nxticks = torch.linspace(0,1,1000)[1:-1]\nyticks0 = diffs(0,xticks[:,None])\nyticks1 = diffs(1,xticks[:,None])\nyticks2 = diffs(2,xticks[:,None])\n# FGP 0\nfgp0 = fastgps.StandardGP(\n    # kernel = qp.KernelDigShiftInvarCombined(d=1,torchify=True),\n    kernel = qp.KernelSquaredExponential(d=1,torchify=True),\n    # kernel = qp.KernelShiftInvar(d=1,torchify=True,alpha=1),\n    seqs = qp.DigitalNetB2(1,seed=7),\n    # seqs = qp.Lattice(1,seed=7),\n)\nxnext0 = fgp0.get_x_next(2**3)\nynext0 = diffs(0,xnext0)\nfgp0.add_y_next(ynext0)\nfgp0.fit(stop_crit_improvement_threshold=1e-2,verbose=False)\nyhat0,_,_,ci_low0,ci_high0 = fgp0.post_ci(xticks[:,None])\nmuhat0,varhat0,_,pci_low0,pci_high0 = fgp0.post_cubature_ci()\n# FGP 1\nfgp1 = fastgps.StandardGP(\n    # kernel = qp.KernelDigShiftInvarCombined(d=1,torchify=True),\n    kernel = qp.KernelSquaredExponential(d=1,torchify=True),\n    # kernel = qp.KernelShiftInvar(d=1,torchify=True,alpha=1),\n    seqs = qp.DigitalNetB2(1,seed=13),\n    # seqs = qp.Lattice(1,seed=13),\n)\nxnext1 = fgp1.get_x_next(2**3)\nynext1 = diffs(1,xnext1)\nfgp1.add_y_next(ynext1)\nfgp1.fit(stop_crit_improvement_threshold=1e-2,verbose=False)\nyhat1,_,_,ci_low1,ci_high1 = fgp1.post_ci(xticks[:,None])\nmuhat1,varhat1,_,pci_low1,pci_high1 = fgp1.post_cubature_ci()\n# FGP 2\nfgp2 = fastgps.StandardGP(\n    # kernel = qp.KernelDigShiftInvarCombined(d=1,torchify=True),\n    kernel = qp.KernelSquaredExponential(d=1,torchify=True),\n    # kernel = qp.KernelShiftInvar(d=1,torchify=True,alpha=1),\n    seqs = qp.DigitalNetB2(1,seed=10),\n    # seqs = qp.Lattice(1,seed=3),\n)\nxnext2 = fgp2.get_x_next(2**2)\nynext2 = diffs(2,xnext2)\nfgp2.add_y_next(ynext2)\nfgp2.fit(stop_crit_improvement_threshold=1e-2,verbose=False)\nyhat2,_,_,ci_low2,ci_high2 = fgp2.post_ci(xticks[:,None])\nmuhat2,varhat2,_,pci_low2,pci_high2 = fgp2.post_cubature_ci()\nfig,ax = pyplot.subplots(nrows=4,ncols=1,figsize=(PW/4,PW))\nax[0].plot(xticks,yticks0,color=\"k\")\nax[0].scatter(xnext0[:,0],ynext0,color=\"k\",s=500)\nax[0].plot(xticks,yhat0,color=COLORS[3])\nax[0].fill_between(xticks,ci_low0,ci_high0,alpha=.25,color=COLORS[3])\nax[1].plot(xticks,yticks1,color=\"k\")\nax[1].scatter(xnext1[:,0],ynext1,color=\"k\",s=500)\nax[1].plot(xticks,yhat1,color=COLORS[4])\nax[1].fill_between(xticks,ci_low1,ci_high1,alpha=.25,color=COLORS[4])\nax[2].plot(xticks,yticks2,color=\"k\")\nax[2].scatter(xnext2[:,0],ynext2,color=\"k\",s=500)\nax[2].plot(xticks,yhat2,color=COLORS[6])\nax[2].fill_between(xticks,ci_low2,ci_high2,alpha=.25,color=COLORS[6])\nfor i in range(3):\n    ax[i].set_xticks([0,1/4,1/2,3/4,1])\n    ax[i].set_xticklabels([0,\"\",\"\",\"\",1])\naxd0 = fig.add_axes([ax[0].get_position().x1,ax[0].get_position().y0,.3,ax[0].get_position().y1-ax[0].get_position().y0])\naxd0.axis(False)\ny0ticks = torch.linspace(*ax[0].get_ylim(),1000)\naxd0.fill_betweenx(y0ticks,0,1/torch.sqrt(2*np.pi*varhat0)*torch.exp(-(y0ticks-muhat0)**2/(2*varhat0)),color=COLORS[3])\naxd0.set_ylim(*ax[0].get_ylim())\naxd1 = fig.add_axes([ax[1].get_position().x1,ax[1].get_position().y0,.3,ax[1].get_position().y1-ax[1].get_position().y0])\naxd1.axis(False)\ny1ticks = torch.linspace(*ax[1].get_ylim(),1000)\naxd1.fill_betweenx(y1ticks,0,1/torch.sqrt(2*np.pi*varhat1)*torch.exp(-(y1ticks-muhat1)**2/(2*varhat1)),color=COLORS[4])\naxd1.set_ylim(*ax[1].get_ylim())\naxd2 = fig.add_axes([ax[2].get_position().x1,ax[2].get_position().y0,.3,ax[2].get_position().y1-ax[2].get_position().y0])\naxd2.axis(False)\ny2ticks = torch.linspace(*ax[2].get_ylim(),1000)\naxd2.fill_betweenx(y2ticks,0,1/torch.sqrt(2*np.pi*varhat2)*torch.exp(-(y2ticks-muhat2)**2/(2*varhat2)),color=COLORS[6])\naxd2.set_ylim(*ax[2].get_ylim())\nvarcomb = varhat0+varhat1+varhat2\nmucomb = muhat0+muhat1+muhat2\nxnewticks = torch.linspace(mucomb-4*torch.sqrt(varcomb),mucomb+4*torch.sqrt(varcomb),1000)\nax[3].fill_between(xnewticks,0,1/torch.sqrt(2*np.pi*varcomb)*torch.exp(-(xnewticks-mucomb)**2/(2*varcomb)),color=COLORS[7])\nax[3].set_yticklabels([])\nprint(muhat0,muhat1,muhat2,mucomb)\nprint(varhat0,varhat1,varhat2,varcomb)\nfig.savefig(\"visual.pdf\",bbox_inches=\"tight\",transparent=True)\n</pre> def diffs(l, x):     assert x.ndim==2 and x.size(1)==1     assert l in [0,1,2]     x = x[:,0]     if l==0:         return 500*torch.abs(x-1/2)*torch.sin(2*np.pi*x)+100     elif l==1:         return 10*(torch.abs(x-1/4)*torch.sin(8*np.pi*x)+torch.cos(16*np.pi*x)/50)+7     else:         return (torch.cos(16*np.pi*x)-5*torch.abs(x-3/8)+torch.cos(16*np.pi*x)/50)/2+3 xticks = torch.linspace(0,1,1000)[1:-1] yticks0 = diffs(0,xticks[:,None]) yticks1 = diffs(1,xticks[:,None]) yticks2 = diffs(2,xticks[:,None]) # FGP 0 fgp0 = fastgps.StandardGP(     # kernel = qp.KernelDigShiftInvarCombined(d=1,torchify=True),     kernel = qp.KernelSquaredExponential(d=1,torchify=True),     # kernel = qp.KernelShiftInvar(d=1,torchify=True,alpha=1),     seqs = qp.DigitalNetB2(1,seed=7),     # seqs = qp.Lattice(1,seed=7), ) xnext0 = fgp0.get_x_next(2**3) ynext0 = diffs(0,xnext0) fgp0.add_y_next(ynext0) fgp0.fit(stop_crit_improvement_threshold=1e-2,verbose=False) yhat0,_,_,ci_low0,ci_high0 = fgp0.post_ci(xticks[:,None]) muhat0,varhat0,_,pci_low0,pci_high0 = fgp0.post_cubature_ci() # FGP 1 fgp1 = fastgps.StandardGP(     # kernel = qp.KernelDigShiftInvarCombined(d=1,torchify=True),     kernel = qp.KernelSquaredExponential(d=1,torchify=True),     # kernel = qp.KernelShiftInvar(d=1,torchify=True,alpha=1),     seqs = qp.DigitalNetB2(1,seed=13),     # seqs = qp.Lattice(1,seed=13), ) xnext1 = fgp1.get_x_next(2**3) ynext1 = diffs(1,xnext1) fgp1.add_y_next(ynext1) fgp1.fit(stop_crit_improvement_threshold=1e-2,verbose=False) yhat1,_,_,ci_low1,ci_high1 = fgp1.post_ci(xticks[:,None]) muhat1,varhat1,_,pci_low1,pci_high1 = fgp1.post_cubature_ci() # FGP 2 fgp2 = fastgps.StandardGP(     # kernel = qp.KernelDigShiftInvarCombined(d=1,torchify=True),     kernel = qp.KernelSquaredExponential(d=1,torchify=True),     # kernel = qp.KernelShiftInvar(d=1,torchify=True,alpha=1),     seqs = qp.DigitalNetB2(1,seed=10),     # seqs = qp.Lattice(1,seed=3), ) xnext2 = fgp2.get_x_next(2**2) ynext2 = diffs(2,xnext2) fgp2.add_y_next(ynext2) fgp2.fit(stop_crit_improvement_threshold=1e-2,verbose=False) yhat2,_,_,ci_low2,ci_high2 = fgp2.post_ci(xticks[:,None]) muhat2,varhat2,_,pci_low2,pci_high2 = fgp2.post_cubature_ci() fig,ax = pyplot.subplots(nrows=4,ncols=1,figsize=(PW/4,PW)) ax[0].plot(xticks,yticks0,color=\"k\") ax[0].scatter(xnext0[:,0],ynext0,color=\"k\",s=500) ax[0].plot(xticks,yhat0,color=COLORS[3]) ax[0].fill_between(xticks,ci_low0,ci_high0,alpha=.25,color=COLORS[3]) ax[1].plot(xticks,yticks1,color=\"k\") ax[1].scatter(xnext1[:,0],ynext1,color=\"k\",s=500) ax[1].plot(xticks,yhat1,color=COLORS[4]) ax[1].fill_between(xticks,ci_low1,ci_high1,alpha=.25,color=COLORS[4]) ax[2].plot(xticks,yticks2,color=\"k\") ax[2].scatter(xnext2[:,0],ynext2,color=\"k\",s=500) ax[2].plot(xticks,yhat2,color=COLORS[6]) ax[2].fill_between(xticks,ci_low2,ci_high2,alpha=.25,color=COLORS[6]) for i in range(3):     ax[i].set_xticks([0,1/4,1/2,3/4,1])     ax[i].set_xticklabels([0,\"\",\"\",\"\",1]) axd0 = fig.add_axes([ax[0].get_position().x1,ax[0].get_position().y0,.3,ax[0].get_position().y1-ax[0].get_position().y0]) axd0.axis(False) y0ticks = torch.linspace(*ax[0].get_ylim(),1000) axd0.fill_betweenx(y0ticks,0,1/torch.sqrt(2*np.pi*varhat0)*torch.exp(-(y0ticks-muhat0)**2/(2*varhat0)),color=COLORS[3]) axd0.set_ylim(*ax[0].get_ylim()) axd1 = fig.add_axes([ax[1].get_position().x1,ax[1].get_position().y0,.3,ax[1].get_position().y1-ax[1].get_position().y0]) axd1.axis(False) y1ticks = torch.linspace(*ax[1].get_ylim(),1000) axd1.fill_betweenx(y1ticks,0,1/torch.sqrt(2*np.pi*varhat1)*torch.exp(-(y1ticks-muhat1)**2/(2*varhat1)),color=COLORS[4]) axd1.set_ylim(*ax[1].get_ylim()) axd2 = fig.add_axes([ax[2].get_position().x1,ax[2].get_position().y0,.3,ax[2].get_position().y1-ax[2].get_position().y0]) axd2.axis(False) y2ticks = torch.linspace(*ax[2].get_ylim(),1000) axd2.fill_betweenx(y2ticks,0,1/torch.sqrt(2*np.pi*varhat2)*torch.exp(-(y2ticks-muhat2)**2/(2*varhat2)),color=COLORS[6]) axd2.set_ylim(*ax[2].get_ylim()) varcomb = varhat0+varhat1+varhat2 mucomb = muhat0+muhat1+muhat2 xnewticks = torch.linspace(mucomb-4*torch.sqrt(varcomb),mucomb+4*torch.sqrt(varcomb),1000) ax[3].fill_between(xnewticks,0,1/torch.sqrt(2*np.pi*varcomb)*torch.exp(-(xnewticks-mucomb)**2/(2*varcomb)),color=COLORS[7]) ax[3].set_yticklabels([]) print(muhat0,muhat1,muhat2,mucomb) print(varhat0,varhat1,varhat2,varcomb) fig.savefig(\"visual.pdf\",bbox_inches=\"tight\",transparent=True) <pre>tensor(99.9225) tensor(8.0512) tensor(2.2201) tensor(110.1937)\ntensor(4.0468) tensor(0.0841) tensor(0.0213) tensor(4.1522)\n</pre>"},{"location":"examples/multitask/compare_1d_gps_plot/","title":"Compare GPs 1d","text":"In\u00a0[31]: Copied! <pre>import fastgps\nimport qmcpy as qp\nimport numpy as np\nimport torch\nimport pandas as pd\nfrom matplotlib import pyplot\nimport tueplots.figsizes\n</pre> import fastgps import qmcpy as qp import numpy as np import torch import pandas as pd from matplotlib import pyplot import tueplots.figsizes In\u00a0[32]: Copied! <pre>device = \"cpu\"\nif device!=\"mps\":\n    torch.set_default_dtype(torch.float64)\n</pre> device = \"cpu\" if device!=\"mps\":     torch.set_default_dtype(torch.float64) In\u00a0[33]: Copied! <pre>colors = [\"xkcd:\"+color[:-1] for color in pd.read_csv(\"../../../xkcd_colors.txt\",comment=\"#\").iloc[:,0].tolist()][::-1]\n_alpha = 0.25\nWIDTH = 2*(500/72)\nLINEWIDTH = 3\nMARKERSIZE = 100\n</pre> colors = [\"xkcd:\"+color[:-1] for color in pd.read_csv(\"../../../xkcd_colors.txt\",comment=\"#\").iloc[:,0].tolist()][::-1] _alpha = 0.25 WIDTH = 2*(500/72) LINEWIDTH = 3 MARKERSIZE = 100 In\u00a0[34]: Copied! <pre>d = 1\ndef f_ackley(x, a=20, b=0.2, c=2*np.pi, scaling=32.768):\n    # https://www.sfu.ca/~ssurjano/ackley.html\n    assert x.ndim==2\n    x = 2*scaling*x-scaling\n    t1 = a*torch.exp(-b*torch.sqrt(torch.mean(x**2,1)))\n    t2 = torch.exp(torch.mean(torch.cos(c*x),1))\n    t3 = a+np.exp(1)\n    y = -t1-t2+t3\n    return y\nfs = [lambda x: f_ackley(x,c=0), lambda x: f_ackley(x)]\nnum_tasks = len(fs)\nns = [\n    torch.tensor([0,2**5],device=device),\n    torch.tensor([2**5,2**4],device=device),\n]\n</pre> d = 1 def f_ackley(x, a=20, b=0.2, c=2*np.pi, scaling=32.768):     # https://www.sfu.ca/~ssurjano/ackley.html     assert x.ndim==2     x = 2*scaling*x-scaling     t1 = a*torch.exp(-b*torch.sqrt(torch.mean(x**2,1)))     t2 = torch.exp(torch.mean(torch.cos(c*x),1))     t3 = a+np.exp(1)     y = -t1-t2+t3     return y fs = [lambda x: f_ackley(x,c=0), lambda x: f_ackley(x)] num_tasks = len(fs) ns = [     torch.tensor([0,2**5],device=device),     torch.tensor([2**5,2**4],device=device), ] In\u00a0[35]: Copied! <pre>seqs_std = [\n    qp.DigitalNetB2(d,seed=11,randomize=\"DS\"),\n    qp.DigitalNetB2(d,seed=13,randomize=\"DS\"),\n]\nseqs_lattice = [\n    qp.Lattice(d,seed=7),\n    qp.Lattice(d,seed=2),\n ]\nseqs_dnb2s = [\n    qp.DigitalNetB2(d,seed=7,randomize=\"DS\"),\n    qp.DigitalNetB2(d,seed=2,randomize=\"DS\"),\n]\nngptypes = 3\n</pre> seqs_std = [     qp.DigitalNetB2(d,seed=11,randomize=\"DS\"),     qp.DigitalNetB2(d,seed=13,randomize=\"DS\"), ] seqs_lattice = [     qp.Lattice(d,seed=7),     qp.Lattice(d,seed=2),  ] seqs_dnb2s = [     qp.DigitalNetB2(d,seed=7,randomize=\"DS\"),     qp.DigitalNetB2(d,seed=2,randomize=\"DS\"), ] ngptypes = 3 In\u00a0[36]: Copied! <pre>xticks = torch.linspace(0,1,1001,device=device)[1:-1,None]\nyticks = torch.vstack([fs[i](xticks) for i in range(num_tasks)])\n</pre> xticks = torch.linspace(0,1,1001,device=device)[1:-1,None] yticks = torch.vstack([fs[i](xticks) for i in range(num_tasks)]) In\u00a0[37]: Copied! <pre>pmeans = torch.nan*torch.ones((ngptypes,num_tasks,len(xticks)))\nci_lows = torch.nan*torch.ones((ngptypes,num_tasks,len(xticks)))\nci_highs = torch.nan*torch.ones((ngptypes,num_tasks,len(xticks)))\nl2rerrors = torch.ones((ngptypes,num_tasks))\nfgp_indep = [ \n    [\n        fastgps.StandardGP(qp.KernelSquaredExponential(d=d,torchify=True,device=device),seqs=seqs_std[0]),\n        fastgps.StandardGP(qp.KernelSquaredExponential(d=d,torchify=True,device=device),seqs=seqs_std[1]),\n    ],\n    [\n        fastgps.FastGPLattice(qp.KernelShiftInvar(d=d,torchify=True,device=device),seqs=seqs_lattice[0]),\n        fastgps.FastGPLattice(qp.KernelShiftInvar(d=d,torchify=True,device=device),seqs=seqs_lattice[1]),\n    ],\n    [\n        fastgps.FastGPDigitalNetB2(qp.KernelDigShiftInvarCombined(d=d,torchify=True,device=device),seqs=seqs_dnb2s[0]),\n        fastgps.FastGPDigitalNetB2(qp.KernelDigShiftInvarCombined(d=d,torchify=True,device=device),seqs=seqs_dnb2s[1]),\n    ],\n]\ngpnames = [type(fgp_indep[i][0]).__name__ for i in range(ngptypes)]\nfor i in range(ngptypes):\n    print(gpnames[i])\n    for l in range(num_tasks):\n        if ns[0][l]&gt;0:\n            x_next = fgp_indep[i][l].get_x_next(n=ns[0][l].item())\n            y_next = torch.vstack([fs[i](x_next) for i in range(num_tasks)])\n            fgp_indep[i][l].add_y_next(y_next[l])\n            fgp_indep[i][l].fit()\n            pmeans[i][l],_,_,ci_lows[i][l],ci_highs[i][l] = fgp_indep[i][l].post_ci(xticks)\n            l2rerrors[i][l] = torch.linalg.norm(pmeans[i][l]-yticks[l])/torch.linalg.norm(yticks[l])\n</pre> pmeans = torch.nan*torch.ones((ngptypes,num_tasks,len(xticks))) ci_lows = torch.nan*torch.ones((ngptypes,num_tasks,len(xticks))) ci_highs = torch.nan*torch.ones((ngptypes,num_tasks,len(xticks))) l2rerrors = torch.ones((ngptypes,num_tasks)) fgp_indep = [      [         fastgps.StandardGP(qp.KernelSquaredExponential(d=d,torchify=True,device=device),seqs=seqs_std[0]),         fastgps.StandardGP(qp.KernelSquaredExponential(d=d,torchify=True,device=device),seqs=seqs_std[1]),     ],     [         fastgps.FastGPLattice(qp.KernelShiftInvar(d=d,torchify=True,device=device),seqs=seqs_lattice[0]),         fastgps.FastGPLattice(qp.KernelShiftInvar(d=d,torchify=True,device=device),seqs=seqs_lattice[1]),     ],     [         fastgps.FastGPDigitalNetB2(qp.KernelDigShiftInvarCombined(d=d,torchify=True,device=device),seqs=seqs_dnb2s[0]),         fastgps.FastGPDigitalNetB2(qp.KernelDigShiftInvarCombined(d=d,torchify=True,device=device),seqs=seqs_dnb2s[1]),     ], ] gpnames = [type(fgp_indep[i][0]).__name__ for i in range(ngptypes)] for i in range(ngptypes):     print(gpnames[i])     for l in range(num_tasks):         if ns[0][l]&gt;0:             x_next = fgp_indep[i][l].get_x_next(n=ns[0][l].item())             y_next = torch.vstack([fs[i](x_next) for i in range(num_tasks)])             fgp_indep[i][l].add_y_next(y_next[l])             fgp_indep[i][l].fit()             pmeans[i][l],_,_,ci_lows[i][l],ci_highs[i][l] = fgp_indep[i][l].post_ci(xticks)             l2rerrors[i][l] = torch.linalg.norm(pmeans[i][l]-yticks[l])/torch.linalg.norm(yticks[l]) <pre>StandardGP\n     iter of 5.0e+03 | best loss  | loss      \n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n            0.00e+00 | 1.92e+06   | 1.92e+06  \n            5.00e+00 | 8.84e+05   | 8.84e+05  \n            1.00e+01 | 1.99e+04   | 1.99e+04  \n            1.50e+01 | 7.67e+01   | 9.68e+01  \n            2.00e+01 | 7.43e+01   | 7.43e+01  \n            2.50e+01 | 7.29e+01   | 7.29e+01  \n            3.00e+01 | 7.28e+01   | 7.29e+01  \n            3.50e+01 | 7.28e+01   | 7.28e+01  \n            4.00e+01 | 7.28e+01   | 7.28e+01  \n            4.40e+01 | 7.28e+01   | 7.28e+01  \nFastGPLattice\n     iter of 5.0e+03 | best loss  | loss      \n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n            0.00e+00 | 1.50e+03   | 1.50e+03  \n            5.00e+00 | 3.38e+02   | 3.38e+02  \n            1.00e+01 | 5.88e+01   | 5.97e+01  \n            1.50e+01 | 5.73e+01   | 5.76e+01  \n            2.00e+01 | 5.72e+01   | 5.73e+01  \n            2.50e+01 | 5.72e+01   | 5.72e+01  \n            2.80e+01 | 5.72e+01   | 5.72e+01  \nFastGPDigitalNetB2\n     iter of 5.0e+03 | best loss  | loss      \n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n            0.00e+00 | 1.29e+02   | 1.29e+02  \n            5.00e+00 | 7.87e+01   | 7.98e+01  \n            1.00e+01 | 7.85e+01   | 7.85e+01  \n            1.50e+01 | 7.84e+01   | 7.84e+01  \n            2.00e+01 | 7.83e+01   | 7.83e+01  \n            2.50e+01 | 7.80e+01   | 7.80e+01  \n            3.00e+01 | 7.78e+01   | 7.78e+01  \n            3.50e+01 | 7.76e+01   | 7.76e+01  \n            4.00e+01 | 7.75e+01   | 7.75e+01  \n            4.50e+01 | 7.74e+01   | 7.74e+01  \n            5.00e+01 | 7.74e+01   | 7.74e+01  \n            5.50e+01 | 7.73e+01   | 7.73e+01  \n            6.00e+01 | 7.73e+01   | 7.73e+01  \n            6.50e+01 | 7.73e+01   | 7.73e+01  \n            7.00e+01 | 7.72e+01   | 7.72e+01  \n            7.50e+01 | 7.72e+01   | 7.72e+01  \n            8.00e+01 | 7.71e+01   | 7.71e+01  \n            8.50e+01 | 7.70e+01   | 7.71e+01  \n            9.00e+01 | 7.70e+01   | 7.72e+01  \n            9.10e+01 | 7.70e+01   | 7.71e+01  \n</pre> In\u00a0[38]: Copied! <pre>fgp_multitask = [\n    fastgps.StandardGP(qp.KernelMultiTask(qp.KernelSquaredExponential(d,torchify=True,device=device),num_tasks=num_tasks),seqs=seqs_std),\n    fastgps.FastGPLattice(qp.KernelMultiTask(qp.KernelShiftInvar(d,torchify=True,device=device),num_tasks=num_tasks),seqs=seqs_lattice),\n    fastgps.FastGPDigitalNetB2(qp.KernelMultiTask(qp.KernelDigShiftInvarCombined(d,torchify=True,device=device),num_tasks=num_tasks),seqs=seqs_dnb2s),\n]\npmeans_mt = torch.nan*torch.ones((ngptypes,num_tasks,len(xticks)))\nci_lows_mt = torch.nan*torch.ones((ngptypes,num_tasks,len(xticks)))\nci_highs_mt = torch.nan*torch.ones((ngptypes,num_tasks,len(xticks)))\nl2rerrors_mt = torch.ones((ngptypes,num_tasks))\nfor i in range(ngptypes):\n    print(gpnames[i])\n    x_next = fgp_multitask[i].get_x_next(n=ns[1])\n    y_next = [fs[i](x_next[i]) for i in range(num_tasks)]\n    fgp_multitask[i].add_y_next(y_next)\n    fgp_multitask[i].fit()\n    pmeans_mt[i],_,_,ci_lows_mt[i],ci_highs_mt[i] = fgp_multitask[i].post_ci(xticks)\n    l2rerrors_mt[i] = torch.linalg.norm(pmeans_mt[i]-yticks,dim=-1)/torch.linalg.norm(yticks,dim=-1)\n</pre> fgp_multitask = [     fastgps.StandardGP(qp.KernelMultiTask(qp.KernelSquaredExponential(d,torchify=True,device=device),num_tasks=num_tasks),seqs=seqs_std),     fastgps.FastGPLattice(qp.KernelMultiTask(qp.KernelShiftInvar(d,torchify=True,device=device),num_tasks=num_tasks),seqs=seqs_lattice),     fastgps.FastGPDigitalNetB2(qp.KernelMultiTask(qp.KernelDigShiftInvarCombined(d,torchify=True,device=device),num_tasks=num_tasks),seqs=seqs_dnb2s), ] pmeans_mt = torch.nan*torch.ones((ngptypes,num_tasks,len(xticks))) ci_lows_mt = torch.nan*torch.ones((ngptypes,num_tasks,len(xticks))) ci_highs_mt = torch.nan*torch.ones((ngptypes,num_tasks,len(xticks))) l2rerrors_mt = torch.ones((ngptypes,num_tasks)) for i in range(ngptypes):     print(gpnames[i])     x_next = fgp_multitask[i].get_x_next(n=ns[1])     y_next = [fs[i](x_next[i]) for i in range(num_tasks)]     fgp_multitask[i].add_y_next(y_next)     fgp_multitask[i].fit()     pmeans_mt[i],_,_,ci_lows_mt[i],ci_highs_mt[i] = fgp_multitask[i].post_ci(xticks)     l2rerrors_mt[i] = torch.linalg.norm(pmeans_mt[i]-yticks,dim=-1)/torch.linalg.norm(yticks,dim=-1) <pre>StandardGP\n     iter of 5.0e+03 | best loss  | loss      \n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n            0.00e+00 | 3.66e+06   | 3.66e+06  \n            5.00e+00 | 1.15e+06   | 1.15e+06  \n            1.00e+01 | 2.52e+04   | 2.52e+04  \n            1.50e+01 | 1.33e+02   | 2.21e+02  \n            2.00e+01 | 1.09e+02   | 1.09e+02  \n            2.50e+01 | 1.02e+02   | 1.02e+02  \n            3.00e+01 | 9.42e+01   | 9.42e+01  \n            3.50e+01 | 9.18e+01   | 9.20e+01  \n            4.00e+01 | 9.17e+01   | 9.18e+01  \n            4.50e+01 | 9.17e+01   | 9.17e+01  \n            4.80e+01 | 9.17e+01   | 9.17e+01  \nFastGPLattice\n     iter of 5.0e+03 | best loss  | loss      \n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n            0.00e+00 | 6.63e+02   | 6.63e+02  \n            5.00e+00 | 9.82e+01   | 9.82e+01  \n            1.00e+01 | 8.17e+01   | 8.17e+01  \n            1.50e+01 | 7.36e+01   | 7.36e+01  \n            2.00e+01 | 7.31e+01   | 7.31e+01  \n            2.50e+01 | 7.31e+01   | 7.31e+01  \n            3.00e+01 | 7.30e+01   | 7.30e+01  \n            3.50e+01 | 7.29e+01   | 7.29e+01  \n            4.00e+01 | 7.26e+01   | 7.26e+01  \n            4.50e+01 | 7.20e+01   | 7.20e+01  \n            5.00e+01 | 7.03e+01   | 7.03e+01  \n            5.50e+01 | 6.66e+01   | 6.66e+01  \n            6.00e+01 | 6.37e+01   | 3.85e+02  \n            6.50e+01 | 6.37e+01   | 6.43e+01  \n            6.90e+01 | 6.37e+01   | 6.48e+01  \nFastGPDigitalNetB2\n     iter of 5.0e+03 | best loss  | loss      \n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n            0.00e+00 | 1.33e+02   | 1.33e+02  \n            5.00e+00 | 1.14e+02   | 1.14e+02  \n            1.00e+01 | 1.10e+02   | 1.10e+02  \n            1.50e+01 | 1.04e+02   | 1.04e+02  \n            2.00e+01 | 1.02e+02   | 1.02e+02  \n            2.50e+01 | 1.02e+02   | 1.02e+02  \n            3.00e+01 | 1.02e+02   | 1.02e+02  \n            3.50e+01 | 1.02e+02   | 1.02e+02  \n            4.00e+01 | 1.01e+02   | 1.01e+02  \n            4.50e+01 | 1.01e+02   | 1.01e+02  \n            5.00e+01 | 1.01e+02   | 1.01e+02  \n            5.50e+01 | 1.01e+02   | 1.01e+02  \n            6.00e+01 | 1.01e+02   | 1.01e+02  \n            6.50e+01 | 1.01e+02   | 1.01e+02  \n            7.00e+01 | 1.01e+02   | 1.01e+02  \n            7.50e+01 | 1.01e+02   | 1.01e+02  \n</pre> In\u00a0[39]: Copied! <pre>pd.DataFrame({gpnames[i]+\" task %d\"%j: [l2rerrors[i,j].item(),l2rerrors_mt[i,j].item()] for j in range(num_tasks) for i in range(ngptypes)},[\"single task GP\",\"MTGP\"])\n</pre> pd.DataFrame({gpnames[i]+\" task %d\"%j: [l2rerrors[i,j].item(),l2rerrors_mt[i,j].item()] for j in range(num_tasks) for i in range(ngptypes)},[\"single task GP\",\"MTGP\"]) Out[39]: StandardGP task 0 FastGPLattice task 0 FastGPDigitalNetB2 task 0 StandardGP task 1 FastGPLattice task 1 FastGPDigitalNetB2 task 1 single task GP 1.00000 1.000000 1.000000 0.062289 0.064188 0.075656 MTGP 0.01473 0.014621 0.043651 0.061912 0.065321 0.054531 In\u00a0[40]: Copied! <pre>fig,ax = pyplot.subplots(nrows=ngptypes,ncols=num_tasks,figsize=(WIDTH,WIDTH/num_tasks*ngptypes/1.5),sharex=True,sharey=\"row\")\nax = ax.reshape((ngptypes,num_tasks))\nfor i in range(ngptypes):\n    for l in range(num_tasks):\n        ax[i,l].plot(xticks[:,0].cpu(),yticks[l].cpu(),color=\"k\",linewidth=LINEWIDTH/2)\n    pltmin = np.nanmin([ci_lows[i][l].min() for l in range(num_tasks)]+[ci_lows_mt[i].min()])\n    pltmax = np.nanmax([ci_highs[i][l].max() for l in range(num_tasks)]+[ci_highs_mt[i].max()])\n    ax[i,0].set_ylim([pltmin-.05*(pltmax-pltmin),pltmax+.05*(pltmax-pltmin)])\n    ax[i,0].set_ylabel(gpnames[i],fontsize=\"xx-large\")\nfig.savefig(\"./mtgps0.1d.pdf\",bbox_inches=\"tight\")\nfor i in range(ngptypes):\n    for l in range(num_tasks):\n        ax[i,l].plot(xticks[:,0].cpu(),pmeans[i][l].cpu(),color=colors[0],linewidth=LINEWIDTH)\n        ax[i,l].fill_between(xticks[:,0].cpu(),ci_lows[i][l].cpu(),ci_highs[i][l].cpu(),color=colors[0],alpha=_alpha,label=\"independent GPs\")\n    ax[i,0].legend(frameon=False,loc=\"upper left\",ncols=2,fontsize=\"xx-large\") \nfig.savefig(\"./mtgps1.1d.pdf\",bbox_inches=\"tight\")\nfor i in range(ngptypes):\n    for l in range(num_tasks):\n        ax[i,l].plot(xticks[:,0].cpu(),pmeans_mt[i][l].cpu(),color=colors[1],linewidth=LINEWIDTH)\n        ax[i,l].fill_between(xticks[:,0].cpu(),ci_lows_mt[i][l].cpu(),ci_highs_mt[i][l].cpu(),color=colors[1],alpha=_alpha,label=\"MTGP\")\n    ax[i,0].legend(frameon=False,loc=\"upper left\",ncols=2,fontsize=\"xx-large\") \nfig.savefig(\"./mtgps2.1d.pdf\",bbox_inches=\"tight\")\n</pre> fig,ax = pyplot.subplots(nrows=ngptypes,ncols=num_tasks,figsize=(WIDTH,WIDTH/num_tasks*ngptypes/1.5),sharex=True,sharey=\"row\") ax = ax.reshape((ngptypes,num_tasks)) for i in range(ngptypes):     for l in range(num_tasks):         ax[i,l].plot(xticks[:,0].cpu(),yticks[l].cpu(),color=\"k\",linewidth=LINEWIDTH/2)     pltmin = np.nanmin([ci_lows[i][l].min() for l in range(num_tasks)]+[ci_lows_mt[i].min()])     pltmax = np.nanmax([ci_highs[i][l].max() for l in range(num_tasks)]+[ci_highs_mt[i].max()])     ax[i,0].set_ylim([pltmin-.05*(pltmax-pltmin),pltmax+.05*(pltmax-pltmin)])     ax[i,0].set_ylabel(gpnames[i],fontsize=\"xx-large\") fig.savefig(\"./mtgps0.1d.pdf\",bbox_inches=\"tight\") for i in range(ngptypes):     for l in range(num_tasks):         ax[i,l].plot(xticks[:,0].cpu(),pmeans[i][l].cpu(),color=colors[0],linewidth=LINEWIDTH)         ax[i,l].fill_between(xticks[:,0].cpu(),ci_lows[i][l].cpu(),ci_highs[i][l].cpu(),color=colors[0],alpha=_alpha,label=\"independent GPs\")     ax[i,0].legend(frameon=False,loc=\"upper left\",ncols=2,fontsize=\"xx-large\")  fig.savefig(\"./mtgps1.1d.pdf\",bbox_inches=\"tight\") for i in range(ngptypes):     for l in range(num_tasks):         ax[i,l].plot(xticks[:,0].cpu(),pmeans_mt[i][l].cpu(),color=colors[1],linewidth=LINEWIDTH)         ax[i,l].fill_between(xticks[:,0].cpu(),ci_lows_mt[i][l].cpu(),ci_highs_mt[i][l].cpu(),color=colors[1],alpha=_alpha,label=\"MTGP\")     ax[i,0].legend(frameon=False,loc=\"upper left\",ncols=2,fontsize=\"xx-large\")  fig.savefig(\"./mtgps2.1d.pdf\",bbox_inches=\"tight\") In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/multitask/compare_1d_gps_plot/#compare-gps-plot","title":"Compare GPs + Plot\u00b6","text":""},{"location":"examples/multitask/compare_1d_gps_plot/#true-function","title":"True Function\u00b6","text":""},{"location":"examples/multitask/compare_1d_gps_plot/#parameters","title":"Parameters\u00b6","text":""},{"location":"examples/multitask/compare_1d_gps_plot/#independent-single-task-gps","title":"Independent Single Task GPs\u00b6","text":""},{"location":"examples/multitask/compare_1d_gps_plot/#multi-task-fast-gps","title":"Multi-Task Fast GPs\u00b6","text":""},{"location":"examples/multitask/compare_1d_gps_plot/#compare-accuracy","title":"Compare Accuracy\u00b6","text":""},{"location":"examples/multitask/compare_1d_gps_plot/#plot","title":"Plot\u00b6","text":""},{"location":"examples/multitask/compare_2d_gps_plot/","title":"Compare GPs 2d","text":"In\u00a0[52]: Copied! <pre>import fastgps\nimport qmcpy as qp\nimport numpy as np\nimport torch\nimport pandas as pd\nfrom matplotlib import pyplot\nimport seaborn as sns\nimport tueplots.figsizes\n</pre> import fastgps import qmcpy as qp import numpy as np import torch import pandas as pd from matplotlib import pyplot import seaborn as sns import tueplots.figsizes In\u00a0[53]: Copied! <pre>device = \"cpu\"\nif device!=\"mps\":\n    torch.set_default_dtype(torch.float64)\n</pre> device = \"cpu\" if device!=\"mps\":     torch.set_default_dtype(torch.float64) In\u00a0[54]: Copied! <pre>colors = [\"xkcd:\"+color[:-1] for color in pd.read_csv(\"../../../xkcd_colors.txt\",comment=\"#\").iloc[:,0].tolist()][::-1]\n_alpha = 0.25\nWIDTH = 2*(500/72)\nLINEWIDTH = 3\nMARKERSIZE = 100\n</pre> colors = [\"xkcd:\"+color[:-1] for color in pd.read_csv(\"../../../xkcd_colors.txt\",comment=\"#\").iloc[:,0].tolist()][::-1] _alpha = 0.25 WIDTH = 2*(500/72) LINEWIDTH = 3 MARKERSIZE = 100 In\u00a0[55]: Copied! <pre>d = 2\ndef f_currin_high(x):\n    # https://www.sfu.ca/~ssurjano/curretal88exp.html\n    assert x.ndim==2 and x.size(-1)==d\n    x1,x2 = x[:,0],x[:,1]\n    y = (1-torch.exp(-1/(2*x2)))*(2300*x1**3+1900*x1**2+2092*x1+60)/(100*x1**3+500*x1**2+4*x1+20)\n    return y\ndef f_currin_low(x):\n    assert x.ndim==2 and x.size(-1)==d\n    x1,x2 = x[:,0],x[:,1]\n    y = 1/4*(\n        f_currin_high(torch.hstack([x1[:,None]+0.05,x2[:,None]+0.05])) + \n        f_currin_high(torch.hstack([x1[:,None]+0.05,torch.maximum(x2[:,None]-0.05,torch.zeros(1))])) + \n        f_currin_high(torch.hstack([x1[:,None]-0.05,x2[:,None]+0.05])) + \n        f_currin_high(torch.hstack([x1[:,None]-0.05,torch.maximum(x2[:,None]-0.05,torch.zeros(1))])))\n    return y\nfs = [f_currin_low, f_currin_high]\nnum_tasks = len(fs)\nns = [\n    torch.tensor([0,2**6],device=device),\n    torch.tensor([2**7,2**5],device=device)]\n</pre> d = 2 def f_currin_high(x):     # https://www.sfu.ca/~ssurjano/curretal88exp.html     assert x.ndim==2 and x.size(-1)==d     x1,x2 = x[:,0],x[:,1]     y = (1-torch.exp(-1/(2*x2)))*(2300*x1**3+1900*x1**2+2092*x1+60)/(100*x1**3+500*x1**2+4*x1+20)     return y def f_currin_low(x):     assert x.ndim==2 and x.size(-1)==d     x1,x2 = x[:,0],x[:,1]     y = 1/4*(         f_currin_high(torch.hstack([x1[:,None]+0.05,x2[:,None]+0.05])) +          f_currin_high(torch.hstack([x1[:,None]+0.05,torch.maximum(x2[:,None]-0.05,torch.zeros(1))])) +          f_currin_high(torch.hstack([x1[:,None]-0.05,x2[:,None]+0.05])) +          f_currin_high(torch.hstack([x1[:,None]-0.05,torch.maximum(x2[:,None]-0.05,torch.zeros(1))])))     return y fs = [f_currin_low, f_currin_high] num_tasks = len(fs) ns = [     torch.tensor([0,2**6],device=device),     torch.tensor([2**7,2**5],device=device)] In\u00a0[56]: Copied! <pre>seqs_std = [\n    qp.DigitalNetB2(d,seed=11,randomize=\"DS\"),\n    qp.DigitalNetB2(d,seed=13,randomize=\"DS\"),\n]\nseqs_lattice = [\n    qp.Lattice(d,seed=7),\n    qp.Lattice(d,seed=2),\n ]\nseqs_dnb2s = [\n    qp.DigitalNetB2(d,seed=7,randomize=\"DS\"),\n    qp.DigitalNetB2(d,seed=2,randomize=\"DS\"),\n]\nngptypes = 3\n</pre> seqs_std = [     qp.DigitalNetB2(d,seed=11,randomize=\"DS\"),     qp.DigitalNetB2(d,seed=13,randomize=\"DS\"), ] seqs_lattice = [     qp.Lattice(d,seed=7),     qp.Lattice(d,seed=2),  ] seqs_dnb2s = [     qp.DigitalNetB2(d,seed=7,randomize=\"DS\"),     qp.DigitalNetB2(d,seed=2,randomize=\"DS\"), ] ngptypes = 3 In\u00a0[57]: Copied! <pre>_xticks = torch.linspace(0,1,100,device=device)[1:-1]\nx0mesh,x1mesh = torch.meshgrid(_xticks,_xticks,indexing=\"ij\")\nxticks = torch.hstack([x0mesh.flatten()[:,None],x1mesh.flatten()[:,None]])\nyticks = torch.vstack([fs[i](xticks) for i in range(num_tasks)])\n</pre> _xticks = torch.linspace(0,1,100,device=device)[1:-1] x0mesh,x1mesh = torch.meshgrid(_xticks,_xticks,indexing=\"ij\") xticks = torch.hstack([x0mesh.flatten()[:,None],x1mesh.flatten()[:,None]]) yticks = torch.vstack([fs[i](xticks) for i in range(num_tasks)]) In\u00a0[58]: Copied! <pre>pmeans = torch.nan*torch.ones((ngptypes,num_tasks,len(xticks)))\nci_lows = torch.nan*torch.ones((ngptypes,num_tasks,len(xticks)))\nci_highs = torch.nan*torch.ones((ngptypes,num_tasks,len(xticks)))\nl2rerrors = torch.ones((ngptypes,num_tasks))\nfgp_indep = [ \n    [\n        fastgps.StandardGP(qp.KernelSquaredExponential(d=d,torchify=True,device=device),seqs=seqs_std[0]),\n        fastgps.StandardGP(qp.KernelSquaredExponential(d=d,torchify=True,device=device),seqs=seqs_std[1]),\n    ],\n    [\n        fastgps.FastGPLattice(qp.KernelShiftInvar(d=d,torchify=True,device=device),seqs=seqs_lattice[0]),\n        fastgps.FastGPLattice(qp.KernelShiftInvar(d=d,torchify=True,device=device),seqs=seqs_lattice[1]),\n    ],\n    [\n        fastgps.FastGPDigitalNetB2(qp.KernelDigShiftInvarCombined(d=d,torchify=True,device=device),seqs=seqs_dnb2s[0]),\n        fastgps.FastGPDigitalNetB2(qp.KernelDigShiftInvarCombined(d=d,torchify=True,device=device),seqs=seqs_dnb2s[1]),\n    ],\n]\ngpnames = [type(fgp_indep[i][0]).__name__ for i in range(ngptypes)]\nfor i in range(ngptypes):\n    print(gpnames[i])\n    for l in range(num_tasks):\n        if ns[0][l]&gt;0:\n            x_next = fgp_indep[i][l].get_x_next(n=ns[0][l].item())\n            y_next = torch.vstack([fs[i](x_next) for i in range(num_tasks)])\n            fgp_indep[i][l].add_y_next(y_next[l])\n            fgp_indep[i][l].fit()\n            pmeans[i][l],_,_,ci_lows[i][l],ci_highs[i][l] = fgp_indep[i][l].post_ci(xticks)\n            l2rerrors[i][l] = torch.linalg.norm(pmeans[i][l]-yticks[l])/torch.linalg.norm(yticks[l])\n</pre> pmeans = torch.nan*torch.ones((ngptypes,num_tasks,len(xticks))) ci_lows = torch.nan*torch.ones((ngptypes,num_tasks,len(xticks))) ci_highs = torch.nan*torch.ones((ngptypes,num_tasks,len(xticks))) l2rerrors = torch.ones((ngptypes,num_tasks)) fgp_indep = [      [         fastgps.StandardGP(qp.KernelSquaredExponential(d=d,torchify=True,device=device),seqs=seqs_std[0]),         fastgps.StandardGP(qp.KernelSquaredExponential(d=d,torchify=True,device=device),seqs=seqs_std[1]),     ],     [         fastgps.FastGPLattice(qp.KernelShiftInvar(d=d,torchify=True,device=device),seqs=seqs_lattice[0]),         fastgps.FastGPLattice(qp.KernelShiftInvar(d=d,torchify=True,device=device),seqs=seqs_lattice[1]),     ],     [         fastgps.FastGPDigitalNetB2(qp.KernelDigShiftInvarCombined(d=d,torchify=True,device=device),seqs=seqs_dnb2s[0]),         fastgps.FastGPDigitalNetB2(qp.KernelDigShiftInvarCombined(d=d,torchify=True,device=device),seqs=seqs_dnb2s[1]),     ], ] gpnames = [type(fgp_indep[i][0]).__name__ for i in range(ngptypes)] for i in range(ngptypes):     print(gpnames[i])     for l in range(num_tasks):         if ns[0][l]&gt;0:             x_next = fgp_indep[i][l].get_x_next(n=ns[0][l].item())             y_next = torch.vstack([fs[i](x_next) for i in range(num_tasks)])             fgp_indep[i][l].add_y_next(y_next[l])             fgp_indep[i][l].fit()             pmeans[i][l],_,_,ci_lows[i][l],ci_highs[i][l] = fgp_indep[i][l].post_ci(xticks)             l2rerrors[i][l] = torch.linalg.norm(pmeans[i][l]-yticks[l])/torch.linalg.norm(yticks[l]) <pre>StandardGP\n     iter of 5.0e+03 | best loss  | loss      \n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n            0.00e+00 | 6.99e+04   | 6.99e+04  \n            5.00e+00 | 5.40e+03   | 5.40e+03  \n            1.00e+01 | 2.68e+01   | 2.68e+01  \n            1.50e+01 | 9.35e+00   | 9.35e+00  \n            2.00e+01 | 2.23e-01   | 2.23e-01  \n            2.50e+01 | -2.76e+00  | -2.76e+00 \n            3.00e+01 | -3.46e+00  | -3.46e+00 \n            3.50e+01 | -3.60e+00  | -3.47e+00 \n            4.00e+01 | -3.61e+00  | -3.61e+00 \n            4.40e+01 | -3.61e+00  | -3.61e+00 \nFastGPLattice\n     iter of 5.0e+03 | best loss  | loss      \n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n            0.00e+00 | 7.11e+02   | 7.11e+02  \n            5.00e+00 | 1.65e+02   | 1.65e+02  \n            1.00e+01 | 1.35e+02   | 1.36e+02  \n            1.50e+01 | 1.35e+02   | 1.35e+02  \n            2.00e+01 | 1.35e+02   | 1.35e+02  \n            2.50e+01 | 1.35e+02   | 1.35e+02  \n            3.00e+01 | 1.34e+02   | 1.34e+02  \n            3.50e+01 | 1.34e+02   | 1.34e+02  \n            4.00e+01 | 1.34e+02   | 1.34e+02  \n            4.50e+01 | 1.34e+02   | 1.34e+02  \n            5.00e+01 | 1.34e+02   | 1.34e+02  \n            5.50e+01 | 1.34e+02   | 1.34e+02  \n            6.00e+01 | 1.34e+02   | 1.34e+02  \n            6.50e+01 | 1.33e+02   | 1.33e+02  \n            7.00e+01 | 1.33e+02   | 1.33e+02  \n            7.50e+01 | 1.33e+02   | 1.33e+02  \n            8.00e+01 | 1.33e+02   | 1.33e+02  \n            8.50e+01 | 1.32e+02   | 1.32e+02  \n            9.00e+01 | 1.32e+02   | 1.32e+02  \n            9.50e+01 | 1.32e+02   | 1.32e+02  \n            1.00e+02 | 1.32e+02   | 1.32e+02  \n            1.05e+02 | 1.32e+02   | 1.32e+02  \n            1.10e+02 | 1.31e+02   | 1.31e+02  \n            1.15e+02 | 1.31e+02   | 1.31e+02  \n            1.20e+02 | 1.30e+02   | 1.30e+02  \n            1.25e+02 | 1.30e+02   | 1.30e+02  \n            1.30e+02 | 1.30e+02   | 1.30e+02  \n            1.35e+02 | 1.29e+02   | 1.29e+02  \n            1.40e+02 | 1.28e+02   | 1.28e+02  \n            1.45e+02 | 1.28e+02   | 1.28e+02  \n            1.50e+02 | 1.28e+02   | 1.28e+02  \n            1.55e+02 | 1.28e+02   | 1.28e+02  \n            1.60e+02 | 1.27e+02   | 1.27e+02  \n            1.65e+02 | 1.27e+02   | 1.27e+02  \n            1.70e+02 | 1.27e+02   | 1.27e+02  \n            1.75e+02 | 1.27e+02   | 1.27e+02  \n            1.80e+02 | 1.26e+02   | 1.26e+02  \n            1.85e+02 | 1.26e+02   | 1.26e+02  \n            1.90e+02 | 1.26e+02   | 1.26e+02  \n            1.95e+02 | 1.25e+02   | 1.25e+02  \n            2.00e+02 | 1.25e+02   | 1.25e+02  \n            2.05e+02 | 1.25e+02   | 1.25e+02  \n            2.10e+02 | 1.25e+02   | 1.25e+02  \n            2.15e+02 | 1.24e+02   | 1.24e+02  \n            2.20e+02 | 1.24e+02   | 1.24e+02  \n            2.25e+02 | 1.24e+02   | 1.24e+02  \n            2.30e+02 | 1.23e+02   | 1.23e+02  \n            2.35e+02 | 1.23e+02   | 1.23e+02  \n            2.40e+02 | 1.23e+02   | 1.23e+02  \n            2.45e+02 | 1.23e+02   | 1.23e+02  \n            2.50e+02 | 1.23e+02   | 1.23e+02  \n            2.55e+02 | 1.23e+02   | 1.23e+02  \n            2.60e+02 | 1.23e+02   | 1.23e+02  \n            2.65e+02 | 1.23e+02   | 1.23e+02  \n            2.70e+02 | 1.23e+02   | 1.23e+02  \n            2.75e+02 | 1.23e+02   | 1.23e+02  \n            2.80e+02 | 1.23e+02   | 1.23e+02  \n            2.85e+02 | 1.23e+02   | 1.23e+02  \n            2.90e+02 | 1.23e+02   | 1.23e+02  \n            2.95e+02 | 1.23e+02   | 1.23e+02  \nFastGPDigitalNetB2\n     iter of 5.0e+03 | best loss  | loss      \n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n            0.00e+00 | 1.33e+02   | 1.33e+02  \n            5.00e+00 | 1.20e+02   | 1.20e+02  \n            1.00e+01 | 1.17e+02   | 1.17e+02  \n            1.50e+01 | 1.12e+02   | 1.12e+02  \n            2.00e+01 | 1.08e+02   | 1.08e+02  \n            2.50e+01 | 1.06e+02   | 1.06e+02  \n            3.00e+01 | 1.05e+02   | 1.05e+02  \n            3.50e+01 | 1.04e+02   | 1.04e+02  \n            4.00e+01 | 1.04e+02   | 1.04e+02  \n            4.50e+01 | 1.03e+02   | 1.03e+02  \n            5.00e+01 | 1.03e+02   | 1.03e+02  \n            5.50e+01 | 1.03e+02   | 1.03e+02  \n            6.00e+01 | 1.03e+02   | 1.03e+02  \n            6.50e+01 | 1.03e+02   | 1.03e+02  \n            7.00e+01 | 1.02e+02   | 1.02e+02  \n            7.50e+01 | 1.02e+02   | 1.02e+02  \n            8.00e+01 | 1.02e+02   | 1.02e+02  \n            8.50e+01 | 1.02e+02   | 1.02e+02  \n            9.00e+01 | 1.02e+02   | 1.02e+02  \n            9.50e+01 | 1.02e+02   | 1.02e+02  \n            1.00e+02 | 1.01e+02   | 1.01e+02  \n            1.05e+02 | 1.01e+02   | 1.01e+02  \n            1.10e+02 | 1.01e+02   | 1.01e+02  \n            1.13e+02 | 1.01e+02   | 1.01e+02  \n</pre> In\u00a0[59]: Copied! <pre>fgp_multitask = [\n    fastgps.StandardGP(qp.KernelMultiTask(qp.KernelSquaredExponential(d,torchify=True,device=device),num_tasks=num_tasks),seqs=seqs_std),\n    fastgps.FastGPLattice(qp.KernelMultiTask(qp.KernelShiftInvar(d,torchify=True,device=device),num_tasks=num_tasks),seqs=seqs_lattice),\n    fastgps.FastGPDigitalNetB2(qp.KernelMultiTask(qp.KernelDigShiftInvarCombined(d,torchify=True,device=device),num_tasks=num_tasks),seqs=seqs_dnb2s),\n]\npmeans_mt = torch.nan*torch.ones((ngptypes,num_tasks,len(xticks)))\nci_lows_mt = torch.nan*torch.ones((ngptypes,num_tasks,len(xticks)))\nci_highs_mt = torch.nan*torch.ones((ngptypes,num_tasks,len(xticks)))\nl2rerrors_mt = torch.ones((ngptypes,num_tasks))\nfor i in range(ngptypes):\n    print(gpnames[i])\n    x_next = fgp_multitask[i].get_x_next(n=ns[1])\n    y_next = [fs[i](x_next[i]) for i in range(num_tasks)]\n    fgp_multitask[i].add_y_next(y_next)\n    fgp_multitask[i].fit()\n    pmeans_mt[i],_,_,ci_lows_mt[i],ci_highs_mt[i] = fgp_multitask[i].post_ci(xticks)\n    l2rerrors_mt[i] = torch.linalg.norm(pmeans_mt[i]-yticks,dim=-1)/torch.linalg.norm(yticks,dim=-1)\n</pre> fgp_multitask = [     fastgps.StandardGP(qp.KernelMultiTask(qp.KernelSquaredExponential(d,torchify=True,device=device),num_tasks=num_tasks),seqs=seqs_std),     fastgps.FastGPLattice(qp.KernelMultiTask(qp.KernelShiftInvar(d,torchify=True,device=device),num_tasks=num_tasks),seqs=seqs_lattice),     fastgps.FastGPDigitalNetB2(qp.KernelMultiTask(qp.KernelDigShiftInvarCombined(d,torchify=True,device=device),num_tasks=num_tasks),seqs=seqs_dnb2s), ] pmeans_mt = torch.nan*torch.ones((ngptypes,num_tasks,len(xticks))) ci_lows_mt = torch.nan*torch.ones((ngptypes,num_tasks,len(xticks))) ci_highs_mt = torch.nan*torch.ones((ngptypes,num_tasks,len(xticks))) l2rerrors_mt = torch.ones((ngptypes,num_tasks)) for i in range(ngptypes):     print(gpnames[i])     x_next = fgp_multitask[i].get_x_next(n=ns[1])     y_next = [fs[i](x_next[i]) for i in range(num_tasks)]     fgp_multitask[i].add_y_next(y_next)     fgp_multitask[i].fit()     pmeans_mt[i],_,_,ci_lows_mt[i],ci_highs_mt[i] = fgp_multitask[i].post_ci(xticks)     l2rerrors_mt[i] = torch.linalg.norm(pmeans_mt[i]-yticks,dim=-1)/torch.linalg.norm(yticks,dim=-1) <pre>StandardGP\n     iter of 5.0e+03 | best loss  | loss      \n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n            0.00e+00 | 1.38e+05   | 1.38e+05  \n            5.00e+00 | 3.04e+03   | 3.04e+03  \n            1.00e+01 | -1.58e+02  | -1.58e+02 \n            1.50e+01 | -1.75e+02  | -1.75e+02 \n            2.00e+01 | -1.97e+02  | -1.97e+02 \n            2.50e+01 | -2.00e+02  | -2.00e+02 \n            3.00e+01 | -2.00e+02  | -2.00e+02 \n            3.50e+01 | -2.01e+02  | -2.01e+02 \n            4.00e+01 | -2.01e+02  | -2.01e+02 \n            4.40e+01 | -2.01e+02  | -2.01e+02 \nFastGPLattice\n     iter of 5.0e+03 | best loss  | loss      \n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n            0.00e+00 | 2.46e+03   | 2.46e+03  \n            5.00e+00 | 3.63e+02   | 3.63e+02  \n            1.00e+01 | 3.39e+02   | 3.39e+02  \n            1.50e+01 | 3.28e+02   | 3.28e+02  \n            2.00e+01 | 3.14e+02   | 3.14e+02  \n            2.50e+01 | 3.13e+02   | 3.15e+02  \n            3.00e+01 | 3.11e+02   | 3.11e+02  \n            3.50e+01 | 3.09e+02   | 3.09e+02  \n            4.00e+01 | 3.07e+02   | 3.07e+02  \n            4.50e+01 | 3.03e+02   | 3.03e+02  \n            5.00e+01 | 3.01e+02   | 3.01e+02  \n            5.50e+01 | 2.99e+02   | 2.99e+02  \n            6.00e+01 | 2.97e+02   | 2.97e+02  \n            6.50e+01 | 2.95e+02   | 2.95e+02  \n            7.00e+01 | 2.95e+02   | 2.95e+02  \n            7.50e+01 | 2.94e+02   | 2.94e+02  \n            8.00e+01 | 2.94e+02   | 2.94e+02  \n            8.50e+01 | 2.94e+02   | 2.94e+02  \n            9.00e+01 | 2.94e+02   | 2.94e+02  \n            9.50e+01 | 2.94e+02   | 2.94e+02  \n            1.00e+02 | 2.93e+02   | 2.93e+02  \n            1.05e+02 | 2.93e+02   | 2.93e+02  \n            1.10e+02 | 2.93e+02   | 2.93e+02  \n            1.15e+02 | 2.93e+02   | 2.93e+02  \n            1.20e+02 | 2.93e+02   | 2.93e+02  \n            1.25e+02 | 2.93e+02   | 2.93e+02  \n            1.30e+02 | 2.93e+02   | 2.93e+02  \n            1.35e+02 | 2.93e+02   | 2.93e+02  \n            1.40e+02 | 2.93e+02   | 2.93e+02  \n            1.44e+02 | 2.93e+02   | 2.93e+02  \nFastGPDigitalNetB2\n     iter of 5.0e+03 | best loss  | loss      \n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n            0.00e+00 | 3.42e+02   | 3.42e+02  \n            5.00e+00 | 2.36e+02   | 2.37e+02  \n            1.00e+01 | 2.16e+02   | 2.16e+02  \n            1.50e+01 | 1.99e+02   | 1.99e+02  \n            2.00e+01 | 1.80e+02   | 1.80e+02  \n            2.50e+01 | 1.71e+02   | 1.71e+02  \n            3.00e+01 | 1.65e+02   | 1.65e+02  \n            3.50e+01 | 1.61e+02   | 1.61e+02  \n            4.00e+01 | 1.60e+02   | 1.60e+02  \n            4.50e+01 | 1.60e+02   | 1.60e+02  \n            5.00e+01 | 1.59e+02   | 1.59e+02  \n            5.50e+01 | 1.59e+02   | 1.59e+02  \n            6.00e+01 | 1.59e+02   | 1.59e+02  \n            6.50e+01 | 1.59e+02   | 1.59e+02  \n            7.00e+01 | 1.59e+02   | 1.59e+02  \n            7.50e+01 | 1.58e+02   | 1.58e+02  \n            8.00e+01 | 1.58e+02   | 1.58e+02  \n            8.50e+01 | 1.58e+02   | 1.58e+02  \n            9.00e+01 | 1.58e+02   | 1.58e+02  \n            9.50e+01 | 1.58e+02   | 1.58e+02  \n            1.00e+02 | 1.58e+02   | 1.58e+02  \n            1.05e+02 | 1.58e+02   | 1.58e+02  \n            1.10e+02 | 1.58e+02   | 1.58e+02  \n            1.12e+02 | 1.58e+02   | 1.58e+02  \n</pre> In\u00a0[60]: Copied! <pre>pd.DataFrame({gpnames[i]+\" task %d\"%j: [l2rerrors[i,j].item(),l2rerrors_mt[i,j].item()] for j in range(num_tasks) for i in range(ngptypes)})\n</pre> pd.DataFrame({gpnames[i]+\" task %d\"%j: [l2rerrors[i,j].item(),l2rerrors_mt[i,j].item()] for j in range(num_tasks) for i in range(ngptypes)}) Out[60]: StandardGP task 0 FastGPLattice task 0 FastGPDigitalNetB2 task 0 StandardGP task 1 FastGPLattice task 1 FastGPDigitalNetB2 task 1 0 1.000000 1.000000 1.000000 0.003604 0.089231 0.072418 1 0.001897 0.082094 0.033718 0.005856 0.089346 0.041112 In\u00a0[61]: Copied! <pre>cmap = sns.cubehelix_palette(start=.5, rot=-.75, as_cmap=True) # https://seaborn.pydata.org/tutorial/color_palettes.html\nnrows = num_tasks\nncols = 3 \nfor l in range(ngptypes):\n    fig,ax = pyplot.subplots(nrows=nrows,ncols=ncols,subplot_kw={'projection': '3d'},figsize=(4*ncols,4*nrows))\n    for i in range(num_tasks):\n        for j,(yplt,name) in enumerate(zip([yticks[i],pmeans[l][i],pmeans_mt[l][i]],[\"true solution\",\"single task GP\",\"MTGP\"])):\n            ax[i,j].plot_surface(x0mesh,x1mesh,yplt.reshape(x0mesh.shape),rstride=1,cstride=1,antialiased=True,cmap=\"gnuplot2\")\n            ax[i,j].set_title(name,fontsize=\"xx-large\")\n            ax[i,j].set_xlim([0,1])\n            ax[i,j].set_xticks([0,1])\n            ax[i,j].set_ylim([0,1])\n            ax[i,j].set_yticks([0,1])\n            ax[i,j].grid(False)\n            ax[i,j].xaxis.pane.set_alpha(0.0)\n            ax[i,j].yaxis.pane.set_alpha(0.0)\n            ax[i,j].zaxis.pane.set_alpha(0.0)\n            ax[i,j].view_init(azim=135)\n        # ax[i,0].set_title(\"task %d\"%i,rotation=\"vertical\",x=0,y=0.5,fontsize=\"xx-large\")\n        fig.text(ax[0,0].get_position().x0-.05,(ax[i,0].get_position().y0+ax[i,0].get_position().y1)/2,\"task %d\"%i,rotation=\"vertical\",fontsize=\"xx-large\")\n    fig.suptitle(gpnames[l],fontsize=\"xx-large\")\n    fig.savefig(\"./mtgps.2d.%s.pdf\"%gpnames[l],bbox_inches=\"tight\")\n</pre> cmap = sns.cubehelix_palette(start=.5, rot=-.75, as_cmap=True) # https://seaborn.pydata.org/tutorial/color_palettes.html nrows = num_tasks ncols = 3  for l in range(ngptypes):     fig,ax = pyplot.subplots(nrows=nrows,ncols=ncols,subplot_kw={'projection': '3d'},figsize=(4*ncols,4*nrows))     for i in range(num_tasks):         for j,(yplt,name) in enumerate(zip([yticks[i],pmeans[l][i],pmeans_mt[l][i]],[\"true solution\",\"single task GP\",\"MTGP\"])):             ax[i,j].plot_surface(x0mesh,x1mesh,yplt.reshape(x0mesh.shape),rstride=1,cstride=1,antialiased=True,cmap=\"gnuplot2\")             ax[i,j].set_title(name,fontsize=\"xx-large\")             ax[i,j].set_xlim([0,1])             ax[i,j].set_xticks([0,1])             ax[i,j].set_ylim([0,1])             ax[i,j].set_yticks([0,1])             ax[i,j].grid(False)             ax[i,j].xaxis.pane.set_alpha(0.0)             ax[i,j].yaxis.pane.set_alpha(0.0)             ax[i,j].zaxis.pane.set_alpha(0.0)             ax[i,j].view_init(azim=135)         # ax[i,0].set_title(\"task %d\"%i,rotation=\"vertical\",x=0,y=0.5,fontsize=\"xx-large\")         fig.text(ax[0,0].get_position().x0-.05,(ax[i,0].get_position().y0+ax[i,0].get_position().y1)/2,\"task %d\"%i,rotation=\"vertical\",fontsize=\"xx-large\")     fig.suptitle(gpnames[l],fontsize=\"xx-large\")     fig.savefig(\"./mtgps.2d.%s.pdf\"%gpnames[l],bbox_inches=\"tight\") In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/multitask/compare_2d_gps_plot/#compare-gps-plot","title":"Compare GPs + Plot\u00b6","text":""},{"location":"examples/multitask/compare_2d_gps_plot/#true-function","title":"True Function\u00b6","text":""},{"location":"examples/multitask/compare_2d_gps_plot/#parameters","title":"Parameters\u00b6","text":""},{"location":"examples/multitask/compare_2d_gps_plot/#independent-single-task-gps","title":"Independent Single Task GPs\u00b6","text":""},{"location":"examples/multitask/compare_2d_gps_plot/#multi-task-fast-gps","title":"Multi-Task Fast GPs\u00b6","text":""},{"location":"examples/multitask/compare_2d_gps_plot/#compare-accuracy","title":"Compare Accuracy\u00b6","text":""},{"location":"examples/multitask/compare_2d_gps_plot/#plot","title":"Plot\u00b6","text":""},{"location":"examples/multitask/elliptic_pde/","title":"Elliptic PDE","text":"In\u00a0[1]: Copied! <pre>import fastgps \nimport qmcpy as qp \nimport torch \ntorch.set_default_dtype(torch.float64)\nimport numpy as np\nfrom scipy.sparse import diags\nfrom scipy.sparse.linalg import spsolve\nfrom scipy.stats import norm\nimport itertools\nimport time\nimport pandas as pd\nimport matplotlib\nfrom matplotlib import pyplot\npyplot.style.use(\"seaborn-v0_8-whitegrid\")\nCOLORS = [\"xkcd:\"+color[:-1] for color in pd.read_csv(\"../../../xkcd_colors.txt\",comment=\"#\",header=None).iloc[:,0].tolist()][::-1]\npyplot.rcParams['axes.prop_cycle'] = matplotlib.cycler(color=COLORS)\nLINESTYLES = ['solid','dotted','dashed','dashdot',(0, (1, 1))]\nDEFAULTFONTSIZE = 30\npyplot.rcParams['xtick.labelsize'] = DEFAULTFONTSIZE\npyplot.rcParams['ytick.labelsize'] = DEFAULTFONTSIZE\npyplot.rcParams['ytick.labelsize'] = DEFAULTFONTSIZE\npyplot.rcParams['axes.titlesize'] = DEFAULTFONTSIZE\npyplot.rcParams['figure.titlesize'] = DEFAULTFONTSIZE\npyplot.rcParams[\"axes.labelsize\"] = DEFAULTFONTSIZE\npyplot.rcParams['legend.fontsize'] = DEFAULTFONTSIZE\npyplot.rcParams['font.size'] = DEFAULTFONTSIZE\npyplot.rcParams['lines.linewidth'] = 5\npyplot.rcParams['lines.markersize'] = 15\nPW = 30 # inches\n</pre> import fastgps  import qmcpy as qp  import torch  torch.set_default_dtype(torch.float64) import numpy as np from scipy.sparse import diags from scipy.sparse.linalg import spsolve from scipy.stats import norm import itertools import time import pandas as pd import matplotlib from matplotlib import pyplot pyplot.style.use(\"seaborn-v0_8-whitegrid\") COLORS = [\"xkcd:\"+color[:-1] for color in pd.read_csv(\"../../../xkcd_colors.txt\",comment=\"#\",header=None).iloc[:,0].tolist()][::-1] pyplot.rcParams['axes.prop_cycle'] = matplotlib.cycler(color=COLORS) LINESTYLES = ['solid','dotted','dashed','dashdot',(0, (1, 1))] DEFAULTFONTSIZE = 30 pyplot.rcParams['xtick.labelsize'] = DEFAULTFONTSIZE pyplot.rcParams['ytick.labelsize'] = DEFAULTFONTSIZE pyplot.rcParams['ytick.labelsize'] = DEFAULTFONTSIZE pyplot.rcParams['axes.titlesize'] = DEFAULTFONTSIZE pyplot.rcParams['figure.titlesize'] = DEFAULTFONTSIZE pyplot.rcParams[\"axes.labelsize\"] = DEFAULTFONTSIZE pyplot.rcParams['legend.fontsize'] = DEFAULTFONTSIZE pyplot.rcParams['font.size'] = DEFAULTFONTSIZE pyplot.rcParams['lines.linewidth'] = 5 pyplot.rcParams['lines.markersize'] = 15 PW = 30 # inches In\u00a0[32]: Copied! <pre>def _spsolve(main_diag, upper_diag, lower_diag, b):\n    N = main_diag.shape[-1]\n    assert main_diag.shape==(N,) and upper_diag.shape==(N-1,) and lower_diag.shape==(N-1,) and b.shape==(N,)\n    A = diags([main_diag, upper_diag, lower_diag],[0, 1, -1],shape=[N,N],format=\"csc\",)\n    u = spsolve(A, b)\n    return u\nvec_spsolve = np.vectorize(_spsolve,signature=\"(n),(m),(m),(n)-&gt;(n)\")\ndef solve_elliptic_pde(level=5, coeffs=None):\n    # Define grid\n    N = 2 ** (level + 2)  # Number of intervals\n    h = 1.0 / N  # Mesh spacing\n    x = np.linspace(h, 1 - h, N - 1)  # Interior points\n    # Compute diffusion coefficient a(x)\n    coeffs = np.random.rand(8) if coeffs is None else coeffs\n    assert isinstance(coeffs,np.ndarray)\n    coeffs = norm.ppf(coeffs)  # Transform from uniform to iid Gaussian\n    batch_shape = list(coeffs.shape)[:-1]\n    k = np.arange(1,coeffs.shape[-1]+1)\n    a_x = np.exp((coeffs[...,None] / k[:,None]  *np.sin(np.pi * k[:,None] * x)).sum(-2))\n    # Compute a at half-grid points (needed for flux terms)\n    a_half = np.zeros(batch_shape+[N])\n    a_half[...,1:-1] = (a_x[...,:-1] + a_x[...,1:]) / 2  # Midpoint values for flux approximation\n    a_half[...,0] = a_x[...,0]  # At the first midpoint\n    a_half[...,-1] = a_x[...,-1]  # At the last midpoint\n    # Construct the finite difference matrix\n    lower_diag = -a_half[...,1:-1] / h**2\n    upper_diag = -a_half[...,1:-1] / h**2\n    main_diag = (a_half[...,:-1] + a_half[...,1:]) / h**2\n    # Right-hand side (forcing term)\n    b = np.ones([N-1])  # Constant source term (1)\n    u = vec_spsolve(main_diag, upper_diag, lower_diag, b)\n    # Find index closest to x = 0.5\n    idx = np.argmin(np.abs(x - 0.5))\n    return u[...,idx], u, x, a_x, x[idx]\ndef elliptic(level, samples):\n    return solve_elliptic_pde(level,samples)[0]\nlevels = np.array([1,5],dtype=int)\nnum_levels = len(levels)\nd = 16\ncosts = 2.**levels/2.**levels.max()\nntest = 2**10\nxtest = qp.Halton(d,seed=7)(ntest)\nqtest = np.zeros((num_levels,ntest))\nfig,ax = pyplot.subplots(nrows=2,ncols=num_levels,figsize=(PW,PW/num_levels*2))\nQprev = 0\nfor l in range(num_levels):\n    Ql,ql,ul,al,u0l = solve_elliptic_pde(levels[l],xtest)\n    Yl = Ql-Qprev\n    Qprev = Ql\n    qtest[l] = Ql\n    print(\"level = %-5d Ql.mean() = %-10.2f Ql.std() = %-10.1e Yl.mean() = %-10.1e Yl.std() = %-10.1e Ql.shape = %-10s ql.shape = %-15s ul.shape = %-10s al.shape = %-15s u0l = %-10s\"%\\\n        (l,Ql.mean(),Ql.std(ddof=1),Yl.mean(),Yl.std(ddof=1),Ql.shape,ql.shape,ul.shape,ql.shape,u0l))\n    ax[0,l].plot(ul,ql.T)\n    ax[0,l].scatter(u0l*np.ones_like(Ql),Ql)\n    ax[1,l].plot(ul,al.T)\nax[0,0].set_ylabel(\"solutions\")\nax[1,0].set_ylabel(\"forcing term\")\nprint(\"xtest.shape = %s\"%str(xtest.shape))\nprint(\"qtest.shape = %s\"%str(qtest.shape))\nprint(costs)\n</pre> def _spsolve(main_diag, upper_diag, lower_diag, b):     N = main_diag.shape[-1]     assert main_diag.shape==(N,) and upper_diag.shape==(N-1,) and lower_diag.shape==(N-1,) and b.shape==(N,)     A = diags([main_diag, upper_diag, lower_diag],[0, 1, -1],shape=[N,N],format=\"csc\",)     u = spsolve(A, b)     return u vec_spsolve = np.vectorize(_spsolve,signature=\"(n),(m),(m),(n)-&gt;(n)\") def solve_elliptic_pde(level=5, coeffs=None):     # Define grid     N = 2 ** (level + 2)  # Number of intervals     h = 1.0 / N  # Mesh spacing     x = np.linspace(h, 1 - h, N - 1)  # Interior points     # Compute diffusion coefficient a(x)     coeffs = np.random.rand(8) if coeffs is None else coeffs     assert isinstance(coeffs,np.ndarray)     coeffs = norm.ppf(coeffs)  # Transform from uniform to iid Gaussian     batch_shape = list(coeffs.shape)[:-1]     k = np.arange(1,coeffs.shape[-1]+1)     a_x = np.exp((coeffs[...,None] / k[:,None]  *np.sin(np.pi * k[:,None] * x)).sum(-2))     # Compute a at half-grid points (needed for flux terms)     a_half = np.zeros(batch_shape+[N])     a_half[...,1:-1] = (a_x[...,:-1] + a_x[...,1:]) / 2  # Midpoint values for flux approximation     a_half[...,0] = a_x[...,0]  # At the first midpoint     a_half[...,-1] = a_x[...,-1]  # At the last midpoint     # Construct the finite difference matrix     lower_diag = -a_half[...,1:-1] / h**2     upper_diag = -a_half[...,1:-1] / h**2     main_diag = (a_half[...,:-1] + a_half[...,1:]) / h**2     # Right-hand side (forcing term)     b = np.ones([N-1])  # Constant source term (1)     u = vec_spsolve(main_diag, upper_diag, lower_diag, b)     # Find index closest to x = 0.5     idx = np.argmin(np.abs(x - 0.5))     return u[...,idx], u, x, a_x, x[idx] def elliptic(level, samples):     return solve_elliptic_pde(level,samples)[0] levels = np.array([1,5],dtype=int) num_levels = len(levels) d = 16 costs = 2.**levels/2.**levels.max() ntest = 2**10 xtest = qp.Halton(d,seed=7)(ntest) qtest = np.zeros((num_levels,ntest)) fig,ax = pyplot.subplots(nrows=2,ncols=num_levels,figsize=(PW,PW/num_levels*2)) Qprev = 0 for l in range(num_levels):     Ql,ql,ul,al,u0l = solve_elliptic_pde(levels[l],xtest)     Yl = Ql-Qprev     Qprev = Ql     qtest[l] = Ql     print(\"level = %-5d Ql.mean() = %-10.2f Ql.std() = %-10.1e Yl.mean() = %-10.1e Yl.std() = %-10.1e Ql.shape = %-10s ql.shape = %-15s ul.shape = %-10s al.shape = %-15s u0l = %-10s\"%\\         (l,Ql.mean(),Ql.std(ddof=1),Yl.mean(),Yl.std(ddof=1),Ql.shape,ql.shape,ul.shape,ql.shape,u0l))     ax[0,l].plot(ul,ql.T)     ax[0,l].scatter(u0l*np.ones_like(Ql),Ql)     ax[1,l].plot(ul,al.T) ax[0,0].set_ylabel(\"solutions\") ax[1,0].set_ylabel(\"forcing term\") print(\"xtest.shape = %s\"%str(xtest.shape)) print(\"qtest.shape = %s\"%str(qtest.shape)) print(costs) <pre>level = 0     Ql.mean() = 0.15       Ql.std() = 9.9e-02    Yl.mean() = 1.5e-01    Yl.std() = 9.9e-02    Ql.shape = (1024,)    ql.shape = (1024, 7)       ul.shape = (7,)       al.shape = (1024, 7)       u0l = 0.5       \nlevel = 1     Ql.mean() = 0.15       Ql.std() = 9.2e-02    Yl.mean() = 6.6e-03    Yl.std() = 1.9e-02    Ql.shape = (1024,)    ql.shape = (1024, 127)     ul.shape = (127,)     al.shape = (1024, 127)     u0l = 0.5       \nxtest.shape = (1024, 16)\nqtest.shape = (2, 1024)\n[0.0625 1.    ]\n</pre> In\u00a0[40]: Copied! <pre>def run_gpr(name,n):\n    assert n.shape==(num_levels,) \n    if name==\"standard multitask GP\":\n        fgp = fastgps.StandardGP(\n            kernel = qp.KernelMultiTask(\n                base_kernel = qp.KernelSquaredExponential(\n                    d = d,\n                    torchify = True,\n                ),\n                num_tasks = num_levels,\n                rank_factor = 1,\n            ),\n            seqs = [qp.DigitalNetB2(dimension = d) for i in range(num_levels)],\n        )\n    elif name==\"fast multitask GP - digital net\":\n        fgp = fastgps.FastGPDigitalNetB2(\n            kernel = qp.KernelMultiTask(\n                base_kernel = qp.KernelDigShiftInvar(\n                    d = d,\n                    torchify = True,\n                ),\n                num_tasks = num_levels,\n                rank_factor = 1,\n            ),\n            seqs = [qp.DigitalNetB2(dimension = d, randomize=\"DS\") for i in range(num_levels)],\n        )\n    elif name==\"fast multitask GP - lattice\":\n        fgp = fastgps.FastGPLattice(\n            kernel = qp.KernelMultiTask(\n                base_kernel = qp.KernelShiftInvar(\n                    d = d,\n                    torchify = True,\n                    alpha = 3,\n                ),\n                num_tasks = num_levels,\n                rank_factor = 1,\n            ),\n            seqs = [qp.Lattice(dimension = d) for i in range(num_levels)],\n        )\n    else:\n        raise Exception(\"invalid name = %s\"%name)\n    xnext = fgp.get_x_next(torch.from_numpy(n))\n    ynext = [torch.from_numpy(elliptic(levels[l],xnext_l.numpy())) for l,xnext_l in enumerate(xnext)]\n    fgp.add_y_next(ynext) \n    t0 = time.perf_counter()\n    fgp.fit(\n        loss_metric = \"MLL\",\n        iterations = 100,\n        #stop_crit_wait_iterations = np.inf,\n        #stop_crit_improvement_threshold = np.inf,\n        verbose = 0,\n    )\n    tdiff = time.perf_counter()-t0\n    yhat = fgp.post_mean(torch.from_numpy(xtest)).numpy()\n    l2rerrors = np.linalg.norm(yhat-qtest,axis=1)/np.linalg.norm(qtest,axis=-1)\n    return l2rerrors,tdiff\nnames = [\n    \"standard multitask GP\",\n    \"fast multitask GP - digital net\",\n    # \"fast multitask GP - lattice\",\n]\nmmin = 10\nk = 1\ntrials = 1\n# ns = (2**(mmin+np.stack([n for n in itertools.product(*[list(range(k)) for l in range(num_levels)])])))[::-1]\nns = np.array([\n    [2**8,2**6],\n    [2**9,2**6],\n    [2**10,2**6],\n    [2**11,2**6],\n    [2**12,2**6],\n])\nprint(\"ns.shape = %s\"%str(ns.shape))\nprint(ns)\nprint()\nprint(\"ns.shape = %s\"%str(ns.shape))\nprint(\"ns[:5]\")\nprint(ns[:5])\nprint()\nverbose = 1\nl2rerrors = np.nan*np.empty((len(names),len(ns),num_levels,trials))\ntimes = np.nan*np.empty((len(names),len(ns),trials))\nfor i in range(len(ns)):\n    for j,name in enumerate(names):\n        for t in range(trials):\n            l2rerrors[j,i,:,t],times[j,i,t] = run_gpr(name,ns[i])\n            with np.printoptions(formatter={\"float\":lambda x: \"%.1e\"%x}):\n                print(\"%35s: t = %-5d i = %-5d n = %-15s times[j,i,t] = %-10.1e l2rerrors[j,i,:,t] = %s\"%(name,t,i,ns[i],times[j,i,t],l2rerrors[j,i,:,t]))\n    print()\n</pre> def run_gpr(name,n):     assert n.shape==(num_levels,)      if name==\"standard multitask GP\":         fgp = fastgps.StandardGP(             kernel = qp.KernelMultiTask(                 base_kernel = qp.KernelSquaredExponential(                     d = d,                     torchify = True,                 ),                 num_tasks = num_levels,                 rank_factor = 1,             ),             seqs = [qp.DigitalNetB2(dimension = d) for i in range(num_levels)],         )     elif name==\"fast multitask GP - digital net\":         fgp = fastgps.FastGPDigitalNetB2(             kernel = qp.KernelMultiTask(                 base_kernel = qp.KernelDigShiftInvar(                     d = d,                     torchify = True,                 ),                 num_tasks = num_levels,                 rank_factor = 1,             ),             seqs = [qp.DigitalNetB2(dimension = d, randomize=\"DS\") for i in range(num_levels)],         )     elif name==\"fast multitask GP - lattice\":         fgp = fastgps.FastGPLattice(             kernel = qp.KernelMultiTask(                 base_kernel = qp.KernelShiftInvar(                     d = d,                     torchify = True,                     alpha = 3,                 ),                 num_tasks = num_levels,                 rank_factor = 1,             ),             seqs = [qp.Lattice(dimension = d) for i in range(num_levels)],         )     else:         raise Exception(\"invalid name = %s\"%name)     xnext = fgp.get_x_next(torch.from_numpy(n))     ynext = [torch.from_numpy(elliptic(levels[l],xnext_l.numpy())) for l,xnext_l in enumerate(xnext)]     fgp.add_y_next(ynext)      t0 = time.perf_counter()     fgp.fit(         loss_metric = \"MLL\",         iterations = 100,         #stop_crit_wait_iterations = np.inf,         #stop_crit_improvement_threshold = np.inf,         verbose = 0,     )     tdiff = time.perf_counter()-t0     yhat = fgp.post_mean(torch.from_numpy(xtest)).numpy()     l2rerrors = np.linalg.norm(yhat-qtest,axis=1)/np.linalg.norm(qtest,axis=-1)     return l2rerrors,tdiff names = [     \"standard multitask GP\",     \"fast multitask GP - digital net\",     # \"fast multitask GP - lattice\", ] mmin = 10 k = 1 trials = 1 # ns = (2**(mmin+np.stack([n for n in itertools.product(*[list(range(k)) for l in range(num_levels)])])))[::-1] ns = np.array([     [2**8,2**6],     [2**9,2**6],     [2**10,2**6],     [2**11,2**6],     [2**12,2**6], ]) print(\"ns.shape = %s\"%str(ns.shape)) print(ns) print() print(\"ns.shape = %s\"%str(ns.shape)) print(\"ns[:5]\") print(ns[:5]) print() verbose = 1 l2rerrors = np.nan*np.empty((len(names),len(ns),num_levels,trials)) times = np.nan*np.empty((len(names),len(ns),trials)) for i in range(len(ns)):     for j,name in enumerate(names):         for t in range(trials):             l2rerrors[j,i,:,t],times[j,i,t] = run_gpr(name,ns[i])             with np.printoptions(formatter={\"float\":lambda x: \"%.1e\"%x}):                 print(\"%35s: t = %-5d i = %-5d n = %-15s times[j,i,t] = %-10.1e l2rerrors[j,i,:,t] = %s\"%(name,t,i,ns[i],times[j,i,t],l2rerrors[j,i,:,t]))     print() <pre>ns.shape = (5, 2)\n[[ 256   64]\n [ 512   64]\n [1024   64]\n [2048   64]\n [4096   64]]\n\nns.shape = (5, 2)\nns[:5]\n[[ 256   64]\n [ 512   64]\n [1024   64]\n [2048   64]\n [4096   64]]\n\n              standard multitask GP: t = 0     i = 0     n = [256  64]       times[j,i,t] = 5.4e-01    l2rerrors[j,i,:,t] = [1.5e-01 2.5e-01]\n    fast multitask GP - digital net: t = 0     i = 0     n = [256  64]       times[j,i,t] = 3.2e-01    l2rerrors[j,i,:,t] = [1.4e-01 1.4e-01]\n\n              standard multitask GP: t = 0     i = 1     n = [512  64]       times[j,i,t] = 1.5e+00    l2rerrors[j,i,:,t] = [9.7e-02 1.7e-01]\n    fast multitask GP - digital net: t = 0     i = 1     n = [512  64]       times[j,i,t] = 3.4e-01    l2rerrors[j,i,:,t] = [9.9e-02 1.3e-01]\n\n              standard multitask GP: t = 0     i = 2     n = [1024   64]     times[j,i,t] = 4.3e+00    l2rerrors[j,i,:,t] = [1.4e-01 2.0e-01]\n    fast multitask GP - digital net: t = 0     i = 2     n = [1024   64]     times[j,i,t] = 3.3e-01    l2rerrors[j,i,:,t] = [1.5e-01 1.2e-01]\n\n              standard multitask GP: t = 0     i = 3     n = [2048   64]     times[j,i,t] = 2.0e+01    l2rerrors[j,i,:,t] = [8.2e-02 1.6e-01]\n    fast multitask GP - digital net: t = 0     i = 3     n = [2048   64]     times[j,i,t] = 7.2e-01    l2rerrors[j,i,:,t] = [6.6e-02 9.8e-02]\n\n              standard multitask GP: t = 0     i = 4     n = [4096   64]     times[j,i,t] = 8.5e+01    l2rerrors[j,i,:,t] = [6.7e-02 1.7e-01]\n    fast multitask GP - digital net: t = 0     i = 4     n = [4096   64]     times[j,i,t] = 9.4e-01    l2rerrors[j,i,:,t] = [5.7e-02 1.2e-01]\n\n</pre> In\u00a0[\u00a0]: Copied! <pre>fig,axs = pyplot.subplots(nrows=1,ncols=2,figsize=(PW/2,PW/2/2))\nax = axs[0]\nx_l0 = qp.DigitalNetB2(2,seed=7)(2**5)\nx_l1 = qp.DigitalNetB2(2,seed=11)(2**3) \nax.scatter(x_l0[:,0],x_l0[:,1],label=\"source\",s=250,color=\"red\",marker=\"o\")\nax.scatter(x_l1[:,0],x_l1[:,1],label=\"target\",s=250,color=\"blue\",marker=\"^\")\n# ax.set_xlim([0,1])\nax.set_xticks([0,1/4,1/2,3/4,1]); ax.set_xticklabels([r\"$0$\",r\"$1/4$\",r\"$1/2$\",r\"$3/4$\",r\"$1$\"])\n# ax.set_ylim([0,1])\nax.set_yticks([0,1/4,1/2,3/4,1]); ax.set_yticklabels([r\"$0$\",r\"$1/4$\",r\"$1/2$\",r\"$3/4$\",r\"$1$\"])\nfig.legend(frameon=False,bbox_to_anchor=(.47,1.1),ncol=3)\nax = axs[1]\nfor j in range(1,len(names)):\n    xtrend = np.median(times[j],axis=-1)/np.median(times[0],axis=-1)\n    ytrend = np.median(l2rerrors[j,:,-1],axis=-1)/np.median(l2rerrors[0,:,-1],axis=-1)\n    for k in range(len(ns)):\n        ax.scatter(xtrend[k],ytrend[k],color=\"k\",zorder=10)\n        ax.annotate(r\"$2^{%d}$\"%int(np.log2(ns[k,0])),(xtrend[k]*.8,ytrend[k]+.015))\n# ax.set_xscale(\"log\",base=10)\n# ax.set_yscale(\"log\",base=10)\nax.set_xlabel(\"computation time ratio\")\nax.set_ylabel(\"prediction error ratio\")\nax.axhline(y=1,color='k',linestyle=\"dotted\",linewidth=2)\nax.axvline(x=1,color='k',linestyle=\"dotted\",linewidth=2)\n# ax.set_xticks([1,3,5,7,9])\nxmin,xmax = ax.get_xlim()\nymin,ymax = ax.get_ylim()\nax.set_ylim([ymin,ymax])\nax.set_xscale(\"log\",base=10)\nax.set_xticks([.01,.1,1]); ax.set_xticklabels([.01,.1,1])\ncurrxmin,currxmax = ax.get_xlim()\nax.set_xlim([currxmin,1.25])\ncurrymin,currymax = ax.get_ylim()\n# ax.set_ylim([currymin,1.025])\n# ax.fill_between(np.array([1,xmax]),np.ones(1),ymax,color=COLORS[1],alpha=.25)\n# ax.fill_between(np.array([xmin,1]),np.ones(1),ymax,color=COLORS[4],alpha=.25)\n# ax.fill_between(np.array([1,xmax]),ymin,np.ones(1),color=COLORS[4],alpha=.25)\n# ax.fill_between(np.array([xmin,1]),ymin,np.ones(1),color=COLORS[3],alpha=.25)\n# fig.legend(frameon=False,bbox_to_anchor=(.9,1.15))\n# fig.suptitle(\"Standard MTGP vs Proposed Fast MTGP\")\nfig.tight_layout()\nfig.savefig(\"./elliptic_viz.pdf\",bbox_inches=\"tight\")\n</pre> fig,axs = pyplot.subplots(nrows=1,ncols=2,figsize=(PW/2,PW/2/2)) ax = axs[0] x_l0 = qp.DigitalNetB2(2,seed=7)(2**5) x_l1 = qp.DigitalNetB2(2,seed=11)(2**3)  ax.scatter(x_l0[:,0],x_l0[:,1],label=\"source\",s=250,color=\"red\",marker=\"o\") ax.scatter(x_l1[:,0],x_l1[:,1],label=\"target\",s=250,color=\"blue\",marker=\"^\") # ax.set_xlim([0,1]) ax.set_xticks([0,1/4,1/2,3/4,1]); ax.set_xticklabels([r\"$0$\",r\"$1/4$\",r\"$1/2$\",r\"$3/4$\",r\"$1$\"]) # ax.set_ylim([0,1]) ax.set_yticks([0,1/4,1/2,3/4,1]); ax.set_yticklabels([r\"$0$\",r\"$1/4$\",r\"$1/2$\",r\"$3/4$\",r\"$1$\"]) fig.legend(frameon=False,bbox_to_anchor=(.47,1.1),ncol=3) ax = axs[1] for j in range(1,len(names)):     xtrend = np.median(times[j],axis=-1)/np.median(times[0],axis=-1)     ytrend = np.median(l2rerrors[j,:,-1],axis=-1)/np.median(l2rerrors[0,:,-1],axis=-1)     for k in range(len(ns)):         ax.scatter(xtrend[k],ytrend[k],color=\"k\",zorder=10)         ax.annotate(r\"$2^{%d}$\"%int(np.log2(ns[k,0])),(xtrend[k]*.8,ytrend[k]+.015)) # ax.set_xscale(\"log\",base=10) # ax.set_yscale(\"log\",base=10) ax.set_xlabel(\"computation time ratio\") ax.set_ylabel(\"prediction error ratio\") ax.axhline(y=1,color='k',linestyle=\"dotted\",linewidth=2) ax.axvline(x=1,color='k',linestyle=\"dotted\",linewidth=2) # ax.set_xticks([1,3,5,7,9]) xmin,xmax = ax.get_xlim() ymin,ymax = ax.get_ylim() ax.set_ylim([ymin,ymax]) ax.set_xscale(\"log\",base=10) ax.set_xticks([.01,.1,1]); ax.set_xticklabels([.01,.1,1]) currxmin,currxmax = ax.get_xlim() ax.set_xlim([currxmin,1.25]) currymin,currymax = ax.get_ylim() # ax.set_ylim([currymin,1.025]) # ax.fill_between(np.array([1,xmax]),np.ones(1),ymax,color=COLORS[1],alpha=.25) # ax.fill_between(np.array([xmin,1]),np.ones(1),ymax,color=COLORS[4],alpha=.25) # ax.fill_between(np.array([1,xmax]),ymin,np.ones(1),color=COLORS[4],alpha=.25) # ax.fill_between(np.array([xmin,1]),ymin,np.ones(1),color=COLORS[3],alpha=.25) # fig.legend(frameon=False,bbox_to_anchor=(.9,1.15)) # fig.suptitle(\"Standard MTGP vs Proposed Fast MTGP\") fig.tight_layout() fig.savefig(\"./elliptic_viz.pdf\",bbox_inches=\"tight\") In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/multitask/elliptic_pde/#elliptic-pde-example","title":"Elliptic PDE Example\u00b6","text":""},{"location":"examples/multitask/fgp_dnb2/","title":"Fast GP Net","text":"In\u00a0[1]: Copied! <pre>import fastgps\nimport qmcpy as qp\nimport torch\nimport numpy as np\n</pre> import fastgps import qmcpy as qp import torch import numpy as np In\u00a0[2]: Copied! <pre>device = \"cpu\"\nif device!=\"mps\":\n    torch.set_default_dtype(torch.float64)\n</pre> device = \"cpu\" if device!=\"mps\":     torch.set_default_dtype(torch.float64) In\u00a0[3]: Copied! <pre>def f_ackley(x, a=20, b=0.2, c=2*np.pi, scaling=32.768):\n    # https://www.sfu.ca/~ssurjano/ackley.html\n    assert x.ndim==2\n    x = 2*scaling*x-scaling\n    t1 = a*torch.exp(-b*torch.sqrt(torch.mean(x**2,1)))\n    t2 = torch.exp(torch.mean(torch.cos(c*x),1))\n    t3 = a+np.exp(1)\n    y = -t1-t2+t3\n    return y\nf_low_fidelity = lambda x: f_ackley(x,c=0)\nf_high_fidelity = lambda x: f_ackley(x)\nf_cos = lambda x: torch.cos(2*np.pi*x).sum(1)\nfs = [f_low_fidelity,f_high_fidelity,f_cos]\nd = 1 # dimension\nrng = torch.Generator().manual_seed(17)\nx = torch.rand((2**7,d),generator=rng).to(device) # random testing locations\ny = torch.vstack([f(x) for f in fs]) # true values at random testing locations\nz = torch.rand((2**8,d),generator=rng).to(device) # other random locations at which to evaluate covariance\nprint(\"x.shape = %s\"%str(tuple(x.shape)))\nprint(\"y.shape = %s\"%str(tuple(y.shape)))\nprint(\"z.shape = %s\"%str(tuple(z.shape)))\n</pre> def f_ackley(x, a=20, b=0.2, c=2*np.pi, scaling=32.768):     # https://www.sfu.ca/~ssurjano/ackley.html     assert x.ndim==2     x = 2*scaling*x-scaling     t1 = a*torch.exp(-b*torch.sqrt(torch.mean(x**2,1)))     t2 = torch.exp(torch.mean(torch.cos(c*x),1))     t3 = a+np.exp(1)     y = -t1-t2+t3     return y f_low_fidelity = lambda x: f_ackley(x,c=0) f_high_fidelity = lambda x: f_ackley(x) f_cos = lambda x: torch.cos(2*np.pi*x).sum(1) fs = [f_low_fidelity,f_high_fidelity,f_cos] d = 1 # dimension rng = torch.Generator().manual_seed(17) x = torch.rand((2**7,d),generator=rng).to(device) # random testing locations y = torch.vstack([f(x) for f in fs]) # true values at random testing locations z = torch.rand((2**8,d),generator=rng).to(device) # other random locations at which to evaluate covariance print(\"x.shape = %s\"%str(tuple(x.shape))) print(\"y.shape = %s\"%str(tuple(y.shape))) print(\"z.shape = %s\"%str(tuple(z.shape))) <pre>x.shape = (128, 1)\ny.shape = (3, 128)\nz.shape = (256, 1)\n</pre> In\u00a0[4]: Copied! <pre>fgp = fastgps.FastGPDigitalNetB2(\n    qp.KernelMultiTask(\n        qp.KernelDigShiftInvar(d,torchify=True,device=device),\n        num_tasks=len(fs)),\n    seqs=7)\nx_next = fgp.get_x_next(n=[2**6,2**3,2**8])\ny_next = [fs[i](x_next[i]) for i in range(fgp.num_tasks)]\nfgp.add_y_next(y_next)\nassert len(x_next)==len(y_next)\nfor i in range(len(x_next)):\n    print(\"i = %d\"%i)\n    print(\"\\tx_next[%d].shape = %s\"%(i,str(tuple(x_next[i].shape))))\n    print(\"\\ty_next[%d].shape = %s\"%(i,str(tuple(y_next[i].shape))))\n</pre> fgp = fastgps.FastGPDigitalNetB2(     qp.KernelMultiTask(         qp.KernelDigShiftInvar(d,torchify=True,device=device),         num_tasks=len(fs)),     seqs=7) x_next = fgp.get_x_next(n=[2**6,2**3,2**8]) y_next = [fs[i](x_next[i]) for i in range(fgp.num_tasks)] fgp.add_y_next(y_next) assert len(x_next)==len(y_next) for i in range(len(x_next)):     print(\"i = %d\"%i)     print(\"\\tx_next[%d].shape = %s\"%(i,str(tuple(x_next[i].shape))))     print(\"\\ty_next[%d].shape = %s\"%(i,str(tuple(y_next[i].shape)))) <pre>i = 0\n\tx_next[0].shape = (64, 1)\n\ty_next[0].shape = (64,)\ni = 1\n\tx_next[1].shape = (8, 1)\n\ty_next[1].shape = (8,)\ni = 2\n\tx_next[2].shape = (256, 1)\n\ty_next[2].shape = (256,)\n</pre> In\u00a0[5]: Copied! <pre>pmean = fgp.post_mean(x)\nprint(\"pmean.shape = %s\"%str(tuple(pmean.shape)))\nprint(\"l2 relative error =\",(torch.linalg.norm(y-pmean,dim=1)/torch.linalg.norm(y,dim=1)))\n</pre> pmean = fgp.post_mean(x) print(\"pmean.shape = %s\"%str(tuple(pmean.shape))) print(\"l2 relative error =\",(torch.linalg.norm(y-pmean,dim=1)/torch.linalg.norm(y,dim=1))) <pre>pmean.shape = (3, 128)\nl2 relative error = tensor([0.0413, 0.1572, 0.0591])\n</pre> In\u00a0[6]: Copied! <pre>data = fgp.fit()\nlist(data.keys())\n</pre> data = fgp.fit() list(data.keys()) <pre>     iter of 5.0e+03 | best loss  | loss      \n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n            0.00e+00 | 3.17e+02   | 3.17e+02  \n            5.00e+00 | 1.59e+01   | 1.59e+01  \n            1.00e+01 | -2.92e+02  | -2.92e+02 \n            1.50e+01 | -2.92e+02  | -2.88e+02 \n            2.00e+01 | -2.96e+02  | -2.96e+02 \n            2.50e+01 | -2.99e+02  | -2.99e+02 \n            3.00e+01 | -3.02e+02  | -3.02e+02 \n            3.50e+01 | -3.02e+02  | -3.02e+02 \n            4.00e+01 | -3.02e+02  | -3.02e+02 \n            4.50e+01 | -3.02e+02  | -3.02e+02 \n            4.80e+01 | -3.02e+02  | -3.02e+02 \n</pre> Out[6]: <pre>[]</pre> In\u00a0[7]: Copied! <pre>pmean,pvar,q,ci_low,ci_high = fgp.post_ci(x,confidence=0.99)\nprint(\"pmean.shape = %s\"%str(tuple(pmean.shape)))\nprint(\"pvar.shape = %s\"%str(tuple(pvar.shape)))\nprint(\"q = %.2f\"%q)\nprint(\"ci_low.shape = %s\"%str(tuple(ci_low.shape)))\nprint(\"ci_high.shape = %s\"%str(tuple(ci_high.shape)))\nprint(\"l2 relative error =\",(torch.linalg.norm(y-pmean,dim=1)/torch.linalg.norm(y,dim=1)))\npcov = fgp.post_cov(x,x)\nprint(\"pcov.shape = %s\"%str(tuple(pcov.shape)))\n_range0,_rangen1 = torch.arange(pcov.size(0)),torch.arange(pcov.size(-1))\npcov2 = fgp.post_cov(x,z)\nprint(\"pcov2.shape = %s\"%str(tuple(pcov2.shape)))\nprint(\"\\npcov diag matches pvar: %s\"%torch.allclose(pcov[_range0,_range0][:,_rangen1,_rangen1],pvar))\nprint(\"non-negative pvar: %s\"%(pvar&gt;=0).all().item())\n</pre> pmean,pvar,q,ci_low,ci_high = fgp.post_ci(x,confidence=0.99) print(\"pmean.shape = %s\"%str(tuple(pmean.shape))) print(\"pvar.shape = %s\"%str(tuple(pvar.shape))) print(\"q = %.2f\"%q) print(\"ci_low.shape = %s\"%str(tuple(ci_low.shape))) print(\"ci_high.shape = %s\"%str(tuple(ci_high.shape))) print(\"l2 relative error =\",(torch.linalg.norm(y-pmean,dim=1)/torch.linalg.norm(y,dim=1))) pcov = fgp.post_cov(x,x) print(\"pcov.shape = %s\"%str(tuple(pcov.shape))) _range0,_rangen1 = torch.arange(pcov.size(0)),torch.arange(pcov.size(-1)) pcov2 = fgp.post_cov(x,z) print(\"pcov2.shape = %s\"%str(tuple(pcov2.shape))) print(\"\\npcov diag matches pvar: %s\"%torch.allclose(pcov[_range0,_range0][:,_rangen1,_rangen1],pvar)) print(\"non-negative pvar: %s\"%(pvar&gt;=0).all().item()) <pre>pmean.shape = (3, 128)\npvar.shape = (3, 128)\nq = 2.58\nci_low.shape = (3, 128)\nci_high.shape = (3, 128)\nl2 relative error = tensor([0.0429, 0.0753, 0.0121])\npcov.shape = (3, 3, 128, 128)\npcov2.shape = (3, 3, 128, 256)\n\npcov diag matches pvar: True\nnon-negative pvar: True\n</pre> In\u00a0[8]: Copied! <pre>pcmean,pcvar,q,cci_low,cci_high = fgp.post_cubature_ci(confidence=0.99)\nprint(\"pcmean =\",pcmean)\nprint(\"pcvar =\",pcvar)\nprint(\"cci_low =\",cci_low)\nprint(\"cci_high\",cci_high)\n</pre> pcmean,pcvar,q,cci_low,cci_high = fgp.post_cubature_ci(confidence=0.99) print(\"pcmean =\",pcmean) print(\"pcvar =\",pcvar) print(\"cci_low =\",cci_low) print(\"cci_high\",cci_high) <pre>pcmean = tensor([1.6943e+01, 1.8305e+01, 7.2164e-16])\npcvar = tensor([8.6268e-02, 5.6407e-02, 9.3929e-05])\ncci_low = tensor([16.1866, 17.6936, -0.0250])\ncci_high tensor([17.6997, 18.9171,  0.0250])\n</pre> In\u00a0[9]: Copied! <pre>n_new = fgp.n*torch.tensor([4,2,8],device=device)\npcov_future = fgp.post_cov(x,z,n=n_new)\npvar_future = fgp.post_var(x,n=n_new)\npcvar_future = fgp.post_cubature_var(n=n_new)\n</pre> n_new = fgp.n*torch.tensor([4,2,8],device=device) pcov_future = fgp.post_cov(x,z,n=n_new) pvar_future = fgp.post_var(x,n=n_new) pcvar_future = fgp.post_cubature_var(n=n_new) In\u00a0[10]: Copied! <pre>x_next = fgp.get_x_next(n_new)\ny_next = [fs[i](x_next[i]) for i in range(fgp.num_tasks)]\nfor _y in y_next:\n    print(_y.shape)\nfgp.add_y_next(y_next)\nprint(\"l2 relative error =\",(torch.linalg.norm(y-fgp.post_mean(x),dim=1)/torch.linalg.norm(y,dim=1)))\nassert torch.allclose(fgp.post_cov(x,z),pcov_future)\nassert torch.allclose(fgp.post_var(x),pvar_future)\nassert torch.allclose(fgp.post_cubature_var(),pcvar_future)\n</pre> x_next = fgp.get_x_next(n_new) y_next = [fs[i](x_next[i]) for i in range(fgp.num_tasks)] for _y in y_next:     print(_y.shape) fgp.add_y_next(y_next) print(\"l2 relative error =\",(torch.linalg.norm(y-fgp.post_mean(x),dim=1)/torch.linalg.norm(y,dim=1))) assert torch.allclose(fgp.post_cov(x,z),pcov_future) assert torch.allclose(fgp.post_var(x),pvar_future) assert torch.allclose(fgp.post_cubature_var(),pcvar_future) <pre>torch.Size([192])\ntorch.Size([8])\ntorch.Size([1792])\nl2 relative error = tensor([0.0193, 0.0640, 0.0016])\n</pre> In\u00a0[11]: Copied! <pre>data = fgp.fit(verbose=False)\nprint(\"l2 relative error =\",(torch.linalg.norm(y-fgp.post_mean(x),dim=1)/torch.linalg.norm(y,dim=1)))\n</pre> data = fgp.fit(verbose=False) print(\"l2 relative error =\",(torch.linalg.norm(y-fgp.post_mean(x),dim=1)/torch.linalg.norm(y,dim=1))) <pre>l2 relative error = tensor([0.0110, 0.0592, 0.0014])\n</pre> In\u00a0[12]: Copied! <pre>n_new = fgp.n*torch.tensor([4,8,2],device=device)\npcov_new = fgp.post_cov(x,z,n=n_new)\npvar_new = fgp.post_var(x,n=n_new)\npcvar_new = fgp.post_cubature_var(n=n_new)\nx_next = fgp.get_x_next(n_new)\ny_next = [fs[i](x_next[i]) for i in range(fgp.num_tasks)]\nfgp.add_y_next(y_next)\nassert torch.allclose(fgp.post_cov(x,z),pcov_new)\nassert torch.allclose(fgp.post_var(x),pvar_new)\nassert torch.allclose(fgp.post_cubature_var(),pcvar_new)\n</pre> n_new = fgp.n*torch.tensor([4,8,2],device=device) pcov_new = fgp.post_cov(x,z,n=n_new) pvar_new = fgp.post_var(x,n=n_new) pcvar_new = fgp.post_cubature_var(n=n_new) x_next = fgp.get_x_next(n_new) y_next = [fs[i](x_next[i]) for i in range(fgp.num_tasks)] fgp.add_y_next(y_next) assert torch.allclose(fgp.post_cov(x,z),pcov_new) assert torch.allclose(fgp.post_var(x),pvar_new) assert torch.allclose(fgp.post_cubature_var(),pcvar_new) In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/multitask/fgp_dnb2/#fast-multitask-net-gp","title":"Fast Multitask Net GP\u00b6","text":""},{"location":"examples/multitask/fgp_dnb2/#true-function","title":"True Function\u00b6","text":""},{"location":"examples/multitask/fgp_dnb2/#construct-fast-gp","title":"Construct Fast GP\u00b6","text":""},{"location":"examples/multitask/fgp_dnb2/#project-and-increase-sample-size","title":"Project and Increase Sample Size\u00b6","text":""},{"location":"examples/multitask/fgp_lattice/","title":"Fast GP Lattice","text":"In\u00a0[1]: Copied! <pre>import fastgps\nimport qmcpy as qp\nimport torch\nimport numpy as np\n</pre> import fastgps import qmcpy as qp import torch import numpy as np In\u00a0[2]: Copied! <pre>device = \"cpu\"\nif device!=\"mps\":\n    torch.set_default_dtype(torch.float64)\n</pre> device = \"cpu\" if device!=\"mps\":     torch.set_default_dtype(torch.float64) In\u00a0[3]: Copied! <pre>def f_ackley(x, a=20, b=0.2, c=2*np.pi, scaling=32.768):\n    # https://www.sfu.ca/~ssurjano/ackley.html\n    assert x.ndim==2\n    x = 2*scaling*x-scaling\n    t1 = a*torch.exp(-b*torch.sqrt(torch.mean(x**2,1)))\n    t2 = torch.exp(torch.mean(torch.cos(c*x),1))\n    t3 = a+np.exp(1)\n    y = -t1-t2+t3\n    return y\nf_low_fidelity = lambda x: f_ackley(x,c=0)\nf_high_fidelity = lambda x: f_ackley(x)\nf_cos = lambda x: torch.cos(2*np.pi*x).sum(1)\nfs = [f_low_fidelity,f_high_fidelity,f_cos]\nd = 1 # dimension\nrng = torch.Generator().manual_seed(17)\nx = torch.rand((2**7,d),generator=rng).to(device) # random testing locations\ny = torch.vstack([f(x) for f in fs]) # true values at random testing locations\nz = torch.rand((2**8,d),generator=rng).to(device) # other random locations at which to evaluate covariance\nprint(\"x.shape = %s\"%str(tuple(x.shape)))\nprint(\"y.shape = %s\"%str(tuple(y.shape)))\nprint(\"z.shape = %s\"%str(tuple(z.shape)))\n</pre> def f_ackley(x, a=20, b=0.2, c=2*np.pi, scaling=32.768):     # https://www.sfu.ca/~ssurjano/ackley.html     assert x.ndim==2     x = 2*scaling*x-scaling     t1 = a*torch.exp(-b*torch.sqrt(torch.mean(x**2,1)))     t2 = torch.exp(torch.mean(torch.cos(c*x),1))     t3 = a+np.exp(1)     y = -t1-t2+t3     return y f_low_fidelity = lambda x: f_ackley(x,c=0) f_high_fidelity = lambda x: f_ackley(x) f_cos = lambda x: torch.cos(2*np.pi*x).sum(1) fs = [f_low_fidelity,f_high_fidelity,f_cos] d = 1 # dimension rng = torch.Generator().manual_seed(17) x = torch.rand((2**7,d),generator=rng).to(device) # random testing locations y = torch.vstack([f(x) for f in fs]) # true values at random testing locations z = torch.rand((2**8,d),generator=rng).to(device) # other random locations at which to evaluate covariance print(\"x.shape = %s\"%str(tuple(x.shape))) print(\"y.shape = %s\"%str(tuple(y.shape))) print(\"z.shape = %s\"%str(tuple(z.shape))) <pre>x.shape = (128, 1)\ny.shape = (3, 128)\nz.shape = (256, 1)\n</pre> In\u00a0[4]: Copied! <pre>fgp = fastgps.FastGPLattice(\n    qp.KernelMultiTask(\n        qp.KernelShiftInvar(d,torchify=True,device=device),\n        num_tasks=len(fs)),\n    seqs=7)\nx_next = fgp.get_x_next(n=[2**6,2**3,2**8])\ny_next = [fs[i](x_next[i]) for i in range(fgp.num_tasks)]\nfgp.add_y_next(y_next)\nassert len(x_next)==len(y_next)\nfor i in range(len(x_next)):\n    print(\"i = %d\"%i)\n    print(\"\\tx_next[%d].shape = %s\"%(i,str(tuple(x_next[i].shape))))\n    print(\"\\ty_next[%d].shape = %s\"%(i,str(tuple(y_next[i].shape))))\n</pre> fgp = fastgps.FastGPLattice(     qp.KernelMultiTask(         qp.KernelShiftInvar(d,torchify=True,device=device),         num_tasks=len(fs)),     seqs=7) x_next = fgp.get_x_next(n=[2**6,2**3,2**8]) y_next = [fs[i](x_next[i]) for i in range(fgp.num_tasks)] fgp.add_y_next(y_next) assert len(x_next)==len(y_next) for i in range(len(x_next)):     print(\"i = %d\"%i)     print(\"\\tx_next[%d].shape = %s\"%(i,str(tuple(x_next[i].shape))))     print(\"\\ty_next[%d].shape = %s\"%(i,str(tuple(y_next[i].shape)))) <pre>i = 0\n\tx_next[0].shape = (64, 1)\n\ty_next[0].shape = (64,)\ni = 1\n\tx_next[1].shape = (8, 1)\n\ty_next[1].shape = (8,)\ni = 2\n\tx_next[2].shape = (256, 1)\n\ty_next[2].shape = (256,)\n</pre> In\u00a0[5]: Copied! <pre>pmean = fgp.post_mean(x)\nprint(\"pmean.shape = %s\"%str(tuple(pmean.shape)))\nprint(\"l2 relative error =\",(torch.linalg.norm(y-pmean,dim=1)/torch.linalg.norm(y,dim=1)))\n</pre> pmean = fgp.post_mean(x) print(\"pmean.shape = %s\"%str(tuple(pmean.shape))) print(\"l2 relative error =\",(torch.linalg.norm(y-pmean,dim=1)/torch.linalg.norm(y,dim=1))) <pre>pmean.shape = (3, 128)\nl2 relative error = tensor([0.0046, 0.0692, 0.0003])\n</pre> In\u00a0[6]: Copied! <pre>data = fgp.fit()\nlist(data.keys())\n</pre> data = fgp.fit() list(data.keys()) <pre>     iter of 5.0e+03 | best loss  | loss      \n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n            0.00e+00 | 4.67e+03   | 4.67e+03  \n            5.00e+00 | -3.81e+02  | -3.81e+02 \n            1.00e+01 | -6.73e+02  | -6.73e+02 \n            1.50e+01 | -1.20e+03  | -1.20e+03 \n            2.00e+01 | -1.55e+03  | -1.55e+03 \n            2.50e+01 | -1.57e+03  | -1.57e+03 \n            3.00e+01 | -1.58e+03  | -1.58e+03 \n            3.50e+01 | -1.58e+03  | -1.58e+03 \n            4.00e+01 | -1.58e+03  | -1.58e+03 \n            4.50e+01 | -1.58e+03  | -1.58e+03 \n            5.00e+01 | -1.58e+03  | -1.58e+03 \n            5.50e+01 | -1.58e+03  | -1.58e+03 \n            6.00e+01 | -1.58e+03  | -1.58e+03 \n            6.50e+01 | -1.58e+03  | -1.58e+03 \n            7.00e+01 | -1.59e+03  | -1.59e+03 \n            7.50e+01 | -1.59e+03  | -1.59e+03 \n            8.00e+01 | -1.59e+03  | -1.59e+03 \n            8.50e+01 | -1.59e+03  | -1.59e+03 \n            9.00e+01 | -1.59e+03  | -1.59e+03 \n            9.50e+01 | -1.59e+03  | -1.59e+03 \n            9.60e+01 | -1.59e+03  | -1.59e+03 \n</pre> Out[6]: <pre>[]</pre> In\u00a0[7]: Copied! <pre>pmean,pvar,q,ci_low,ci_high = fgp.post_ci(x,confidence=0.99)\nprint(\"pmean.shape = %s\"%str(tuple(pmean.shape)))\nprint(\"pvar.shape = %s\"%str(tuple(pvar.shape)))\nprint(\"q = %.2f\"%q)\nprint(\"ci_low.shape = %s\"%str(tuple(ci_low.shape)))\nprint(\"ci_high.shape = %s\"%str(tuple(ci_high.shape)))\nprint(\"l2 relative error =\",(torch.linalg.norm(y-pmean,dim=1)/torch.linalg.norm(y,dim=1)))\npcov = fgp.post_cov(x,x)\nprint(\"pcov.shape = %s\"%str(tuple(pcov.shape)))\n_range0,_rangen1 = torch.arange(pcov.size(0)),torch.arange(pcov.size(-1))\npcov2 = fgp.post_cov(x,z)\nprint(\"pcov2.shape = %s\"%str(tuple(pcov2.shape)))\nprint(\"\\npcov diag matches pvar: %s\"%torch.allclose(pcov[_range0,_range0][:,_rangen1,_rangen1],pvar))\nprint(\"non-negative pvar: %s\"%(pvar&gt;=0).all().item())\n</pre> pmean,pvar,q,ci_low,ci_high = fgp.post_ci(x,confidence=0.99) print(\"pmean.shape = %s\"%str(tuple(pmean.shape))) print(\"pvar.shape = %s\"%str(tuple(pvar.shape))) print(\"q = %.2f\"%q) print(\"ci_low.shape = %s\"%str(tuple(ci_low.shape))) print(\"ci_high.shape = %s\"%str(tuple(ci_high.shape))) print(\"l2 relative error =\",(torch.linalg.norm(y-pmean,dim=1)/torch.linalg.norm(y,dim=1))) pcov = fgp.post_cov(x,x) print(\"pcov.shape = %s\"%str(tuple(pcov.shape))) _range0,_rangen1 = torch.arange(pcov.size(0)),torch.arange(pcov.size(-1)) pcov2 = fgp.post_cov(x,z) print(\"pcov2.shape = %s\"%str(tuple(pcov2.shape))) print(\"\\npcov diag matches pvar: %s\"%torch.allclose(pcov[_range0,_range0][:,_rangen1,_rangen1],pvar)) print(\"non-negative pvar: %s\"%(pvar&gt;=0).all().item()) <pre>pmean.shape = (3, 128)\npvar.shape = (3, 128)\nq = 2.58\nci_low.shape = (3, 128)\nci_high.shape = (3, 128)\nl2 relative error = tensor([4.6781e-03, 5.7351e-02, 1.2322e-08])\npcov.shape = (3, 3, 128, 128)\npcov2.shape = (3, 3, 128, 256)\n\npcov diag matches pvar: True\nnon-negative pvar: True\n</pre> In\u00a0[8]: Copied! <pre>pcmean,pcvar,q,cci_low,cci_high = fgp.post_cubature_ci(confidence=0.99)\nprint(\"pcmean =\",pcmean)\nprint(\"pcvar =\",pcvar)\nprint(\"cci_low =\",cci_low)\nprint(\"cci_high\",cci_high)\n</pre> pcmean,pcvar,q,cci_low,cci_high = fgp.post_cubature_ci(confidence=0.99) print(\"pcmean =\",pcmean) print(\"pcvar =\",pcvar) print(\"cci_low =\",cci_low) print(\"cci_high\",cci_high) <pre>pcmean = tensor([1.6943e+01, 1.8141e+01, 2.7756e-17])\npcvar = tensor([3.4645e-05, 2.7259e-03, 9.8626e-13])\ncci_low = tensor([ 1.6928e+01,  1.8007e+01, -2.5581e-06])\ncci_high tensor([1.6958e+01, 1.8276e+01, 2.5581e-06])\n</pre> In\u00a0[9]: Copied! <pre>n_new = fgp.n*torch.tensor([4,2,8],device=device)\npcov_future = fgp.post_cov(x,z,n=n_new)\npvar_future = fgp.post_var(x,n=n_new)\npcvar_future = fgp.post_cubature_var(n=n_new)\n</pre> n_new = fgp.n*torch.tensor([4,2,8],device=device) pcov_future = fgp.post_cov(x,z,n=n_new) pvar_future = fgp.post_var(x,n=n_new) pcvar_future = fgp.post_cubature_var(n=n_new) In\u00a0[10]: Copied! <pre>x_next = fgp.get_x_next(n_new)\ny_next = [fs[i](x_next[i]) for i in range(fgp.num_tasks)]\nfor _y in y_next:\n    print(_y.shape)\nfgp.add_y_next(y_next)\nprint(\"l2 relative error =\",(torch.linalg.norm(y-fgp.post_mean(x),dim=1)/torch.linalg.norm(y,dim=1)))\nassert torch.allclose(fgp.post_cov(x,z),pcov_future)\nassert torch.allclose(fgp.post_var(x),pvar_future)\nassert torch.allclose(fgp.post_cubature_var(),pcvar_future)\n</pre> x_next = fgp.get_x_next(n_new) y_next = [fs[i](x_next[i]) for i in range(fgp.num_tasks)] for _y in y_next:     print(_y.shape) fgp.add_y_next(y_next) print(\"l2 relative error =\",(torch.linalg.norm(y-fgp.post_mean(x),dim=1)/torch.linalg.norm(y,dim=1))) assert torch.allclose(fgp.post_cov(x,z),pcov_future) assert torch.allclose(fgp.post_var(x),pvar_future) assert torch.allclose(fgp.post_cubature_var(),pcvar_future) <pre>torch.Size([192])\ntorch.Size([8])\ntorch.Size([1792])\nl2 relative error = tensor([3.4560e-04, 5.9656e-02, 4.3859e-11])\n</pre> In\u00a0[11]: Copied! <pre>data = fgp.fit(verbose=False)\nprint(\"l2 relative error =\",(torch.linalg.norm(y-fgp.post_mean(x),dim=1)/torch.linalg.norm(y,dim=1)))\n</pre> data = fgp.fit(verbose=False) print(\"l2 relative error =\",(torch.linalg.norm(y-fgp.post_mean(x),dim=1)/torch.linalg.norm(y,dim=1))) <pre>l2 relative error = tensor([3.4559e-04, 5.9690e-02, 6.0888e-12])\n</pre> In\u00a0[12]: Copied! <pre>n_new = fgp.n*torch.tensor([4,8,2],device=device)\npcov_new = fgp.post_cov(x,z,n=n_new)\npvar_new = fgp.post_var(x,n=n_new)\npcvar_new = fgp.post_cubature_var(n=n_new)\nx_next = fgp.get_x_next(n_new)\ny_next = [fs[i](x_next[i]) for i in range(fgp.num_tasks)]\nfgp.add_y_next(y_next)\nassert torch.allclose(fgp.post_cov(x,z),pcov_new)\nassert torch.allclose(fgp.post_var(x),pvar_new)\nassert torch.allclose(fgp.post_cubature_var(),pcvar_new)\n</pre> n_new = fgp.n*torch.tensor([4,8,2],device=device) pcov_new = fgp.post_cov(x,z,n=n_new) pvar_new = fgp.post_var(x,n=n_new) pcvar_new = fgp.post_cubature_var(n=n_new) x_next = fgp.get_x_next(n_new) y_next = [fs[i](x_next[i]) for i in range(fgp.num_tasks)] fgp.add_y_next(y_next) assert torch.allclose(fgp.post_cov(x,z),pcov_new) assert torch.allclose(fgp.post_var(x),pvar_new) assert torch.allclose(fgp.post_cubature_var(),pcvar_new) In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/multitask/fgp_lattice/#fast-multitask-lattice-gp","title":"Fast Multitask Lattice GP\u00b6","text":""},{"location":"examples/multitask/fgp_lattice/#true-function","title":"True Function\u00b6","text":""},{"location":"examples/multitask/fgp_lattice/#construct-fast-gp","title":"Construct Fast GP\u00b6","text":""},{"location":"examples/multitask/fgp_lattice/#project-and-increase-sample-size","title":"Project and Increase Sample Size\u00b6","text":""},{"location":"examples/multitask/standard_gp/","title":"Standard GP","text":"In\u00a0[1]: Copied! <pre>import fastgps\nimport qmcpy as qp\nimport torch\nimport numpy as np\n</pre> import fastgps import qmcpy as qp import torch import numpy as np In\u00a0[2]: Copied! <pre>device = \"cpu\"\nif device!=\"mps\":\n    torch.set_default_dtype(torch.float64)\n</pre> device = \"cpu\" if device!=\"mps\":     torch.set_default_dtype(torch.float64) In\u00a0[3]: Copied! <pre>def f_ackley(x, a=20, b=0.2, c=2*np.pi, scaling=32.768):\n    # https://www.sfu.ca/~ssurjano/ackley.html\n    assert x.ndim==2\n    x = 2*scaling*x-scaling\n    t1 = a*torch.exp(-b*torch.sqrt(torch.mean(x**2,1)))\n    t2 = torch.exp(torch.mean(torch.cos(c*x),1))\n    t3 = a+np.exp(1)\n    y = -t1-t2+t3\n    return y\nf_low_fidelity = lambda x: f_ackley(x,c=0)\nf_high_fidelity = lambda x: f_ackley(x)\nf_cos = lambda x: torch.cos(2*np.pi*x).sum(1)\nfs = [f_low_fidelity,f_high_fidelity,f_cos]\nd = 1 # dimension\nrng = torch.Generator().manual_seed(17)\nx = torch.rand((2**7,d),generator=rng).to(device) # random testing locations\ny = torch.vstack([f(x) for f in fs]) # true values at random testing locations\nz = torch.rand((2**8,d),generator=rng).to(device) # other random locations at which to evaluate covariance\nprint(\"x.shape = %s\"%str(tuple(x.shape)))\nprint(\"y.shape = %s\"%str(tuple(y.shape)))\nprint(\"z.shape = %s\"%str(tuple(z.shape)))\n</pre> def f_ackley(x, a=20, b=0.2, c=2*np.pi, scaling=32.768):     # https://www.sfu.ca/~ssurjano/ackley.html     assert x.ndim==2     x = 2*scaling*x-scaling     t1 = a*torch.exp(-b*torch.sqrt(torch.mean(x**2,1)))     t2 = torch.exp(torch.mean(torch.cos(c*x),1))     t3 = a+np.exp(1)     y = -t1-t2+t3     return y f_low_fidelity = lambda x: f_ackley(x,c=0) f_high_fidelity = lambda x: f_ackley(x) f_cos = lambda x: torch.cos(2*np.pi*x).sum(1) fs = [f_low_fidelity,f_high_fidelity,f_cos] d = 1 # dimension rng = torch.Generator().manual_seed(17) x = torch.rand((2**7,d),generator=rng).to(device) # random testing locations y = torch.vstack([f(x) for f in fs]) # true values at random testing locations z = torch.rand((2**8,d),generator=rng).to(device) # other random locations at which to evaluate covariance print(\"x.shape = %s\"%str(tuple(x.shape))) print(\"y.shape = %s\"%str(tuple(y.shape))) print(\"z.shape = %s\"%str(tuple(z.shape))) <pre>x.shape = (128, 1)\ny.shape = (3, 128)\nz.shape = (256, 1)\n</pre> In\u00a0[4]: Copied! <pre>fgp = fastgps.StandardGP(\n    kernel = qp.KernelMultiTask(\n        qp.KernelSquaredExponential(d,torchify=True,device=device),\n        num_tasks=len(fs)),\n    seqs = 7)\nx_next = fgp.get_x_next(n=[2**4,2**2,2**3])\ny_next = [fs[i](x_next[i]) for i in range(fgp.num_tasks)]\nfgp.add_y_next(y_next)\nassert len(x_next)==len(y_next)\nfor i in range(len(x_next)):\n    print(\"i = %d\"%i)\n    print(\"\\tx_next[%d].shape = %s\"%(i,str(tuple(x_next[i].shape))))\n    print(\"\\ty_next[%d].shape = %s\"%(i,str(tuple(y_next[i].shape))))\n</pre> fgp = fastgps.StandardGP(     kernel = qp.KernelMultiTask(         qp.KernelSquaredExponential(d,torchify=True,device=device),         num_tasks=len(fs)),     seqs = 7) x_next = fgp.get_x_next(n=[2**4,2**2,2**3]) y_next = [fs[i](x_next[i]) for i in range(fgp.num_tasks)] fgp.add_y_next(y_next) assert len(x_next)==len(y_next) for i in range(len(x_next)):     print(\"i = %d\"%i)     print(\"\\tx_next[%d].shape = %s\"%(i,str(tuple(x_next[i].shape))))     print(\"\\ty_next[%d].shape = %s\"%(i,str(tuple(y_next[i].shape)))) <pre>i = 0\n\tx_next[0].shape = (16, 1)\n\ty_next[0].shape = (16,)\ni = 1\n\tx_next[1].shape = (4, 1)\n\ty_next[1].shape = (4,)\ni = 2\n\tx_next[2].shape = (8, 1)\n\ty_next[2].shape = (8,)\n</pre> In\u00a0[5]: Copied! <pre>pmean = fgp.post_mean(x)\nprint(\"pmean.shape = %s\"%str(tuple(pmean.shape)))\nprint(\"l2 relative error =\",(torch.linalg.norm(y-pmean,dim=1)/torch.linalg.norm(y,dim=1)))\n</pre> pmean = fgp.post_mean(x) print(\"pmean.shape = %s\"%str(tuple(pmean.shape))) print(\"l2 relative error =\",(torch.linalg.norm(y-pmean,dim=1)/torch.linalg.norm(y,dim=1))) <pre>pmean.shape = (3, 128)\nl2 relative error = tensor([0.1861, 0.2679, 0.0975])\n</pre> In\u00a0[6]: Copied! <pre>data = fgp.fit()\nlist(data.keys())\n</pre> data = fgp.fit() list(data.keys()) <pre>     iter of 5.0e+03 | best loss  | loss      \n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n            0.00e+00 | 6.91e+05   | 6.91e+05  \n            5.00e+00 | 2.97e+05   | 2.97e+05  \n            1.00e+01 | 8.64e+01   | 8.64e+01  \n            1.50e+01 | 7.87e+01   | 8.03e+01  \n            2.00e+01 | 6.65e+01   | 6.65e+01  \n            2.50e+01 | 5.07e+01   | 5.07e+01  \n            3.00e+01 | 4.96e+01   | 4.96e+01  \n            3.50e+01 | 4.87e+01   | 4.87e+01  \n            4.00e+01 | 4.76e+01   | 4.76e+01  \n            4.50e+01 | 4.73e+01   | 4.73e+01  \n            5.00e+01 | 4.73e+01   | 4.73e+01  \n            5.30e+01 | 4.73e+01   | 4.73e+01  \n</pre> Out[6]: <pre>[]</pre> In\u00a0[7]: Copied! <pre>pmean,pvar,q,ci_low,ci_high = fgp.post_ci(x,confidence=0.99)\nprint(\"pmean.shape = %s\"%str(tuple(pmean.shape)))\nprint(\"pvar.shape = %s\"%str(tuple(pvar.shape)))\nprint(\"q = %.2f\"%q)\nprint(\"ci_low.shape = %s\"%str(tuple(ci_low.shape)))\nprint(\"ci_high.shape = %s\"%str(tuple(ci_high.shape)))\nprint(\"l2 relative error =\",(torch.linalg.norm(y-pmean,dim=1)/torch.linalg.norm(y,dim=1)))\npcov = fgp.post_cov(x,x)\nprint(\"pcov.shape = %s\"%str(tuple(pcov.shape)))\n_range0,_rangen1 = torch.arange(pcov.size(0)),torch.arange(pcov.size(-1))\npcov2 = fgp.post_cov(x,z)\nprint(\"pcov2.shape = %s\"%str(tuple(pcov2.shape)))\nprint(\"\\npcov diag matches pvar: %s\"%torch.allclose(pcov[_range0,_range0][:,_rangen1,_rangen1],pvar))\nprint(\"non-negative pvar: %s\"%(pvar&gt;=0).all().item())\n</pre> pmean,pvar,q,ci_low,ci_high = fgp.post_ci(x,confidence=0.99) print(\"pmean.shape = %s\"%str(tuple(pmean.shape))) print(\"pvar.shape = %s\"%str(tuple(pvar.shape))) print(\"q = %.2f\"%q) print(\"ci_low.shape = %s\"%str(tuple(ci_low.shape))) print(\"ci_high.shape = %s\"%str(tuple(ci_high.shape))) print(\"l2 relative error =\",(torch.linalg.norm(y-pmean,dim=1)/torch.linalg.norm(y,dim=1))) pcov = fgp.post_cov(x,x) print(\"pcov.shape = %s\"%str(tuple(pcov.shape))) _range0,_rangen1 = torch.arange(pcov.size(0)),torch.arange(pcov.size(-1)) pcov2 = fgp.post_cov(x,z) print(\"pcov2.shape = %s\"%str(tuple(pcov2.shape))) print(\"\\npcov diag matches pvar: %s\"%torch.allclose(pcov[_range0,_range0][:,_rangen1,_rangen1],pvar)) print(\"non-negative pvar: %s\"%(pvar&gt;=0).all().item()) <pre>pmean.shape = (3, 128)\npvar.shape = (3, 128)\nq = 2.58\nci_low.shape = (3, 128)\nci_high.shape = (3, 128)\nl2 relative error = tensor([0.0315, 0.1709, 0.2248])\npcov.shape = (3, 3, 128, 128)\npcov2.shape = (3, 3, 128, 256)\n\npcov diag matches pvar: True\nnon-negative pvar: True\n</pre> In\u00a0[8]: Copied! <pre>pcmean,pcvar,q,cci_low,cci_high = fgp.post_cubature_ci(confidence=0.99)\nprint(\"pcmean =\",pcmean)\nprint(\"pcvar =\",pcvar)\nprint(\"cci_low =\",cci_low)\nprint(\"cci_high\",cci_high)\n</pre> pcmean,pcvar,q,cci_low,cci_high = fgp.post_cubature_ci(confidence=0.99) print(\"pcmean =\",pcmean) print(\"pcvar =\",pcvar) print(\"cci_low =\",cci_low) print(\"cci_high\",cci_high) <pre>pcmean = tensor([ 1.7030e+01,  1.9753e+01, -5.4638e-03])\npcvar = tensor([0.0007, 0.0464, 0.0016])\ncci_low = tensor([16.9613, 19.1977, -0.1090])\ncci_high tensor([17.0985, 20.3074,  0.0981])\n</pre> In\u00a0[9]: Copied! <pre>n_new = fgp.n*torch.tensor([4,2,8],device=device)\npcov_future = fgp.post_cov(x,z,n=n_new)\npvar_future = fgp.post_var(x,n=n_new)\npcvar_future = fgp.post_cubature_var(n=n_new)\n</pre> n_new = fgp.n*torch.tensor([4,2,8],device=device) pcov_future = fgp.post_cov(x,z,n=n_new) pvar_future = fgp.post_var(x,n=n_new) pcvar_future = fgp.post_cubature_var(n=n_new) In\u00a0[10]: Copied! <pre>x_next = fgp.get_x_next(n_new)\ny_next = [fs[i](x_next[i]) for i in range(fgp.num_tasks)]\nfor _y in y_next:\n    print(_y.shape)\nfgp.add_y_next(y_next)\nprint(\"l2 relative error =\",(torch.linalg.norm(y-fgp.post_mean(x),dim=1)/torch.linalg.norm(y,dim=1)))\nassert torch.allclose(fgp.post_cov(x,z),pcov_future)\nassert torch.allclose(fgp.post_var(x),pvar_future)\nassert torch.allclose(fgp.post_cubature_var(),pcvar_future)\n</pre> x_next = fgp.get_x_next(n_new) y_next = [fs[i](x_next[i]) for i in range(fgp.num_tasks)] for _y in y_next:     print(_y.shape) fgp.add_y_next(y_next) print(\"l2 relative error =\",(torch.linalg.norm(y-fgp.post_mean(x),dim=1)/torch.linalg.norm(y,dim=1))) assert torch.allclose(fgp.post_cov(x,z),pcov_future) assert torch.allclose(fgp.post_var(x),pvar_future) assert torch.allclose(fgp.post_cubature_var(),pcvar_future) <pre>torch.Size([48])\ntorch.Size([4])\ntorch.Size([56])\nl2 relative error = tensor([0.0243, 0.1789, 0.2022])\n</pre> In\u00a0[11]: Copied! <pre>data = fgp.fit(verbose=False)\nprint(\"l2 relative error =\",(torch.linalg.norm(y-fgp.post_mean(x),dim=1)/torch.linalg.norm(y,dim=1)))\n</pre> data = fgp.fit(verbose=False) print(\"l2 relative error =\",(torch.linalg.norm(y-fgp.post_mean(x),dim=1)/torch.linalg.norm(y,dim=1))) <pre>l2 relative error = tensor([0.0565, 0.1815, 0.3245])\n</pre> In\u00a0[12]: Copied! <pre>n_new = fgp.n*torch.tensor([4,8,2],device=device)\npcov_new = fgp.post_cov(x,z,n=n_new)\npvar_new = fgp.post_var(x,n=n_new)\npcvar_new = fgp.post_cubature_var(n=n_new)\nx_next = fgp.get_x_next(n_new)\ny_next = [fs[i](x_next[i]) for i in range(fgp.num_tasks)]\nfgp.add_y_next(y_next)\nassert torch.allclose(fgp.post_cov(x,z),pcov_new)\nassert torch.allclose(fgp.post_var(x),pvar_new)\nassert torch.allclose(fgp.post_cubature_var(),pcvar_new)\n</pre> n_new = fgp.n*torch.tensor([4,8,2],device=device) pcov_new = fgp.post_cov(x,z,n=n_new) pvar_new = fgp.post_var(x,n=n_new) pcvar_new = fgp.post_cubature_var(n=n_new) x_next = fgp.get_x_next(n_new) y_next = [fs[i](x_next[i]) for i in range(fgp.num_tasks)] fgp.add_y_next(y_next) assert torch.allclose(fgp.post_cov(x,z),pcov_new) assert torch.allclose(fgp.post_var(x),pvar_new) assert torch.allclose(fgp.post_cubature_var(),pcvar_new) In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/multitask/standard_gp/#standard-gp","title":"Standard GP\u00b6","text":""},{"location":"examples/multitask/standard_gp/#true-function","title":"True Function\u00b6","text":""},{"location":"examples/multitask/standard_gp/#construct-fast-gp","title":"Construct Fast GP\u00b6","text":""},{"location":"examples/multitask/standard_gp/#project-and-increase-sample-size","title":"Project and Increase Sample Size\u00b6","text":""},{"location":"examples/papers/fmtgps/fmtgps/","title":"FMTGPs","text":"In\u00a0[1]: Copied! <pre>import fastgps\nimport torch\ntorch.set_default_dtype(torch.float64)\nimport qmcpy as qp\nimport umbridge\nimport numpy as np \nimport time\nimport pandas as pd\nimport itertools\nimport warnings\nwarnings.simplefilter(\"ignore\")\nimport seaborn as sns\nimport matplotlib\nfrom matplotlib import pyplot\npyplot.style.use(\"seaborn-v0_8-whitegrid\")\nCOLORS = [\"xkcd:\"+color[:-1] for color in pd.read_csv(\"../../../../xkcd_colors.txt\",comment=\"#\",header=None).iloc[:,0].tolist()][::-1]\npyplot.rcParams['axes.prop_cycle'] = matplotlib.cycler(color=COLORS)\nLINESTYLES = ['solid','dotted','dashed','dashdot',(0, (1, 1))]\nMARKERS = [\"o\",\"D\",\"s\"]\nDEFAULTFONTSIZE = 30\npyplot.rcParams['xtick.labelsize'] = DEFAULTFONTSIZE\npyplot.rcParams['ytick.labelsize'] = DEFAULTFONTSIZE\npyplot.rcParams['ytick.labelsize'] = DEFAULTFONTSIZE\npyplot.rcParams['axes.titlesize'] = DEFAULTFONTSIZE\npyplot.rcParams['figure.titlesize'] = DEFAULTFONTSIZE\npyplot.rcParams[\"axes.labelsize\"] = DEFAULTFONTSIZE\npyplot.rcParams['legend.fontsize'] = DEFAULTFONTSIZE\npyplot.rcParams['font.size'] = DEFAULTFONTSIZE\npyplot.rcParams['lines.linewidth'] = 5\npyplot.rcParams['lines.markersize'] = 15\nPW = 30 # inches\n</pre> import fastgps import torch torch.set_default_dtype(torch.float64) import qmcpy as qp import umbridge import numpy as np  import time import pandas as pd import itertools import warnings warnings.simplefilter(\"ignore\") import seaborn as sns import matplotlib from matplotlib import pyplot pyplot.style.use(\"seaborn-v0_8-whitegrid\") COLORS = [\"xkcd:\"+color[:-1] for color in pd.read_csv(\"../../../../xkcd_colors.txt\",comment=\"#\",header=None).iloc[:,0].tolist()][::-1] pyplot.rcParams['axes.prop_cycle'] = matplotlib.cycler(color=COLORS) LINESTYLES = ['solid','dotted','dashed','dashdot',(0, (1, 1))] MARKERS = [\"o\",\"D\",\"s\"] DEFAULTFONTSIZE = 30 pyplot.rcParams['xtick.labelsize'] = DEFAULTFONTSIZE pyplot.rcParams['ytick.labelsize'] = DEFAULTFONTSIZE pyplot.rcParams['ytick.labelsize'] = DEFAULTFONTSIZE pyplot.rcParams['axes.titlesize'] = DEFAULTFONTSIZE pyplot.rcParams['figure.titlesize'] = DEFAULTFONTSIZE pyplot.rcParams[\"axes.labelsize\"] = DEFAULTFONTSIZE pyplot.rcParams['legend.fontsize'] = DEFAULTFONTSIZE pyplot.rcParams['font.size'] = DEFAULTFONTSIZE pyplot.rcParams['lines.linewidth'] = 5 pyplot.rcParams['lines.markersize'] = 15 PW = 30 # inches In\u00a0[\u00a0]: Copied! <pre>n = np.array([2**8,2**7,2**6],dtype=int)\nassert np.all(np.log2(n)%1==0)\nlevels = len(n)\nn_max = n.max()\npoint_sets = [\n    (\"IID\",qp.IIDStdUniform(2,seed=11,replications=levels)(n_max)),\n    (\"LD lattice sequence\",qp.Lattice(2,seed=5,replications=levels)(n_max)),\n    (\"LD digital sequence\",qp.DigitalNetB2(2,seed=11,replications=levels,randomize=\"DS\")(n_max)),\n]\nncols = len(point_sets)\nfig,ax = pyplot.subplots(nrows=1,ncols=ncols,figsize=(PW,PW/ncols),sharex=True,sharey=True)\nfor i in range(ncols):\n    name,x = point_sets[i]\n    for r in range(levels):\n        x_r = x[r,:n[r],:]\n        ax[i].scatter(x_r[:,0],x_r[:,1],color=COLORS[r],marker=MARKERS[r],label=(r\"task $%d$ with $2^{%d}$ points\"%(r+1,int(np.log2(n[r]))) if i==0 else None))\n    ax[i].set_title(name)\n    ax[i].set_xlabel(r\"$x_1$\");\nax[0].set_ylabel(r\"$x_2$\")\nax[0].set_xlim([0,1]); ax[0].set_xticks([0,1/8,1/4,3/8,1/2,5/8,3/4,7/8,1]); ax[0].set_xticklabels([r\"$0$\",\"\",r\"$1/4$\",\"\",r\"$1/2$\",\"\",r\"$3/4$\",\"\",r\"$1$\"])\nax[0].set_ylim([0,1]); ax[0].set_yticks([0,1/8,1/4,3/8,1/2,5/8,3/4,7/8,1]); ax[0].set_yticklabels([r\"$0$\",\"\",r\"$1/4$\",\"\",r\"$1/2$\",\"\",r\"$3/4$\",\"\",r\"$1$\"])\nfig.legend(loc=\"lower center\",ncols=levels,bbox_to_anchor=(1/2,-1/12))\nfig.savefig(\"./points.pdf\",bbox_inches=\"tight\")\n</pre> n = np.array([2**8,2**7,2**6],dtype=int) assert np.all(np.log2(n)%1==0) levels = len(n) n_max = n.max() point_sets = [     (\"IID\",qp.IIDStdUniform(2,seed=11,replications=levels)(n_max)),     (\"LD lattice sequence\",qp.Lattice(2,seed=5,replications=levels)(n_max)),     (\"LD digital sequence\",qp.DigitalNetB2(2,seed=11,replications=levels,randomize=\"DS\")(n_max)), ] ncols = len(point_sets) fig,ax = pyplot.subplots(nrows=1,ncols=ncols,figsize=(PW,PW/ncols),sharex=True,sharey=True) for i in range(ncols):     name,x = point_sets[i]     for r in range(levels):         x_r = x[r,:n[r],:]         ax[i].scatter(x_r[:,0],x_r[:,1],color=COLORS[r],marker=MARKERS[r],label=(r\"task $%d$ with $2^{%d}$ points\"%(r+1,int(np.log2(n[r]))) if i==0 else None))     ax[i].set_title(name)     ax[i].set_xlabel(r\"$x_1$\"); ax[0].set_ylabel(r\"$x_2$\") ax[0].set_xlim([0,1]); ax[0].set_xticks([0,1/8,1/4,3/8,1/2,5/8,3/4,7/8,1]); ax[0].set_xticklabels([r\"$0$\",\"\",r\"$1/4$\",\"\",r\"$1/2$\",\"\",r\"$3/4$\",\"\",r\"$1$\"]) ax[0].set_ylim([0,1]); ax[0].set_yticks([0,1/8,1/4,3/8,1/2,5/8,3/4,7/8,1]); ax[0].set_yticklabels([r\"$0$\",\"\",r\"$1/4$\",\"\",r\"$1/2$\",\"\",r\"$3/4$\",\"\",r\"$1$\"]) fig.legend(loc=\"lower center\",ncols=levels,bbox_to_anchor=(1/2,-1/12)) fig.savefig(\"./points.pdf\",bbox_inches=\"tight\") In\u00a0[\u00a0]: Copied! <pre>fig,ax = pyplot.subplots(nrows=1,ncols=5,figsize=(PW,PW/5))\narr1 = torch.tensor([\n    [1,0,0,0,0,0,0,0, 1,0,0,0, 1,0],\n    [0,1,0,0,0,0,0,0, 0,1,0,0, 0,1],\n    [0,0,1,0,0,0,0,0, 0,0,1,0, 1,0],\n    [0,0,0,1,0,0,0,0, 0,0,0,1, 0,1],\n    [0,0,0,0,1,0,0,0, 1,0,0,0, 1,0],\n    [0,0,0,0,0,1,0,0, 0,1,0,0, 0,1],\n    [0,0,0,0,0,0,1,0, 0,0,1,0, 1,0],\n    [0,0,0,0,0,0,0,1, 0,0,0,1, 0,1],\n    [1,0,0,0,1,0,0,0, 1,0,0,0, 1,0],\n    [0,1,0,0,0,1,0,0, 0,1,0,0, 0,1],\n    [0,0,1,0,0,0,1,0, 0,0,1,0, 1,0],\n    [0,0,0,1,0,0,0,1, 0,0,0,1, 0,1],\n    [1,0,1,0,1,0,1,0, 1,0,1,0, 1,0],\n    [0,1,0,1,0,1,0,1, 0,1,0,1, 0,1],\n])\narr2 = torch.tensor([\n    [2,0,0,0,0,0,0,0, 1,0,0,0, 1,0],\n    [0,2,0,0,0,0,0,0, 0,1,0,0, 0,1],\n    [0,0,2,0,0,0,0,0, 0,0,1,0, 1,0],\n    [0,0,0,2,0,0,0,0, 0,0,0,1, 0,1],\n    [0,0,0,0,2,0,0,0, 1,0,0,0, 1,0],\n    [0,0,0,0,0,2,0,0, 0,1,0,0, 0,1],\n    [0,0,0,0,0,0,2,0, 0,0,1,0, 1,0],\n    [0,0,0,0,0,0,0,2, 0,0,0,1, 0,1],\n    [1,0,0,0,1,0,0,0, 1,0,0,0, 1,0],\n    [0,1,0,0,0,1,0,0, 0,1,0,0, 0,1],\n    [0,0,1,0,0,0,1,0, 0,0,1,0, 1,0],\n    [0,0,0,1,0,0,0,1, 0,0,0,1, 0,1],\n    [1,0,1,0,1,0,1,0, 1,0,1,0, 1,0],\n    [0,1,0,1,0,1,0,1, 0,1,0,1, 0,1],\n])\narr3 = torch.tensor([\n    [2,0,0,0,2,0,0,0, 2,0,0,0, 1,0],\n    [0,2,0,0,0,2,0,0, 0,2,0,0, 0,1],\n    [0,0,2,0,0,0,2,0, 0,0,2,0, 1,0],\n    [0,0,0,2,0,0,0,2, 0,0,0,2, 0,1],\n    [2,0,0,0,2,0,0,0, 2,0,0,0, 1,0],\n    [0,2,0,0,0,2,0,0, 0,2,0,0, 0,1],\n    [0,0,2,0,0,0,2,0, 0,0,2,0, 1,0],\n    [0,0,0,2,0,0,0,2, 0,0,0,2, 0,1],\n    [2,0,0,0,2,0,0,0, 2,0,0,0, 1,0],\n    [0,2,0,0,0,2,0,0, 0,2,0,0, 0,1],\n    [0,0,2,0,0,0,2,0, 0,0,2,0, 1,0],\n    [0,0,0,2,0,0,0,2, 0,0,0,2, 0,1],\n    [1,0,1,0,1,0,1,0, 1,0,1,0, 1,0],\n    [0,1,0,1,0,1,0,1, 0,1,0,1, 0,1],\n],dtype=int)\narr4 = torch.tensor([\n    [2,0,2,0,2,0,2,0, 2,0,2,0, 2,0],\n    [0,2,0,2,0,2,0,2, 0,2,0,2, 0,2],\n    [2,0,2,0,2,0,2,0, 2,0,2,0, 2,0],\n    [0,2,0,2,0,2,0,2, 0,2,0,2, 0,2],\n    [2,0,2,0,2,0,2,0, 2,0,2,0, 2,0],\n    [0,2,0,2,0,2,0,2, 0,2,0,2, 0,2],\n    [2,0,2,0,2,0,2,0, 2,0,2,0, 2,0],\n    [0,2,0,2,0,2,0,2, 0,2,0,2, 0,2],\n    [2,0,2,0,2,0,2,0, 2,0,2,0, 2,0],\n    [0,2,0,2,0,2,0,2, 0,2,0,2, 0,2],\n    [2,0,2,0,2,0,2,0, 2,0,2,0, 2,0],\n    [0,2,0,2,0,2,0,2, 0,2,0,2, 0,2],\n    [2,0,2,0,2,0,2,0, 2,0,2,0, 2,0],\n    [0,2,0,2,0,2,0,2, 0,2,0,2, 0,2],\n])\ncmat = \"k\"#COLORS[0]\ncinv = \"gray\"#COLORS[1]\nimport matplotlib.colors as mcolors\ncmap = mcolors.ListedColormap([\"w\",\"k\",\"gray\"])\nnorm = mcolors.BoundaryNorm([-.5,0.5,1.5,2.5], cmap.N)\nlinewidth = 5\nlinewidthboarder = 10\nfor l in range(5):\n    ax[l].axis(\"off\")\n    # if l==0: continue \n    ax[l].axhline(y=-0.5,color=cmat,linewidth=linewidthboarder)\n    ax[l].axhline(y=13.5,color=cmat,linewidth=linewidthboarder)\n    ax[l].axvline(x=-0.5,color=cmat,linewidth=linewidthboarder)\n    ax[l].axvline(x=13.5,color=cmat,linewidth=linewidthboarder)\nax[1].imshow(arr1,cmap=cmap,norm=norm)\nax[1].hlines(y=7.5,xmin=-0.5,xmax=7.5,color=cmat,linewidth=linewidth)\nax[1].vlines(x=7.5,ymin=-0.5,ymax=7.5,color=cmat,linewidth=linewidth)\nax[1].hlines(y=11.5,xmin=-0.5,xmax=11.5,color=cmat,linewidth=linewidth)\nax[1].vlines(x=11.5,ymin=-0.5,ymax=11.5,color=cmat,linewidth=linewidth)\nax[2].imshow(arr2,cmap=cmap,norm=norm)\nax[2].hlines(y=7.5,xmin=-0.5,xmax=7.5,color=cinv,linewidth=linewidth)\nax[2].vlines(x=7.5,ymin=-0.5,ymax=7.5,color=cinv,linewidth=linewidth)\nax[2].hlines(y=-0.5,xmin=-0.5,xmax=7.5,color=cinv,linewidth=linewidthboarder)\nax[2].vlines(x=-0.5,ymin=-0.5,ymax=7.5,color=cinv,linewidth=linewidthboarder)\nax[2].hlines(y=11.5,xmin=-0.5,xmax=11.5,color=cmat,linewidth=linewidth)\nax[2].vlines(x=11.5,ymin=-0.5,ymax=11.5,color=cmat,linewidth=linewidth)\nax[3].imshow(arr3,cmap=cmap,norm=norm)\nax[3].hlines(y=-0.5,xmin=-0.5,xmax=11.5,color=cinv,linewidth=linewidthboarder)\nax[3].vlines(x=-0.5,ymin=-0.5,ymax=11.5,color=cinv,linewidth=linewidthboarder)\nax[3].hlines(y=11.5,xmin=-0.5,xmax=11.5,color=cinv,linewidth=linewidth)\nax[3].vlines(x=11.5,ymin=-0.5,ymax=11.5,color=cinv,linewidth=linewidth)\nax[4].imshow(arr4,cmap=cmap,norm=norm)\nax[4].axhline(y=-0.5,color=cinv,linewidth=linewidthboarder)\nax[4].axhline(y=13.5,color=cinv,linewidth=linewidthboarder)\nax[4].axvline(x=-0.5,color=cinv,linewidth=linewidthboarder)\nax[4].axvline(x=13.5,color=cinv,linewidth=linewidthboarder)\nfgp = fastgps.StandardGP(\n    kernel=qp.KernelMultiTask(qp.KernelDigShiftInvar(d=1,alpha=1,torchify=True,t=63,lengthscales=1e3),num_tasks=3,rank_factor=3),\n    seqs=[qp.DigitalNetB2(1,seed=7+l,randomize=\"DS\",t=63) for l in range(3)],\n    noise = 1e-4)\nx0 = fgp.get_x_next(n=8,task=0)\nfgp.add_y_next(torch.sin(x0[:,0]),task=0)\nx1 = fgp.get_x_next(n=4,task=1)\nfgp.add_y_next(torch.cos(x1[:,0]),task=1)\nx2 = fgp.get_x_next(n=2,task=2)\nfgp.add_y_next(torch.sin(x2[:,0])+torch.cos(x2[:,0]),task=2)\n# fgp.fit(verbose=False)\nkmatinv,_ = fgp.get_inv_log_det_cache()(fgp)\nkmat = torch.linalg.inv(kmatinv.detach())\nkmat_tf = kmat#1/(1+torch.exp(-kmat))\n# kmat_tf = 1/(1+torch.exp(-kmat))\nprint(kmat_tf[:-8,:-8])\nax[0].imshow(kmat_tf,cmap=sns.color_palette(\"Spectral_r\",as_cmap=True))\nax[0].hlines(y=7.5,xmin=-0.5,xmax=7.5,color=cmat,linewidth=linewidth)\nax[0].vlines(x=7.5,ymin=-0.5,ymax=7.5,color=cmat,linewidth=linewidth)\nax[0].hlines(y=11.5,xmin=-0.5,xmax=11.5,color=cmat,linewidth=linewidth)\nax[0].vlines(x=11.5,ymin=-0.5,ymax=11.5,color=cmat,linewidth=linewidth)\nax[0].set_title(r\"$\\widetilde{\\mathsf{K}} = \\mathsf{V} \\;\\widetilde{\\mathsf{\\Lambda}}\\;\\overline{\\mathsf{V}}$\")\nax[1].set_title(r\"$\\widetilde{\\mathsf{\\Lambda}} = \\widetilde{\\mathsf{\\Lambda}}_{:3,:3}$\")\nax[2].set_title(r\"$\\left(\\widetilde{\\mathsf{\\Lambda}}_{:1,:1}\\right)^{-1}$\")\nax[3].set_title(r\"$\\left(\\widetilde{\\mathsf{\\Lambda}}_{:2,:2}\\right)^{-1}$\")\nax[4].set_title(r\"$\\widetilde{\\mathsf{\\Lambda}}^{-1} = \\left(\\widetilde{\\mathsf{\\Lambda}}_{:3,:3}\\right)^{-1}$\")\nfig.savefig(\"./kmat_struct_inv_alg.pdf\",bbox_inches=\"tight\")\n</pre> fig,ax = pyplot.subplots(nrows=1,ncols=5,figsize=(PW,PW/5)) arr1 = torch.tensor([     [1,0,0,0,0,0,0,0, 1,0,0,0, 1,0],     [0,1,0,0,0,0,0,0, 0,1,0,0, 0,1],     [0,0,1,0,0,0,0,0, 0,0,1,0, 1,0],     [0,0,0,1,0,0,0,0, 0,0,0,1, 0,1],     [0,0,0,0,1,0,0,0, 1,0,0,0, 1,0],     [0,0,0,0,0,1,0,0, 0,1,0,0, 0,1],     [0,0,0,0,0,0,1,0, 0,0,1,0, 1,0],     [0,0,0,0,0,0,0,1, 0,0,0,1, 0,1],     [1,0,0,0,1,0,0,0, 1,0,0,0, 1,0],     [0,1,0,0,0,1,0,0, 0,1,0,0, 0,1],     [0,0,1,0,0,0,1,0, 0,0,1,0, 1,0],     [0,0,0,1,0,0,0,1, 0,0,0,1, 0,1],     [1,0,1,0,1,0,1,0, 1,0,1,0, 1,0],     [0,1,0,1,0,1,0,1, 0,1,0,1, 0,1], ]) arr2 = torch.tensor([     [2,0,0,0,0,0,0,0, 1,0,0,0, 1,0],     [0,2,0,0,0,0,0,0, 0,1,0,0, 0,1],     [0,0,2,0,0,0,0,0, 0,0,1,0, 1,0],     [0,0,0,2,0,0,0,0, 0,0,0,1, 0,1],     [0,0,0,0,2,0,0,0, 1,0,0,0, 1,0],     [0,0,0,0,0,2,0,0, 0,1,0,0, 0,1],     [0,0,0,0,0,0,2,0, 0,0,1,0, 1,0],     [0,0,0,0,0,0,0,2, 0,0,0,1, 0,1],     [1,0,0,0,1,0,0,0, 1,0,0,0, 1,0],     [0,1,0,0,0,1,0,0, 0,1,0,0, 0,1],     [0,0,1,0,0,0,1,0, 0,0,1,0, 1,0],     [0,0,0,1,0,0,0,1, 0,0,0,1, 0,1],     [1,0,1,0,1,0,1,0, 1,0,1,0, 1,0],     [0,1,0,1,0,1,0,1, 0,1,0,1, 0,1], ]) arr3 = torch.tensor([     [2,0,0,0,2,0,0,0, 2,0,0,0, 1,0],     [0,2,0,0,0,2,0,0, 0,2,0,0, 0,1],     [0,0,2,0,0,0,2,0, 0,0,2,0, 1,0],     [0,0,0,2,0,0,0,2, 0,0,0,2, 0,1],     [2,0,0,0,2,0,0,0, 2,0,0,0, 1,0],     [0,2,0,0,0,2,0,0, 0,2,0,0, 0,1],     [0,0,2,0,0,0,2,0, 0,0,2,0, 1,0],     [0,0,0,2,0,0,0,2, 0,0,0,2, 0,1],     [2,0,0,0,2,0,0,0, 2,0,0,0, 1,0],     [0,2,0,0,0,2,0,0, 0,2,0,0, 0,1],     [0,0,2,0,0,0,2,0, 0,0,2,0, 1,0],     [0,0,0,2,0,0,0,2, 0,0,0,2, 0,1],     [1,0,1,0,1,0,1,0, 1,0,1,0, 1,0],     [0,1,0,1,0,1,0,1, 0,1,0,1, 0,1], ],dtype=int) arr4 = torch.tensor([     [2,0,2,0,2,0,2,0, 2,0,2,0, 2,0],     [0,2,0,2,0,2,0,2, 0,2,0,2, 0,2],     [2,0,2,0,2,0,2,0, 2,0,2,0, 2,0],     [0,2,0,2,0,2,0,2, 0,2,0,2, 0,2],     [2,0,2,0,2,0,2,0, 2,0,2,0, 2,0],     [0,2,0,2,0,2,0,2, 0,2,0,2, 0,2],     [2,0,2,0,2,0,2,0, 2,0,2,0, 2,0],     [0,2,0,2,0,2,0,2, 0,2,0,2, 0,2],     [2,0,2,0,2,0,2,0, 2,0,2,0, 2,0],     [0,2,0,2,0,2,0,2, 0,2,0,2, 0,2],     [2,0,2,0,2,0,2,0, 2,0,2,0, 2,0],     [0,2,0,2,0,2,0,2, 0,2,0,2, 0,2],     [2,0,2,0,2,0,2,0, 2,0,2,0, 2,0],     [0,2,0,2,0,2,0,2, 0,2,0,2, 0,2], ]) cmat = \"k\"#COLORS[0] cinv = \"gray\"#COLORS[1] import matplotlib.colors as mcolors cmap = mcolors.ListedColormap([\"w\",\"k\",\"gray\"]) norm = mcolors.BoundaryNorm([-.5,0.5,1.5,2.5], cmap.N) linewidth = 5 linewidthboarder = 10 for l in range(5):     ax[l].axis(\"off\")     # if l==0: continue      ax[l].axhline(y=-0.5,color=cmat,linewidth=linewidthboarder)     ax[l].axhline(y=13.5,color=cmat,linewidth=linewidthboarder)     ax[l].axvline(x=-0.5,color=cmat,linewidth=linewidthboarder)     ax[l].axvline(x=13.5,color=cmat,linewidth=linewidthboarder) ax[1].imshow(arr1,cmap=cmap,norm=norm) ax[1].hlines(y=7.5,xmin=-0.5,xmax=7.5,color=cmat,linewidth=linewidth) ax[1].vlines(x=7.5,ymin=-0.5,ymax=7.5,color=cmat,linewidth=linewidth) ax[1].hlines(y=11.5,xmin=-0.5,xmax=11.5,color=cmat,linewidth=linewidth) ax[1].vlines(x=11.5,ymin=-0.5,ymax=11.5,color=cmat,linewidth=linewidth) ax[2].imshow(arr2,cmap=cmap,norm=norm) ax[2].hlines(y=7.5,xmin=-0.5,xmax=7.5,color=cinv,linewidth=linewidth) ax[2].vlines(x=7.5,ymin=-0.5,ymax=7.5,color=cinv,linewidth=linewidth) ax[2].hlines(y=-0.5,xmin=-0.5,xmax=7.5,color=cinv,linewidth=linewidthboarder) ax[2].vlines(x=-0.5,ymin=-0.5,ymax=7.5,color=cinv,linewidth=linewidthboarder) ax[2].hlines(y=11.5,xmin=-0.5,xmax=11.5,color=cmat,linewidth=linewidth) ax[2].vlines(x=11.5,ymin=-0.5,ymax=11.5,color=cmat,linewidth=linewidth) ax[3].imshow(arr3,cmap=cmap,norm=norm) ax[3].hlines(y=-0.5,xmin=-0.5,xmax=11.5,color=cinv,linewidth=linewidthboarder) ax[3].vlines(x=-0.5,ymin=-0.5,ymax=11.5,color=cinv,linewidth=linewidthboarder) ax[3].hlines(y=11.5,xmin=-0.5,xmax=11.5,color=cinv,linewidth=linewidth) ax[3].vlines(x=11.5,ymin=-0.5,ymax=11.5,color=cinv,linewidth=linewidth) ax[4].imshow(arr4,cmap=cmap,norm=norm) ax[4].axhline(y=-0.5,color=cinv,linewidth=linewidthboarder) ax[4].axhline(y=13.5,color=cinv,linewidth=linewidthboarder) ax[4].axvline(x=-0.5,color=cinv,linewidth=linewidthboarder) ax[4].axvline(x=13.5,color=cinv,linewidth=linewidthboarder) fgp = fastgps.StandardGP(     kernel=qp.KernelMultiTask(qp.KernelDigShiftInvar(d=1,alpha=1,torchify=True,t=63,lengthscales=1e3),num_tasks=3,rank_factor=3),     seqs=[qp.DigitalNetB2(1,seed=7+l,randomize=\"DS\",t=63) for l in range(3)],     noise = 1e-4) x0 = fgp.get_x_next(n=8,task=0) fgp.add_y_next(torch.sin(x0[:,0]),task=0) x1 = fgp.get_x_next(n=4,task=1) fgp.add_y_next(torch.cos(x1[:,0]),task=1) x2 = fgp.get_x_next(n=2,task=2) fgp.add_y_next(torch.sin(x2[:,0])+torch.cos(x2[:,0]),task=2) # fgp.fit(verbose=False) kmatinv,_ = fgp.get_inv_log_det_cache()(fgp) kmat = torch.linalg.inv(kmatinv.detach()) kmat_tf = kmat#1/(1+torch.exp(-kmat)) # kmat_tf = 1/(1+torch.exp(-kmat)) print(kmat_tf[:-8,:-8]) ax[0].imshow(kmat_tf,cmap=sns.color_palette(\"Spectral_r\",as_cmap=True)) ax[0].hlines(y=7.5,xmin=-0.5,xmax=7.5,color=cmat,linewidth=linewidth) ax[0].vlines(x=7.5,ymin=-0.5,ymax=7.5,color=cmat,linewidth=linewidth) ax[0].hlines(y=11.5,xmin=-0.5,xmax=11.5,color=cmat,linewidth=linewidth) ax[0].vlines(x=11.5,ymin=-0.5,ymax=11.5,color=cmat,linewidth=linewidth) ax[0].set_title(r\"$\\widetilde{\\mathsf{K}} = \\mathsf{V} \\;\\widetilde{\\mathsf{\\Lambda}}\\;\\overline{\\mathsf{V}}$\") ax[1].set_title(r\"$\\widetilde{\\mathsf{\\Lambda}} = \\widetilde{\\mathsf{\\Lambda}}_{:3,:3}$\") ax[2].set_title(r\"$\\left(\\widetilde{\\mathsf{\\Lambda}}_{:1,:1}\\right)^{-1}$\") ax[3].set_title(r\"$\\left(\\widetilde{\\mathsf{\\Lambda}}_{:2,:2}\\right)^{-1}$\") ax[4].set_title(r\"$\\widetilde{\\mathsf{\\Lambda}}^{-1} = \\left(\\widetilde{\\mathsf{\\Lambda}}_{:3,:3}\\right)^{-1}$\") fig.savefig(\"./kmat_struct_inv_alg.pdf\",bbox_inches=\"tight\") <pre>tensor([[ 4004.0001, -1996.0000,  1004.0000, -1996.0000,  2504.0000, -1996.0000],\n        [-1996.0000,  4004.0001, -1996.0000,  1004.0000, -1996.0000,  2504.0000],\n        [ 1004.0000, -1996.0000,  4004.0001, -1996.0000,  1004.0000, -1996.0000],\n        [-1996.0000,  1004.0000, -1996.0000,  4004.0001, -1996.0000,  1004.0000],\n        [ 2504.0000, -1996.0000,  1004.0000, -1996.0000,  4004.0001, -1996.0000],\n        [-1996.0000,  2504.0000, -1996.0000,  1004.0000, -1996.0000,  4004.0001]])\n</pre> In\u00a0[\u00a0]: Copied! <pre>def currin(level, x): # https://www.sfu.ca/~ssurjano/curretal88exp.html\n    assert x.size(-1)==2 \n    assert level in [0,1]\n    x1,x2 = x[...,0],x[...,1]\n    f = lambda x1,x2: (1-torch.exp(-1/(2*x2)))*(2300*x1**3+1900*x1**2+2092*x1+60)/(100*x1**3+500*x1**2+4*x1+20)\n    if level==0:\n        y = 1/4*(f(x1+0.05,x2+0.05)+f(x1+0.05,torch.maximum(torch.zeros(1),x2-0.05))+f(x1-0.05,x2+0.05)+f(x1-0.05,torch.maximum(torch.zeros(1),x2-0.05)))\n    else:\n        y = f(x1,x2)\n    return y\n\ndef ackley(level, x): # https://www.sfu.ca/~ssurjano/ackley.html\n    assert level in [0,1]\n    a = 20\n    b = 0.2 \n    c = 0 if level==0 else 2*np.pi\n    t = 2*32.768*x-32.768\n    d = x.size(-1)\n    y = -a*torch.exp(-b*torch.linalg.norm(t,dim=-1))-torch.exp(1/d*torch.cos(c*t).sum(-1))+a+np.exp(1)\n    return y\n\ndef park1(level, x): # https://www.sfu.ca/~ssurjano/park91a.html\n    assert x.size(-1)==4 \n    assert level in [0,1]\n    x1,x2,x3,x4 = x[...,0],x[...,1],x[...,2],x[...,3]\n    y = x1/2*(torch.sqrt(1+(x2+x3**2)*x4/x1**2)-1)+(x1+3*x4)*torch.exp(1+torch.sin(x3))\n    if level==0:\n        y = (1+torch.sin(x1)/10)*y-2*x1+x2**2+x3**2+0.5\n    return y \n\ndef park2(level, x): # https://www.sfu.ca/~ssurjano/park91b.html\n    assert x.size(-1)==4 \n    assert level in [0,1]\n    x1,x2,x3,x4 = x[...,0],x[...,1],x[...,2],x[...,3]\n    y = 2/3*torch.exp(x1+x2)-x4*torch.sin(x3)+x3\n    if level==0:\n        y = 1.2*y-1\n    return y \n\ndef borehole(level, x): # https://www.sfu.ca/~ssurjano/borehole.html\n    assert level in [0,1]\n    assert x.size(-1)==8\n    x = x.numpy()\n    import scipy.stats\n    rw = torch.from_numpy(scipy.stats.norm(loc=0.10,scale=0.0161812).ppf(x[...,0]))\n    r  = torch.from_numpy(scipy.stats.lognorm(scale=np.exp(7.71),s=1.0056).ppf(x[...,1]))\n    Tu = torch.from_numpy(scipy.stats.uniform(loc=63070,scale=115600-63070).ppf(x[...,2]))\n    Hu = torch.from_numpy(scipy.stats.uniform(loc=990,scale=1110-990).ppf(x[...,3]))\n    Tl = torch.from_numpy(scipy.stats.uniform(loc=63.1,scale=116-63.1).ppf(x[...,4]))\n    Hl = torch.from_numpy(scipy.stats.uniform(loc=700,scale=820-700).ppf(x[...,5]))\n    L  = torch.from_numpy(scipy.stats.uniform(loc=1120,scale=1680-1120).ppf(x[...,6]))\n    Kw = torch.from_numpy(scipy.stats.uniform(loc=9855,scale=12045-9855).ppf(x[...,7]))\n    C1 = 5 if level==0 else 2\n    C2 = 1.5 if level==0 else 1\n    frac1 = C1 * torch.pi * Tu * (Hu-Hl)\n    frac2a = 2*L*Tu / (torch.log(r/rw)*rw**2*Kw)\n    frac2b = Tu / Tl\n    frac2 = torch.log(r/rw) * (C2+frac2a+frac2b)\n    y = frac1 / frac2\n    return y\n\ndef elliptic(level, x):\n    from scipy.sparse import diags\n    from scipy.sparse.linalg import spsolve\n    from scipy.stats import norm\n    coeffs = x.numpy()\n    # Define grid\n    N = 2**level  # Number of intervals\n    h = 1.0 / N  # Mesh spacing\n    x = np.linspace(h, 1 - h, N - 1)  # Interior points\n    # Compute diffusion coefficient a(x)\n    coeffs = np.random.rand(8) if coeffs is None else coeffs\n    assert isinstance(coeffs,np.ndarray)\n    coeffs = norm.ppf(coeffs)  # Transform from uniform to iid Gaussian\n    batch_shape = list(coeffs.shape)[:-1]\n    k = np.arange(1,coeffs.shape[-1]+1)\n    a_x = np.exp((coeffs[...,None] / k[:,None]  *np.sin(np.pi * k[:,None] * x)).sum(-2))\n    # Compute a at half-grid points (needed for flux terms)\n    a_half = np.zeros(batch_shape+[N])\n    a_half[...,1:-1] = (a_x[...,:-1] + a_x[...,1:]) / 2  # Midpoint values for flux approximation\n    a_half[...,0] = a_x[...,0]  # At the first midpoint\n    a_half[...,-1] = a_x[...,-1]  # At the last midpoint\n    # Construct the finite difference matrix\n    lower_diag = -a_half[...,1:-1] / h**2\n    upper_diag = -a_half[...,1:-1] / h**2\n    main_diag = (a_half[...,:-1] + a_half[...,1:]) / h**2\n    # Right-hand side (forcing term)\n    b = np.ones([N-1])  # Constant source term (1)\n    def _spsolve(main_diag, upper_diag, lower_diag, b):\n        N = main_diag.shape[-1]\n        assert main_diag.shape==(N,) and upper_diag.shape==(N-1,) and lower_diag.shape==(N-1,) and b.shape==(N,)\n        A = diags([main_diag, upper_diag, lower_diag],[0, 1, -1],shape=[N,N],format=\"csc\",)\n        u = spsolve(A, b)\n        return u\n    vec_spsolve = np.vectorize(_spsolve,signature=\"(n),(m),(m),(n)-&gt;(n)\")\n    u = vec_spsolve(main_diag, upper_diag, lower_diag, b)\n    # Find index closest to x = 0.5\n    idx = np.argmin(np.abs(x - 0.5))\n    # return u[...,idx], u, x, a_x, x[idx]\n    # return torch.from_numpy(u[...,idx])\n    return torch.from_numpy(u.max(-1))\n\ndef asian_option(level, x):\n    assert level&gt;0\n    d = 2**level\n    assert x.size(-1)&gt;=d\n    ao = qp.FinancialOption(\n        qp.IIDStdUniform(d),\n        option = \"ASIAN\",\n        call_put = \"CALL\",\n        asian_mean = \"GEOMETRIC\",\n        asian_mean_quadrature_rule = \"RIGHT\",\n        volatility = 0.2, \n        start_price = 100, \n        strike_price = 100, \n        interest_rate = 0.05, \n        t_final = 1, \n    )\n    y = torch.from_numpy(ao.f(x[...,:d].numpy()))\n    return y\n</pre> def currin(level, x): # https://www.sfu.ca/~ssurjano/curretal88exp.html     assert x.size(-1)==2      assert level in [0,1]     x1,x2 = x[...,0],x[...,1]     f = lambda x1,x2: (1-torch.exp(-1/(2*x2)))*(2300*x1**3+1900*x1**2+2092*x1+60)/(100*x1**3+500*x1**2+4*x1+20)     if level==0:         y = 1/4*(f(x1+0.05,x2+0.05)+f(x1+0.05,torch.maximum(torch.zeros(1),x2-0.05))+f(x1-0.05,x2+0.05)+f(x1-0.05,torch.maximum(torch.zeros(1),x2-0.05)))     else:         y = f(x1,x2)     return y  def ackley(level, x): # https://www.sfu.ca/~ssurjano/ackley.html     assert level in [0,1]     a = 20     b = 0.2      c = 0 if level==0 else 2*np.pi     t = 2*32.768*x-32.768     d = x.size(-1)     y = -a*torch.exp(-b*torch.linalg.norm(t,dim=-1))-torch.exp(1/d*torch.cos(c*t).sum(-1))+a+np.exp(1)     return y  def park1(level, x): # https://www.sfu.ca/~ssurjano/park91a.html     assert x.size(-1)==4      assert level in [0,1]     x1,x2,x3,x4 = x[...,0],x[...,1],x[...,2],x[...,3]     y = x1/2*(torch.sqrt(1+(x2+x3**2)*x4/x1**2)-1)+(x1+3*x4)*torch.exp(1+torch.sin(x3))     if level==0:         y = (1+torch.sin(x1)/10)*y-2*x1+x2**2+x3**2+0.5     return y   def park2(level, x): # https://www.sfu.ca/~ssurjano/park91b.html     assert x.size(-1)==4      assert level in [0,1]     x1,x2,x3,x4 = x[...,0],x[...,1],x[...,2],x[...,3]     y = 2/3*torch.exp(x1+x2)-x4*torch.sin(x3)+x3     if level==0:         y = 1.2*y-1     return y   def borehole(level, x): # https://www.sfu.ca/~ssurjano/borehole.html     assert level in [0,1]     assert x.size(-1)==8     x = x.numpy()     import scipy.stats     rw = torch.from_numpy(scipy.stats.norm(loc=0.10,scale=0.0161812).ppf(x[...,0]))     r  = torch.from_numpy(scipy.stats.lognorm(scale=np.exp(7.71),s=1.0056).ppf(x[...,1]))     Tu = torch.from_numpy(scipy.stats.uniform(loc=63070,scale=115600-63070).ppf(x[...,2]))     Hu = torch.from_numpy(scipy.stats.uniform(loc=990,scale=1110-990).ppf(x[...,3]))     Tl = torch.from_numpy(scipy.stats.uniform(loc=63.1,scale=116-63.1).ppf(x[...,4]))     Hl = torch.from_numpy(scipy.stats.uniform(loc=700,scale=820-700).ppf(x[...,5]))     L  = torch.from_numpy(scipy.stats.uniform(loc=1120,scale=1680-1120).ppf(x[...,6]))     Kw = torch.from_numpy(scipy.stats.uniform(loc=9855,scale=12045-9855).ppf(x[...,7]))     C1 = 5 if level==0 else 2     C2 = 1.5 if level==0 else 1     frac1 = C1 * torch.pi * Tu * (Hu-Hl)     frac2a = 2*L*Tu / (torch.log(r/rw)*rw**2*Kw)     frac2b = Tu / Tl     frac2 = torch.log(r/rw) * (C2+frac2a+frac2b)     y = frac1 / frac2     return y  def elliptic(level, x):     from scipy.sparse import diags     from scipy.sparse.linalg import spsolve     from scipy.stats import norm     coeffs = x.numpy()     # Define grid     N = 2**level  # Number of intervals     h = 1.0 / N  # Mesh spacing     x = np.linspace(h, 1 - h, N - 1)  # Interior points     # Compute diffusion coefficient a(x)     coeffs = np.random.rand(8) if coeffs is None else coeffs     assert isinstance(coeffs,np.ndarray)     coeffs = norm.ppf(coeffs)  # Transform from uniform to iid Gaussian     batch_shape = list(coeffs.shape)[:-1]     k = np.arange(1,coeffs.shape[-1]+1)     a_x = np.exp((coeffs[...,None] / k[:,None]  *np.sin(np.pi * k[:,None] * x)).sum(-2))     # Compute a at half-grid points (needed for flux terms)     a_half = np.zeros(batch_shape+[N])     a_half[...,1:-1] = (a_x[...,:-1] + a_x[...,1:]) / 2  # Midpoint values for flux approximation     a_half[...,0] = a_x[...,0]  # At the first midpoint     a_half[...,-1] = a_x[...,-1]  # At the last midpoint     # Construct the finite difference matrix     lower_diag = -a_half[...,1:-1] / h**2     upper_diag = -a_half[...,1:-1] / h**2     main_diag = (a_half[...,:-1] + a_half[...,1:]) / h**2     # Right-hand side (forcing term)     b = np.ones([N-1])  # Constant source term (1)     def _spsolve(main_diag, upper_diag, lower_diag, b):         N = main_diag.shape[-1]         assert main_diag.shape==(N,) and upper_diag.shape==(N-1,) and lower_diag.shape==(N-1,) and b.shape==(N,)         A = diags([main_diag, upper_diag, lower_diag],[0, 1, -1],shape=[N,N],format=\"csc\",)         u = spsolve(A, b)         return u     vec_spsolve = np.vectorize(_spsolve,signature=\"(n),(m),(m),(n)-&gt;(n)\")     u = vec_spsolve(main_diag, upper_diag, lower_diag, b)     # Find index closest to x = 0.5     idx = np.argmin(np.abs(x - 0.5))     # return u[...,idx], u, x, a_x, x[idx]     # return torch.from_numpy(u[...,idx])     return torch.from_numpy(u.max(-1))  def asian_option(level, x):     assert level&gt;0     d = 2**level     assert x.size(-1)&gt;=d     ao = qp.FinancialOption(         qp.IIDStdUniform(d),         option = \"ASIAN\",         call_put = \"CALL\",         asian_mean = \"GEOMETRIC\",         asian_mean_quadrature_rule = \"RIGHT\",         volatility = 0.2,          start_price = 100,          strike_price = 100,          interest_rate = 0.05,          t_final = 1,      )     y = torch.from_numpy(ao.f(x[...,:d].numpy()))     return y In\u00a0[\u00a0]: Copied! <pre>def muq_beam(level, x): # https://um-bridge-benchmarks.readthedocs.io/en/docs/forward-benchmarks/muq-beam-propagation.html\n    # docker run -it -p 4243:4243 linusseelinger/benchmark-muq-beam-propagation:latest\n    assert level in [0]\n    true_measure = qp.Uniform(qp.IIDStdUniform(3),lower_bound=1,upper_bound=1.05)\n    um_bridge_model = umbridge.HTTPModel('http://localhost:4243','forward')\n    um_bridge_config = {}\n    integrand = qp.UMBridgeWrapper(true_measure,um_bridge_model,um_bridge_config,parallel=10)\n    y = torch.from_numpy(integrand.f(x.numpy()))\n    return y\nif False: # uncomment to generate MUQ data\n    dseq = qp.DigitalNetB2(3,randomize=\"DS\",seed=7)\n    x = torch.from_numpy(dseq(2**15))\n    y = muq_beam(0,x)\n    muq_data = {\"x\":x,\"y\":y}\n    print(muq_data)\n    torch.save(muq_data,\"./muq_data.pt\")\n</pre> def muq_beam(level, x): # https://um-bridge-benchmarks.readthedocs.io/en/docs/forward-benchmarks/muq-beam-propagation.html     # docker run -it -p 4243:4243 linusseelinger/benchmark-muq-beam-propagation:latest     assert level in [0]     true_measure = qp.Uniform(qp.IIDStdUniform(3),lower_bound=1,upper_bound=1.05)     um_bridge_model = umbridge.HTTPModel('http://localhost:4243','forward')     um_bridge_config = {}     integrand = qp.UMBridgeWrapper(true_measure,um_bridge_model,um_bridge_config,parallel=10)     y = torch.from_numpy(integrand.f(x.numpy()))     return y if False: # uncomment to generate MUQ data     dseq = qp.DigitalNetB2(3,randomize=\"DS\",seed=7)     x = torch.from_numpy(dseq(2**15))     y = muq_beam(0,x)     muq_data = {\"x\":x,\"y\":y}     print(muq_data)     torch.save(muq_data,\"./muq_data.pt\") In\u00a0[\u00a0]: Copied! <pre>def cookie(level, x): # https://um-bridge-benchmarks.readthedocs.io/en/docs/forward-benchmarks/cookies-problem-propagation.html\n    # docker run -p 4242:4242 -it linusseelinger/cookies-problem:latest\n    assert level in [1,2,3,4]\n    true_measure = qp.Uniform(qp.IIDStdUniform(8),lower_bound=-0.99,upper_bound=-0.2)\n    um_bridge_model = umbridge.HTTPModel('http://localhost:4242','forward')\n    um_bridge_config = {\"Fidelity\":level}\n    integrand = qp.UMBridgeWrapper(true_measure,um_bridge_model,um_bridge_config,parallel=10)\n    y = torch.from_numpy(integrand.f(x.numpy()))\n    return y\nif False: # uncomment to generate cookie data\n    cookie_data = {}\n    dseqs = [qp.DigitalNetB2(8,randomize=\"DS\",seed=7+i) for i in range(4)]\n    levelops = [1,2,3,4]\n    for l in range(len(levelops)):\n        x = torch.from_numpy(dseqs[l](2**12))\n        y = cookie(levelops[l],x)\n        cookie_data[levelops[l]] = (x,y) \n    print(cookie_data)\n    torch.save(cookie_data,\"./cookie_data.pt\")\n</pre> def cookie(level, x): # https://um-bridge-benchmarks.readthedocs.io/en/docs/forward-benchmarks/cookies-problem-propagation.html     # docker run -p 4242:4242 -it linusseelinger/cookies-problem:latest     assert level in [1,2,3,4]     true_measure = qp.Uniform(qp.IIDStdUniform(8),lower_bound=-0.99,upper_bound=-0.2)     um_bridge_model = umbridge.HTTPModel('http://localhost:4242','forward')     um_bridge_config = {\"Fidelity\":level}     integrand = qp.UMBridgeWrapper(true_measure,um_bridge_model,um_bridge_config,parallel=10)     y = torch.from_numpy(integrand.f(x.numpy()))     return y if False: # uncomment to generate cookie data     cookie_data = {}     dseqs = [qp.DigitalNetB2(8,randomize=\"DS\",seed=7+i) for i in range(4)]     levelops = [1,2,3,4]     for l in range(len(levelops)):         x = torch.from_numpy(dseqs[l](2**12))         y = cookie(levelops[l],x)         cookie_data[levelops[l]] = (x,y)      print(cookie_data)     torch.save(cookie_data,\"./cookie_data.pt\") In\u00a0[5]: Copied! <pre>def cookie_pregen(level, x):\n    assert level in [1,2,3,4]\n    assert x.ndim==2 and x.size(-1)==8\n    cookie_data = torch.load(\"./cookie_data.pt\")\n    x_data,y_data = cookie_data[level]\n    x_data = x_data[:x.size(0)]\n    y_data = y_data[:x.size(0)]\n    assert (x_data==x).all()\n    return y_data\n</pre> def cookie_pregen(level, x):     assert level in [1,2,3,4]     assert x.ndim==2 and x.size(-1)==8     cookie_data = torch.load(\"./cookie_data.pt\")     x_data,y_data = cookie_data[level]     x_data = x_data[:x.size(0)]     y_data = y_data[:x.size(0)]     assert (x_data==x).all()     return y_data In\u00a0[\u00a0]: Copied! <pre>def fit_gps(func, \n        d, \n        levelops, \n        n, \n        iterations, \n        xtest, \n        fit_std_gp, \n        fit_gpytorch_gp, \n        fit_fgpdnet, \n        fit_fgplat, \n        requires_grad_noise, \n        seeds, \n        rank, \n        sgp_threshold, \n        gpt_threshold,\n        verbose,\n        predlevels,\n        compute_muhats,\n        compute_sigmahats,\n        n_gp = None,\n        n_gpt = None,\n        n_fgpdnet = None,\n        n_fgplat = None,\n        kernel_class_fgpdnet = qp.KernelDigShiftInvarCombined,\n        kernel_class_fgplat = qp.KernelShiftInvar\n    ):\n    levels = len(levelops)\n    assert n.shape==(levels,)\n    seqs_dnet = [qp.DigitalNetB2(dimension=d,randomize=\"DS\",seed=seeds[l]) for l in range(levels)]\n    xs_dnet = [torch.from_numpy(seqs_dnet[l](n[l])) for l in range(levels)]\n    ys_dnet = [func(levelops[l],xs_dnet[l]) for l in range(levels)]\n    times = []\n    yhattests = []\n    muhats = []\n    sigmahats = []\n    if fit_std_gp: # standard GP\n        if n_gp is not None:\n            assert n_gp.shape==n.shape and (n_gp&lt;=n).all()\n        else:\n            n_gp = n\n        if n_gp.sum()&gt;sgp_threshold:\n            times.append(np.nan)\n            yhattests.append(torch.nan*torch.empty(len(xtest)) if np.isscalar(predlevels) else torch.nan*torch.empty((len(predlevels),len(xtest))))\n            if compute_muhats:\n                muhats.append(torch.tensor(torch.nan) if np.isscalar(predlevels) else torch.nan*torch.ones(len(predlevels))) \n            if compute_sigmahats:\n                sigmahats.append(torch.nan*torch.empty(len(xtest)) if np.isscalar(predlevels) else torch.nan*torch.empty((len(predlevels),len(xtest))))\n        else:\n            gp = fastgps.StandardGP(\n                kernel=qp.KernelMultiTask(\n                    base_kernel = qp.KernelSquaredExponential(d=d,torchify=True),\n                    num_tasks = levels,\n                    rank_factor = levels if rank is None else rank,),\n                seqs=seqs_dnet,\n                noise = 1e-4,\n                adaptive_nugget = False,\n                requires_grad_noise = requires_grad_noise)\n            gp.add_y_next([ys_dnet[l][:n_gp[l]] for l in range(levels)])\n            t0 = time.perf_counter()\n            data = gp.fit(iterations=iterations,verbose=verbose,store_hists=True)\n            times.append((time.perf_counter()-t0)/len(data[\"iteration\"]))\n            yhattests.append(gp.post_mean(xtest,task=predlevels))\n            if compute_muhats:\n                muhats.append(gp.post_cubature_mean(task=predlevels))\n            if compute_sigmahats:\n                sigmahats.append(torch.sqrt(gp.post_var(xtest,task=predlevels)))\n    if fit_gpytorch_gp: # GPyTorch Multitask GP https://docs.gpytorch.ai/en/v1.6.0/examples/03_Multitask_Exact_GPs/Hadamard_Multitask_GP_Regression.html\n        if n_gpt is not None:\n            assert n_gpt.shape==n.shape and (n_gpt&lt;=n).all()\n        else:\n            n_gpt = n\n        if n_gpt.sum()&gt;gpt_threshold:\n            times.append(np.nan)\n            yhattests.append(torch.nan*torch.empty(len(xtest)) if np.isscalar(predlevels) else torch.nan*torch.empty((len(predlevels),len(xtest))))\n            if compute_muhats:\n                muhats.append(torch.tensor(torch.nan) if np.isscalar(predlevels) else torch.nan*torch.ones(len(predlevels))) \n            if compute_sigmahats:\n                sigmahats.append(torch.nan*torch.empty(len(xtest)) if np.isscalar(predlevels) else torch.nan*torch.empty((len(predlevels),len(xtest))))\n        else:\n            import gpytorch\n            class MultitaskGPModel(gpytorch.models.ExactGP):\n                def __init__(self, train_x, train_y, likelihood):\n                    super(MultitaskGPModel, self).__init__(train_x, train_y, likelihood)\n                    self.mean_module = [gpytorch.means.ConstantMean() for i in range(levels)]\n                    self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel(ard_num_dims=d))\n                    self.task_covar_module = gpytorch.kernels.IndexKernel(num_tasks=levels,rank=levels if rank is None else rank)\n                def forward(self,x,i):\n                    mean_x = torch.stack([self.mean_module[l](x) for l in range(levels)],dim=0)\n                    mean_x = mean_x[i[:,0],torch.arange(len(i))]\n                    covar_x = self.covar_module(x)\n                    covar_i = self.task_covar_module(i)\n                    covar = covar_x.mul(covar_i)\n                    return gpytorch.distributions.MultivariateNormal(mean_x,covar)\n            likelihood = gpytorch.likelihoods.GaussianLikelihood()\n            likelihood.noise = 1e-4\n            if not requires_grad_noise:\n                likelihood.noise_covar.raw_noise.requires_grad_(False)\n            _full_train_i = torch.cat([l*torch.ones((n_gpt[l],1),dtype=int) for l in range(levels)])\n            _full_train_x = torch.cat([xs_dnet[l][:n_gpt[l]] for l in range(levels)])\n            _full_train_y = torch.cat([ys_dnet[l][:n_gpt[l]] for l in range(levels)])\n            gpt = MultitaskGPModel((_full_train_x,_full_train_i),_full_train_y,likelihood)\n            gpt.train(); likelihood.train()\n            optimizer = torch.optim.Rprop(gpt.parameters(),lr=0.1,etas=(0.5,1.2),step_sizes=(0,10))\n            # optimizer = torch.optim.Adam(gpt.parameters(),lr=0.1)\n            mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood,gpt)\n            t0 = time.perf_counter()\n            for i in range(iterations):\n                optimizer.zero_grad()\n                output = gpt(_full_train_x,_full_train_i)\n                loss = -mll(output, _full_train_y)\n                if verbose and (i%verbose==0 or i==iterations-1): print(\"iter: %-7d GPT loss: %.2e\"%(i,loss))\n                loss.backward()\n                optimizer.step()\n            times.append((time.perf_counter()-t0)/iterations)\n            gpt.eval(); likelihood.eval()\n            with torch.no_grad(), gpytorch.settings.fast_pred_var():\n                if np.isscalar(predlevels):\n                    gpt_yhattest_mvngpt = gpt(xtest,predlevels*torch.ones((xtest.size(0),1),dtype=int))\n                    gpt_yhattest = gpt_yhattest_mvngpt.mean\n                    if compute_sigmahats:\n                        sigmahats.append(torch.sqrt(gpt_yhattest_mvngpt.variance))\n                else:\n                    gpt_yhattest_mvns = [gpt(xtest,pl*torch.ones((xtest.size(0),1),dtype=int)) for pl in predlevels]\n                    gpt_yhattest = torch.stack([mvn.mean for mvn in gpt_yhattest_mvns],axis=0)\n                    if compute_sigmahats:\n                        sigmahats.append(torch.sqrt(torch.stack([mvn.variance for mvn in gpt_yhattest_mvns],axis=0)))\n            yhattests.append(gpt_yhattest)\n            if compute_muhats:\n                muhats.append(gpt_yhattest.mean(dim=-1)) # cannot exactly compute the mean, so just take a QMC estimate\n    if fit_fgpdnet: # fast GP Dnet \n        if n_fgpdnet is not None:\n            assert n_fgpdnet.shape==n.shape and (n_fgpdnet&lt;=n).all()\n        else:\n            n_fgpdnet = n\n        fgp_dnet = fastgps.FastGPDigitalNetB2(\n            kernel = qp.KernelMultiTask(\n                base_kernel = kernel_class_fgpdnet(d=d,torchify=True),\n                num_tasks = levels,\n                rank_factor = levels if rank is None else rank,),\n            seqs=seqs_dnet,\n            noise = 1e-4 if requires_grad_noise else 2*qp.util.transforms.EPS64,\n            adaptive_nugget = False,\n            requires_grad_noise = requires_grad_noise,)\n        fgp_dnet.add_y_next([ys_dnet[l][:n_fgpdnet[l]] for l in range(levels)])\n        t0 = time.perf_counter()\n        data = fgp_dnet.fit(iterations=iterations,verbose=verbose,store_hists=True)\n        times.append((time.perf_counter()-t0)/len(data[\"iteration\"]))\n        yhattests.append(fgp_dnet.post_mean(xtest,task=predlevels))\n        if compute_muhats:\n            muhats.append(fgp_dnet.post_cubature_mean(task=predlevels))\n        if compute_sigmahats:\n            sigmahats.append(torch.sqrt(fgp_dnet.post_var(xtest,task=predlevels)))\n    if fit_fgplat: # fast GP lattice\n        if n_fgplat is not None:\n            assert n_fgplat.shape==n.shape and (n_fgplat&lt;=n).all()\n        else:\n            n_fgplat = n\n        seqs_lat = [qp.Lattice(dimension=d) for l in range(levels)]\n        xs_lat = [torch.from_numpy(seqs_dnet[l](n_fgplat[l])) for l in range(levels)]\n        # ys_lat = [func(l,xs_lat[l]) for l in range(levels)]\n        ys_lat = [func(levelops[l],1-2*torch.abs(xs_lat[l]-1/2)) for l in range(levels)]\n        fgp_lat = fastgps.FastGPLattice(\n            kernel = qp.KernelMultiTask(\n                base_kernel = kernel_class_fgplat(d=d,torchify=True,alpha=2),\n                num_tasks = levels,\n                rank_factor = levels if rank is None else rank),\n            seqs = seqs_lat,\n            noise = 1e-4 if requires_grad_noise else 2*qp.util.transforms.EPS64,\n            adaptive_nugget = False,\n            requires_grad_noise = requires_grad_noise,\n            )\n        fgp_lat.add_y_next([ys_lat[l][:n_fgplat[l]] for l in range(levels)])\n        t0 = time.perf_counter()\n        data = fgp_lat.fit(iterations=iterations,verbose=verbose,store_hists=True)\n        times.append((time.perf_counter()-t0)/len(data[\"iteration\"]))\n        # fgp_lat_yhattest = fgp.post_mean(xtest,task=levels-1)\n        yhattests.append(1/2*(fgp_lat.post_mean(xtest/2,task=predlevels)+fgp_lat.post_mean(1-xtest/2,task=predlevels)))\n        muhats.append(fgp_lat.post_cubature_mean(task=predlevels))\n        if compute_muhats:\n            muhats.append(fgp_lat.post_cubature_mean(task=predlevels))\n        if compute_sigmahats:\n            sigmahats.append(torch.nan*torch.empty(torch.nan*torch.empty(len(xtest)) if np.isscalar(predlevels) else torch.nan*torch.empty((len(predlevels),len(xtest)))))\n    return torch.tensor(times),torch.stack(yhattests,dim=0),(torch.stack(muhats,dim=0) if compute_muhats else None),(torch.stack(sigmahats,dim=0) if compute_sigmahats else None)\n                                                                                                                     \ndef run(\n        func, \n        d, \n        levelops, \n        ns, \n        iterations = 100, \n        trials = 1, \n        fit_std_gp = True, \n        fit_gpytorch_gp = True, \n        fit_fgpdnet = True, \n        fit_fgplat = False, \n        requires_grad_noise = False, \n        n_ref = 2**18, \n        n_test = 2**11, \n        ref_mu = None, \n        seeds = None, \n        test_data = None, \n        tf_output = lambda x: x,\n        rank = None,\n        sgp_threshold = 2**11,\n        gpt_threshold = 2**12,\n        verbose = False):\n    levels = len(levelops)\n    assert ns.ndim==2 and ns.size(1)==levels\n    assert n_ref&gt;=n_test\n    num_ns = len(ns)\n    num_gptypes = int(fit_std_gp)+int(fit_gpytorch_gp)+int(fit_fgpdnet)+int(fit_fgplat)\n    gptypes = []\n    if fit_std_gp: gptypes.append(\"Cholesky SE\")\n    if fit_gpytorch_gp: gptypes.append(\"CG SE (GPyTorch)\")\n    if fit_fgpdnet: gptypes.append(\"Fast DSI (ours)\")\n    if fit_fgplat: gptypes.append(\"Fast SI Lat (ours)\")\n    if test_data is None:\n        xtest = torch.from_numpy(qp.Halton(dimension=d,randomize=\"QRNG\",seed=11)(n_ref))\n        ytest = func(levelops[-1],xtest)\n    else:\n        xtest,ytest = test_data\n    if ref_mu is None:\n        mu = ytest.mean() # true mean computed with large-sample QMC estimate\n    else:\n        mu = ref_mu\n    times = torch.empty((num_gptypes,num_ns,trials))\n    l2rerrors = torch.empty((num_gptypes,num_ns,trials))\n    bcerrors = torch.empty((num_gptypes,num_ns,trials))\n    if seeds is None: seeds = [None]*levels\n    print(\"\\tgptypes:\",gptypes)\n    print()\n    print(\"\\tns\")\n    for i in range(num_ns): print(\"\\t\\t%d: %s\"%(i,ns[i].numpy()))\n    print()\n    print(\"\\t~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n    print()\n    t0 = time.perf_counter()\n    for i in range(num_ns):\n        print(\"\\tns[%d] = %s\"%(i,ns[i].numpy()))\n        print(\"\\t\\t%-10s %-20s %-35s %-35s %-35s\"%(\"trial\",\"clock\",\"times\",\"l2rerrors\",\"bcerrors\"))\n        for j in range(trials):\n            times[:,i,j],yhattest,muhat,_ = fit_gps(func,d,levelops,ns[i],iterations,xtest[:n_test],fit_std_gp,fit_gpytorch_gp,fit_fgpdnet,fit_fgplat,requires_grad_noise,seeds,rank,sgp_threshold,gpt_threshold,verbose,predlevels=levels-1,compute_muhats=True,compute_sigmahats=False)\n            yhattest = tf_output(yhattest)\n            l2rerrors[:,i,j] = torch.linalg.norm(yhattest-ytest[:n_test],dim=-1)/torch.linalg.norm(ytest[:n_test])\n            bcerrors[:,i,j] = torch.abs(muhat-mu)\n            with np.printoptions(formatter={\"float\":lambda x: \"%.1e\"%x}):\n                print(\"\\t\\t%-10d %-20.1f %-35s %-35s %-35s\"%(j,time.perf_counter()-t0,times[:,i,j].numpy(),l2rerrors[:,i,j].numpy(),bcerrors[:,i,j].numpy()))\n        print()\n    return gptypes,times,l2rerrors,bcerrors\n</pre> def fit_gps(func,          d,          levelops,          n,          iterations,          xtest,          fit_std_gp,          fit_gpytorch_gp,          fit_fgpdnet,          fit_fgplat,          requires_grad_noise,          seeds,          rank,          sgp_threshold,          gpt_threshold,         verbose,         predlevels,         compute_muhats,         compute_sigmahats,         n_gp = None,         n_gpt = None,         n_fgpdnet = None,         n_fgplat = None,         kernel_class_fgpdnet = qp.KernelDigShiftInvarCombined,         kernel_class_fgplat = qp.KernelShiftInvar     ):     levels = len(levelops)     assert n.shape==(levels,)     seqs_dnet = [qp.DigitalNetB2(dimension=d,randomize=\"DS\",seed=seeds[l]) for l in range(levels)]     xs_dnet = [torch.from_numpy(seqs_dnet[l](n[l])) for l in range(levels)]     ys_dnet = [func(levelops[l],xs_dnet[l]) for l in range(levels)]     times = []     yhattests = []     muhats = []     sigmahats = []     if fit_std_gp: # standard GP         if n_gp is not None:             assert n_gp.shape==n.shape and (n_gp&lt;=n).all()         else:             n_gp = n         if n_gp.sum()&gt;sgp_threshold:             times.append(np.nan)             yhattests.append(torch.nan*torch.empty(len(xtest)) if np.isscalar(predlevels) else torch.nan*torch.empty((len(predlevels),len(xtest))))             if compute_muhats:                 muhats.append(torch.tensor(torch.nan) if np.isscalar(predlevels) else torch.nan*torch.ones(len(predlevels)))              if compute_sigmahats:                 sigmahats.append(torch.nan*torch.empty(len(xtest)) if np.isscalar(predlevels) else torch.nan*torch.empty((len(predlevels),len(xtest))))         else:             gp = fastgps.StandardGP(                 kernel=qp.KernelMultiTask(                     base_kernel = qp.KernelSquaredExponential(d=d,torchify=True),                     num_tasks = levels,                     rank_factor = levels if rank is None else rank,),                 seqs=seqs_dnet,                 noise = 1e-4,                 adaptive_nugget = False,                 requires_grad_noise = requires_grad_noise)             gp.add_y_next([ys_dnet[l][:n_gp[l]] for l in range(levels)])             t0 = time.perf_counter()             data = gp.fit(iterations=iterations,verbose=verbose,store_hists=True)             times.append((time.perf_counter()-t0)/len(data[\"iteration\"]))             yhattests.append(gp.post_mean(xtest,task=predlevels))             if compute_muhats:                 muhats.append(gp.post_cubature_mean(task=predlevels))             if compute_sigmahats:                 sigmahats.append(torch.sqrt(gp.post_var(xtest,task=predlevels)))     if fit_gpytorch_gp: # GPyTorch Multitask GP https://docs.gpytorch.ai/en/v1.6.0/examples/03_Multitask_Exact_GPs/Hadamard_Multitask_GP_Regression.html         if n_gpt is not None:             assert n_gpt.shape==n.shape and (n_gpt&lt;=n).all()         else:             n_gpt = n         if n_gpt.sum()&gt;gpt_threshold:             times.append(np.nan)             yhattests.append(torch.nan*torch.empty(len(xtest)) if np.isscalar(predlevels) else torch.nan*torch.empty((len(predlevels),len(xtest))))             if compute_muhats:                 muhats.append(torch.tensor(torch.nan) if np.isscalar(predlevels) else torch.nan*torch.ones(len(predlevels)))              if compute_sigmahats:                 sigmahats.append(torch.nan*torch.empty(len(xtest)) if np.isscalar(predlevels) else torch.nan*torch.empty((len(predlevels),len(xtest))))         else:             import gpytorch             class MultitaskGPModel(gpytorch.models.ExactGP):                 def __init__(self, train_x, train_y, likelihood):                     super(MultitaskGPModel, self).__init__(train_x, train_y, likelihood)                     self.mean_module = [gpytorch.means.ConstantMean() for i in range(levels)]                     self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel(ard_num_dims=d))                     self.task_covar_module = gpytorch.kernels.IndexKernel(num_tasks=levels,rank=levels if rank is None else rank)                 def forward(self,x,i):                     mean_x = torch.stack([self.mean_module[l](x) for l in range(levels)],dim=0)                     mean_x = mean_x[i[:,0],torch.arange(len(i))]                     covar_x = self.covar_module(x)                     covar_i = self.task_covar_module(i)                     covar = covar_x.mul(covar_i)                     return gpytorch.distributions.MultivariateNormal(mean_x,covar)             likelihood = gpytorch.likelihoods.GaussianLikelihood()             likelihood.noise = 1e-4             if not requires_grad_noise:                 likelihood.noise_covar.raw_noise.requires_grad_(False)             _full_train_i = torch.cat([l*torch.ones((n_gpt[l],1),dtype=int) for l in range(levels)])             _full_train_x = torch.cat([xs_dnet[l][:n_gpt[l]] for l in range(levels)])             _full_train_y = torch.cat([ys_dnet[l][:n_gpt[l]] for l in range(levels)])             gpt = MultitaskGPModel((_full_train_x,_full_train_i),_full_train_y,likelihood)             gpt.train(); likelihood.train()             optimizer = torch.optim.Rprop(gpt.parameters(),lr=0.1,etas=(0.5,1.2),step_sizes=(0,10))             # optimizer = torch.optim.Adam(gpt.parameters(),lr=0.1)             mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood,gpt)             t0 = time.perf_counter()             for i in range(iterations):                 optimizer.zero_grad()                 output = gpt(_full_train_x,_full_train_i)                 loss = -mll(output, _full_train_y)                 if verbose and (i%verbose==0 or i==iterations-1): print(\"iter: %-7d GPT loss: %.2e\"%(i,loss))                 loss.backward()                 optimizer.step()             times.append((time.perf_counter()-t0)/iterations)             gpt.eval(); likelihood.eval()             with torch.no_grad(), gpytorch.settings.fast_pred_var():                 if np.isscalar(predlevels):                     gpt_yhattest_mvngpt = gpt(xtest,predlevels*torch.ones((xtest.size(0),1),dtype=int))                     gpt_yhattest = gpt_yhattest_mvngpt.mean                     if compute_sigmahats:                         sigmahats.append(torch.sqrt(gpt_yhattest_mvngpt.variance))                 else:                     gpt_yhattest_mvns = [gpt(xtest,pl*torch.ones((xtest.size(0),1),dtype=int)) for pl in predlevels]                     gpt_yhattest = torch.stack([mvn.mean for mvn in gpt_yhattest_mvns],axis=0)                     if compute_sigmahats:                         sigmahats.append(torch.sqrt(torch.stack([mvn.variance for mvn in gpt_yhattest_mvns],axis=0)))             yhattests.append(gpt_yhattest)             if compute_muhats:                 muhats.append(gpt_yhattest.mean(dim=-1)) # cannot exactly compute the mean, so just take a QMC estimate     if fit_fgpdnet: # fast GP Dnet          if n_fgpdnet is not None:             assert n_fgpdnet.shape==n.shape and (n_fgpdnet&lt;=n).all()         else:             n_fgpdnet = n         fgp_dnet = fastgps.FastGPDigitalNetB2(             kernel = qp.KernelMultiTask(                 base_kernel = kernel_class_fgpdnet(d=d,torchify=True),                 num_tasks = levels,                 rank_factor = levels if rank is None else rank,),             seqs=seqs_dnet,             noise = 1e-4 if requires_grad_noise else 2*qp.util.transforms.EPS64,             adaptive_nugget = False,             requires_grad_noise = requires_grad_noise,)         fgp_dnet.add_y_next([ys_dnet[l][:n_fgpdnet[l]] for l in range(levels)])         t0 = time.perf_counter()         data = fgp_dnet.fit(iterations=iterations,verbose=verbose,store_hists=True)         times.append((time.perf_counter()-t0)/len(data[\"iteration\"]))         yhattests.append(fgp_dnet.post_mean(xtest,task=predlevels))         if compute_muhats:             muhats.append(fgp_dnet.post_cubature_mean(task=predlevels))         if compute_sigmahats:             sigmahats.append(torch.sqrt(fgp_dnet.post_var(xtest,task=predlevels)))     if fit_fgplat: # fast GP lattice         if n_fgplat is not None:             assert n_fgplat.shape==n.shape and (n_fgplat&lt;=n).all()         else:             n_fgplat = n         seqs_lat = [qp.Lattice(dimension=d) for l in range(levels)]         xs_lat = [torch.from_numpy(seqs_dnet[l](n_fgplat[l])) for l in range(levels)]         # ys_lat = [func(l,xs_lat[l]) for l in range(levels)]         ys_lat = [func(levelops[l],1-2*torch.abs(xs_lat[l]-1/2)) for l in range(levels)]         fgp_lat = fastgps.FastGPLattice(             kernel = qp.KernelMultiTask(                 base_kernel = kernel_class_fgplat(d=d,torchify=True,alpha=2),                 num_tasks = levels,                 rank_factor = levels if rank is None else rank),             seqs = seqs_lat,             noise = 1e-4 if requires_grad_noise else 2*qp.util.transforms.EPS64,             adaptive_nugget = False,             requires_grad_noise = requires_grad_noise,             )         fgp_lat.add_y_next([ys_lat[l][:n_fgplat[l]] for l in range(levels)])         t0 = time.perf_counter()         data = fgp_lat.fit(iterations=iterations,verbose=verbose,store_hists=True)         times.append((time.perf_counter()-t0)/len(data[\"iteration\"]))         # fgp_lat_yhattest = fgp.post_mean(xtest,task=levels-1)         yhattests.append(1/2*(fgp_lat.post_mean(xtest/2,task=predlevels)+fgp_lat.post_mean(1-xtest/2,task=predlevels)))         muhats.append(fgp_lat.post_cubature_mean(task=predlevels))         if compute_muhats:             muhats.append(fgp_lat.post_cubature_mean(task=predlevels))         if compute_sigmahats:             sigmahats.append(torch.nan*torch.empty(torch.nan*torch.empty(len(xtest)) if np.isscalar(predlevels) else torch.nan*torch.empty((len(predlevels),len(xtest)))))     return torch.tensor(times),torch.stack(yhattests,dim=0),(torch.stack(muhats,dim=0) if compute_muhats else None),(torch.stack(sigmahats,dim=0) if compute_sigmahats else None)                                                                                                                       def run(         func,          d,          levelops,          ns,          iterations = 100,          trials = 1,          fit_std_gp = True,          fit_gpytorch_gp = True,          fit_fgpdnet = True,          fit_fgplat = False,          requires_grad_noise = False,          n_ref = 2**18,          n_test = 2**11,          ref_mu = None,          seeds = None,          test_data = None,          tf_output = lambda x: x,         rank = None,         sgp_threshold = 2**11,         gpt_threshold = 2**12,         verbose = False):     levels = len(levelops)     assert ns.ndim==2 and ns.size(1)==levels     assert n_ref&gt;=n_test     num_ns = len(ns)     num_gptypes = int(fit_std_gp)+int(fit_gpytorch_gp)+int(fit_fgpdnet)+int(fit_fgplat)     gptypes = []     if fit_std_gp: gptypes.append(\"Cholesky SE\")     if fit_gpytorch_gp: gptypes.append(\"CG SE (GPyTorch)\")     if fit_fgpdnet: gptypes.append(\"Fast DSI (ours)\")     if fit_fgplat: gptypes.append(\"Fast SI Lat (ours)\")     if test_data is None:         xtest = torch.from_numpy(qp.Halton(dimension=d,randomize=\"QRNG\",seed=11)(n_ref))         ytest = func(levelops[-1],xtest)     else:         xtest,ytest = test_data     if ref_mu is None:         mu = ytest.mean() # true mean computed with large-sample QMC estimate     else:         mu = ref_mu     times = torch.empty((num_gptypes,num_ns,trials))     l2rerrors = torch.empty((num_gptypes,num_ns,trials))     bcerrors = torch.empty((num_gptypes,num_ns,trials))     if seeds is None: seeds = [None]*levels     print(\"\\tgptypes:\",gptypes)     print()     print(\"\\tns\")     for i in range(num_ns): print(\"\\t\\t%d: %s\"%(i,ns[i].numpy()))     print()     print(\"\\t~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")     print()     t0 = time.perf_counter()     for i in range(num_ns):         print(\"\\tns[%d] = %s\"%(i,ns[i].numpy()))         print(\"\\t\\t%-10s %-20s %-35s %-35s %-35s\"%(\"trial\",\"clock\",\"times\",\"l2rerrors\",\"bcerrors\"))         for j in range(trials):             times[:,i,j],yhattest,muhat,_ = fit_gps(func,d,levelops,ns[i],iterations,xtest[:n_test],fit_std_gp,fit_gpytorch_gp,fit_fgpdnet,fit_fgplat,requires_grad_noise,seeds,rank,sgp_threshold,gpt_threshold,verbose,predlevels=levels-1,compute_muhats=True,compute_sigmahats=False)             yhattest = tf_output(yhattest)             l2rerrors[:,i,j] = torch.linalg.norm(yhattest-ytest[:n_test],dim=-1)/torch.linalg.norm(ytest[:n_test])             bcerrors[:,i,j] = torch.abs(muhat-mu)             with np.printoptions(formatter={\"float\":lambda x: \"%.1e\"%x}):                 print(\"\\t\\t%-10d %-20.1f %-35s %-35s %-35s\"%(j,time.perf_counter()-t0,times[:,i,j].numpy(),l2rerrors[:,i,j].numpy(),bcerrors[:,i,j].numpy()))         print()     return gptypes,times,l2rerrors,bcerrors In\u00a0[\u00a0]: Copied! <pre>def ackley3(level, x): # https://www.sfu.ca/~ssurjano/ackley.html\n    assert level in [0,1,2]\n    a = 20\n    b = 0.2 \n    if level==0:\n        c = 0 \n    elif level==1:\n        c = np.pi/2\n    elif level==2:\n        c = 2*np.pi\n    t = 2*32.768*x-32.768\n    d = x.size(-1)\n    y = -a*torch.exp(-b*torch.linalg.norm(t,dim=-1))-torch.exp(1/d*torch.cos(c*t).sum(-1))+a+np.exp(1)\n    return y\nALPHA = 0.25\nseeds = [11,12,13]\nns = [32,16,8]\nxticks = torch.linspace(0,1,100)[1:-1,None]\nyticks = torch.stack([ackley3(l,xticks) for l in range(3)],dim=0)\nfig,ax = pyplot.subplots(nrows=1,ncols=4,figsize=(PW,PW/4))\ntimes,yhattests,muhats,sigmahats = fit_gps(\n    func = ackley3,\n    d = 1,\n    levelops = [0,1,2],\n    n = torch.tensor(ns,dtype=int),\n    iterations = 100,\n    xtest = xticks,\n    fit_std_gp = True,\n    fit_gpytorch_gp = True,\n    fit_fgpdnet = True,\n    fit_fgplat = False, \n    requires_grad_noise = False,\n    seeds = seeds,\n    rank = 3,\n    sgp_threshold = 2**10, \n    gpt_threshold = 2**10,\n    verbose = 25, \n    predlevels = [0,1,2],\n    compute_muhats = True, \n    compute_sigmahats = True, \n)\nxs = [torch.from_numpy(qp.DigitalNetB2(1,seed=seeds[l],randomize=\"DS\")(ns[l])) for l in range(3)]\nys = [ackley3(l,xs[l]) for l in range(3)]\nprint(yhattests.shape)\nprint(sigmahats.shape)\nfor l in range(3):\n    ax[l].scatter(xs[l],ys[l],color=\"k\")\n    ax[l].plot(xticks,yticks[l],color=\"k\")\n    ax[l].plot(xticks,yhattests[-1,l],color=COLORS[2])\n    ax[l].fill_between(xticks[:,0],yhattests[-1,l]-sigmahats[-1,l],yhattests[-1,l]+sigmahats[-1,l],color=COLORS[2],alpha=ALPHA)\n</pre> def ackley3(level, x): # https://www.sfu.ca/~ssurjano/ackley.html     assert level in [0,1,2]     a = 20     b = 0.2      if level==0:         c = 0      elif level==1:         c = np.pi/2     elif level==2:         c = 2*np.pi     t = 2*32.768*x-32.768     d = x.size(-1)     y = -a*torch.exp(-b*torch.linalg.norm(t,dim=-1))-torch.exp(1/d*torch.cos(c*t).sum(-1))+a+np.exp(1)     return y ALPHA = 0.25 seeds = [11,12,13] ns = [32,16,8] xticks = torch.linspace(0,1,100)[1:-1,None] yticks = torch.stack([ackley3(l,xticks) for l in range(3)],dim=0) fig,ax = pyplot.subplots(nrows=1,ncols=4,figsize=(PW,PW/4)) times,yhattests,muhats,sigmahats = fit_gps(     func = ackley3,     d = 1,     levelops = [0,1,2],     n = torch.tensor(ns,dtype=int),     iterations = 100,     xtest = xticks,     fit_std_gp = True,     fit_gpytorch_gp = True,     fit_fgpdnet = True,     fit_fgplat = False,      requires_grad_noise = False,     seeds = seeds,     rank = 3,     sgp_threshold = 2**10,      gpt_threshold = 2**10,     verbose = 25,      predlevels = [0,1,2],     compute_muhats = True,      compute_sigmahats = True,  ) xs = [torch.from_numpy(qp.DigitalNetB2(1,seed=seeds[l],randomize=\"DS\")(ns[l])) for l in range(3)] ys = [ackley3(l,xs[l]) for l in range(3)] print(yhattests.shape) print(sigmahats.shape) for l in range(3):     ax[l].scatter(xs[l],ys[l],color=\"k\")     ax[l].plot(xticks,yticks[l],color=\"k\")     ax[l].plot(xticks,yhattests[-1,l],color=COLORS[2])     ax[l].fill_between(xticks[:,0],yhattests[-1,l]-sigmahats[-1,l],yhattests[-1,l]+sigmahats[-1,l],color=COLORS[2],alpha=ALPHA) <pre>     iter of 1.0e+02 | best loss  | loss      \n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n            0.00e+00 | 2.81e+06   | 2.81e+06  \n            2.50e+01 | 1.61e+02   | 1.61e+02  \n            5.00e+01 | 1.02e+02   | 1.02e+02  \n            7.50e+01 | 8.76e+01   | 8.76e+01  \n            8.90e+01 | 8.75e+01   | 8.75e+01  \niter: 0       GPT loss: 4.10e+04\niter: 25      GPT loss: 2.76e+00\niter: 50      GPT loss: 2.24e+00\niter: 75      GPT loss: 2.24e+00\niter: 99      GPT loss: 2.24e+00\n     iter of 1.0e+02 | best loss  | loss      \n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n            0.00e+00 | 1.71e+02   | 1.71e+02  \n            2.50e+01 | 1.10e+02   | 1.10e+02  \n            5.00e+01 | 1.08e+02   | 1.08e+02  \n            7.50e+01 | 1.06e+02   | 1.06e+02  \n            1.00e+02 | 1.05e+02   | 1.05e+02  \ntorch.Size([3, 3, 98])\ntorch.Size([3, 3, 98])\n</pre> In\u00a0[\u00a0]: Copied! <pre>def rosenbrock(level, x): # https://link.springer.com/article/10.1007/s40722-022-00268-5\n    assert level in [0,1,2]\n    assert x.size(-1)==2\n    x = 2*2.048*x-2.048\n    x1,x2 = x[...,0],x[...,1]\n    if level==2:\n        return 100*(x2-x1**2)**2+(1-x1)**2\n    elif level==1:\n        return 50*(x2-x1**2)**2+(-2-x1)**2-80-0.5*x1*0.5*x2\n    elif level==0:\n        return ((100*(x2-x1**2)**2+(1-x1)**2)-4-0.5*x1-0.5*x2)/(10+0.25*x1+0.25*x2)\nALPHA = 0.25\n# seeds = [11,12,13]\nns = [2**15,2**14,2**13]\n# ns = [2**10,2**9,2**8]\nx0mesh,x1mesh = torch.meshgrid(torch.linspace(0,1,100)[1:-1],torch.linspace(0,1,100)[1:-1],indexing=\"ij\")\nxticks = torch.stack([x0mesh.flatten(),x1mesh.flatten()],axis=-1)\nyticks = torch.stack([rosenbrock(l,xticks) for l in range(3)],dim=0)\nfig = pyplot.figure(figsize=(PW,PW/3*2))\nax = []\nax.append(fig.add_subplot(131,projection='3d'))\nax.append(fig.add_subplot(132,projection='3d',sharex=ax[0],sharey=ax[0]))#,sharez=ax[0]))\nax.append(fig.add_subplot(133,projection='3d',sharex=ax[0],sharey=ax[0]))#,sharez=ax[0]))\nax = np.array(ax)\ntimes,yhattests,muhats,sigmahats = fit_gps(\n    func = rosenbrock,\n    d = 2,\n    levelops = [0,1,2],\n    n = torch.tensor(ns,dtype=int),\n    iterations = 1000,\n    xtest = xticks,\n    fit_std_gp = False,\n    fit_gpytorch_gp = False,\n    fit_fgpdnet = True,\n    fit_fgplat = False, \n    requires_grad_noise = False,\n    seeds = seeds,\n    rank = 3,\n    sgp_threshold = 2**10, \n    gpt_threshold = 2**10,\n    verbose = 25, \n    predlevels = [0,1,2],\n    compute_muhats = False, \n    compute_sigmahats = False, \n    # kernel_class_fgpdnet = qp.KernelDigShiftInvar\n)\nxs = [torch.from_numpy(qp.DigitalNetB2(2,seed=seeds[l],randomize=\"DS\")(ns[l])) for l in range(3)]\nys = [rosenbrock(l,xs[l]) for l in range(3)]\nl2rerrors = [torch.linalg.norm(yticks[l]-yhattests[-1,l])/torch.linalg.norm(yticks[l]) for l in range(3)]\nprint(yhattests.shape)\nfidelities = [\"low\",\"medium\",\"high\"]\ncmap = sns.color_palette(\"Blues_r\", as_cmap=True) # https://seaborn.pydata.org/tutorial/color_palettes.html\n# cmap = sns.color_palette(\"mako\", as_cmap=True)\nfor l in range(3):\n    ax[l].plot_surface(x0mesh,x1mesh,yhattests[-1,l].reshape(x0mesh.shape),rstride=1,cstride=1,antialiased=False,linewidth=0,cmap=cmap)\n    # ax[l].scatter(xs[l][:,0],xs[l][:,1],ys[l],color=\"k\")\n    ax[l].set_xlabel(r\"$x_1$\",labelpad=15)\n    ax[l].set_ylabel(r\"$x_2$\",labelpad=15)\n    ax[l].set_xlim([0,1])\n    ax[l].set_xticks([0,1/4,1/2,3/4,1])\n    ax[l].set_xticklabels([r\"$0$\",\"\",\"\",\"\",r\"$1$\"])\n    ax[l].set_ylim([0,1])\n    ax[l].set_yticks([0,1/4,1/2,3/4,1])\n    ax[l].set_yticklabels([r\"$0$\",\"\",\"\",\"\",r\"$1$\"])\n    ax[l].tick_params(axis='z',pad=25)\n    ax[l].view_init(azim=115)\n    ax[l].xaxis.pane.fill = False\n    ax[l].yaxis.pane.fill = False\n    ax[l].zaxis.pane.fill = False\nfor l in range(3):\n    ax[l].set_zlim(ax[-1].get_zlim())\n    if l!=2:\n        ax[l].set_zticklabels([\"\" for i in range(len(ax[l].get_zticks()))])\n    ax[l].xaxis.pane.set_edgecolor('k')\n    ax[l].yaxis.pane.set_edgecolor('k')\n    ax[l].zaxis.pane.set_edgecolor('k')\n    # ax[l].xaxis.pane.set_linewidth(5)\n    # ax[l].yaxis.pane.set_linewidth(5)\n    # ax[l].zaxis.pane.set_linewidth(5)\n    ax[l].set_title(r\"%s-fidelity\"%(fidelities[l]))#+\"\\n\"+r\"$L_2$ relative error = %.2f%%\"%(100*l2rerrors[l]))\nfig.suptitle(\"Fast multitask Gaussian process (MTGP) ~ digital sequences + digitally-shift-invariant kernels\\n\"+r\"$N = %d, \\qquad \\boldsymbol{n} = [%d, %d, %d], \\qquad L_2$ rel errors = [%.1e, %.1e, %.1e],$\\qquad$\"%(sum(ns),ns[0],ns[1],ns[2],l2rerrors[0],l2rerrors[1],l2rerrors[2])+\"time per opt step = %.1e (sec)\"%times[-1],x=0.53,y=.75)\nfig.savefig(\"./rosenbrock.png\",bbox_inches=\"tight\",dpi=512,transparent=True)\n</pre> def rosenbrock(level, x): # https://link.springer.com/article/10.1007/s40722-022-00268-5     assert level in [0,1,2]     assert x.size(-1)==2     x = 2*2.048*x-2.048     x1,x2 = x[...,0],x[...,1]     if level==2:         return 100*(x2-x1**2)**2+(1-x1)**2     elif level==1:         return 50*(x2-x1**2)**2+(-2-x1)**2-80-0.5*x1*0.5*x2     elif level==0:         return ((100*(x2-x1**2)**2+(1-x1)**2)-4-0.5*x1-0.5*x2)/(10+0.25*x1+0.25*x2) ALPHA = 0.25 # seeds = [11,12,13] ns = [2**15,2**14,2**13] # ns = [2**10,2**9,2**8] x0mesh,x1mesh = torch.meshgrid(torch.linspace(0,1,100)[1:-1],torch.linspace(0,1,100)[1:-1],indexing=\"ij\") xticks = torch.stack([x0mesh.flatten(),x1mesh.flatten()],axis=-1) yticks = torch.stack([rosenbrock(l,xticks) for l in range(3)],dim=0) fig = pyplot.figure(figsize=(PW,PW/3*2)) ax = [] ax.append(fig.add_subplot(131,projection='3d')) ax.append(fig.add_subplot(132,projection='3d',sharex=ax[0],sharey=ax[0]))#,sharez=ax[0])) ax.append(fig.add_subplot(133,projection='3d',sharex=ax[0],sharey=ax[0]))#,sharez=ax[0])) ax = np.array(ax) times,yhattests,muhats,sigmahats = fit_gps(     func = rosenbrock,     d = 2,     levelops = [0,1,2],     n = torch.tensor(ns,dtype=int),     iterations = 1000,     xtest = xticks,     fit_std_gp = False,     fit_gpytorch_gp = False,     fit_fgpdnet = True,     fit_fgplat = False,      requires_grad_noise = False,     seeds = seeds,     rank = 3,     sgp_threshold = 2**10,      gpt_threshold = 2**10,     verbose = 25,      predlevels = [0,1,2],     compute_muhats = False,      compute_sigmahats = False,      # kernel_class_fgpdnet = qp.KernelDigShiftInvar ) xs = [torch.from_numpy(qp.DigitalNetB2(2,seed=seeds[l],randomize=\"DS\")(ns[l])) for l in range(3)] ys = [rosenbrock(l,xs[l]) for l in range(3)] l2rerrors = [torch.linalg.norm(yticks[l]-yhattests[-1,l])/torch.linalg.norm(yticks[l]) for l in range(3)] print(yhattests.shape) fidelities = [\"low\",\"medium\",\"high\"] cmap = sns.color_palette(\"Blues_r\", as_cmap=True) # https://seaborn.pydata.org/tutorial/color_palettes.html # cmap = sns.color_palette(\"mako\", as_cmap=True) for l in range(3):     ax[l].plot_surface(x0mesh,x1mesh,yhattests[-1,l].reshape(x0mesh.shape),rstride=1,cstride=1,antialiased=False,linewidth=0,cmap=cmap)     # ax[l].scatter(xs[l][:,0],xs[l][:,1],ys[l],color=\"k\")     ax[l].set_xlabel(r\"$x_1$\",labelpad=15)     ax[l].set_ylabel(r\"$x_2$\",labelpad=15)     ax[l].set_xlim([0,1])     ax[l].set_xticks([0,1/4,1/2,3/4,1])     ax[l].set_xticklabels([r\"$0$\",\"\",\"\",\"\",r\"$1$\"])     ax[l].set_ylim([0,1])     ax[l].set_yticks([0,1/4,1/2,3/4,1])     ax[l].set_yticklabels([r\"$0$\",\"\",\"\",\"\",r\"$1$\"])     ax[l].tick_params(axis='z',pad=25)     ax[l].view_init(azim=115)     ax[l].xaxis.pane.fill = False     ax[l].yaxis.pane.fill = False     ax[l].zaxis.pane.fill = False for l in range(3):     ax[l].set_zlim(ax[-1].get_zlim())     if l!=2:         ax[l].set_zticklabels([\"\" for i in range(len(ax[l].get_zticks()))])     ax[l].xaxis.pane.set_edgecolor('k')     ax[l].yaxis.pane.set_edgecolor('k')     ax[l].zaxis.pane.set_edgecolor('k')     # ax[l].xaxis.pane.set_linewidth(5)     # ax[l].yaxis.pane.set_linewidth(5)     # ax[l].zaxis.pane.set_linewidth(5)     ax[l].set_title(r\"%s-fidelity\"%(fidelities[l]))#+\"\\n\"+r\"$L_2$ relative error = %.2f%%\"%(100*l2rerrors[l])) fig.suptitle(\"Fast multitask Gaussian process (MTGP) ~ digital sequences + digitally-shift-invariant kernels\\n\"+r\"$N = %d, \\qquad \\boldsymbol{n} = [%d, %d, %d], \\qquad L_2$ rel errors = [%.1e, %.1e, %.1e],$\\qquad$\"%(sum(ns),ns[0],ns[1],ns[2],l2rerrors[0],l2rerrors[1],l2rerrors[2])+\"time per opt step = %.1e (sec)\"%times[-1],x=0.53,y=.75) fig.savefig(\"./rosenbrock.png\",bbox_inches=\"tight\",dpi=512,transparent=True) <pre>     iter of 1.0e+03 | best loss  | loss      \n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n            0.00e+00 | 1.68e+07   | 1.68e+07  \n            2.50e+01 | 6.03e+04   | 6.03e+04  \n            5.00e+01 | 5.12e+04   | 5.12e+04  \n            7.50e+01 | 4.94e+04   | 4.94e+04  \n            1.00e+02 | 4.91e+04   | 4.91e+04  \n            1.25e+02 | 4.90e+04   | 4.90e+04  \n            1.50e+02 | 4.88e+04   | 4.88e+04  \n            1.75e+02 | 4.87e+04   | 4.87e+04  \n            1.92e+02 | 4.86e+04   | 4.86e+04  \ntorch.Size([1, 3, 9604])\n</pre> In\u00a0[\u00a0]: Copied! <pre>ns = torch.tensor([\n    # [2**15,2**15],\n    # [2**14,2**14],\n    # [2**13,2**13],\n    # [2**12,2**12],\n    # [2**11,2**11],\n    # [2**10,2**10],\n    # [2**9,2**9],\n    # [2**8,2**8],\n    [2**7,2**7],\n    [2**6,2**6],\n    [2**5,2**5],\n    [2**4,2**4]\n    ],dtype=int)\ngptypes,times,l2rerrors,bcerrors = run(currin,2,[0,1],ns)\nwith np.printoptions(formatter={\"float\":lambda x: \"%-7.1e\"%x},linewidth=np.inf):\n    print()\n    print(\"times.shape = %s\"%str(tuple(times.shape)))\n    print(\"times.median(dim=-1).values\")\n    print(times.nanmedian(dim=-1).values.numpy())\n    print()\n    print(\"l2rerrors.shape = %s\"%str(tuple(l2rerrors.shape)))\n    print(\"l2rerrors.median(dim=-1).values\")\n    print(l2rerrors.nanmedian(dim=-1).values.numpy())\n    print()\n    print(\"bcerrors.shape = %s\"%str(tuple(bcerrors.shape)))\n    print(\"bcerrors.median(dim=-1).values\")\n    print(bcerrors.nanmedian(dim=-1).values.numpy())\n</pre> ns = torch.tensor([     # [2**15,2**15],     # [2**14,2**14],     # [2**13,2**13],     # [2**12,2**12],     # [2**11,2**11],     # [2**10,2**10],     # [2**9,2**9],     # [2**8,2**8],     [2**7,2**7],     [2**6,2**6],     [2**5,2**5],     [2**4,2**4]     ],dtype=int) gptypes,times,l2rerrors,bcerrors = run(currin,2,[0,1],ns) with np.printoptions(formatter={\"float\":lambda x: \"%-7.1e\"%x},linewidth=np.inf):     print()     print(\"times.shape = %s\"%str(tuple(times.shape)))     print(\"times.median(dim=-1).values\")     print(times.nanmedian(dim=-1).values.numpy())     print()     print(\"l2rerrors.shape = %s\"%str(tuple(l2rerrors.shape)))     print(\"l2rerrors.median(dim=-1).values\")     print(l2rerrors.nanmedian(dim=-1).values.numpy())     print()     print(\"bcerrors.shape = %s\"%str(tuple(bcerrors.shape)))     print(\"bcerrors.median(dim=-1).values\")     print(bcerrors.nanmedian(dim=-1).values.numpy()) In\u00a0[\u00a0]: Copied! <pre>if False: # switch to True to run this cell\n    m1ticks = torch.arange(15,5,-1)\n    m2ticks = torch.arange(15,5,-1)\n    m1mesh,m2mesh = torch.meshgrid(m1ticks,m2ticks,indexing=\"ij\")\n    ms = torch.stack([m1mesh.flatten(),m2mesh.flatten()],dim=-1)\n    _,times_d10,_,_ = run(lambda l,x: x.sum(-1),10,[0,1],2**ms,iterations=1,trials=5,fit_std_gp=False,fit_gpytorch_gp=False,fit_fgpdnet=True,fit_fgplat=False,n_ref=2**3,n_test=2**2)\n    _,times_d100,_,_ = run(lambda l,x: x.sum(-1),100,[0,1],2**ms,fit_std_gp=False,trials=5,fit_gpytorch_gp=False,fit_fgpdnet=True,fit_fgplat=False,n_ref=2**3,n_test=2**2)\n    torch.save({\"m1ticks\":m1ticks,\"m2ticks\":m2ticks,\"times_d10\":times_d10,\"times_d100\":times_d100},\"./timing.pt\")\n</pre> if False: # switch to True to run this cell     m1ticks = torch.arange(15,5,-1)     m2ticks = torch.arange(15,5,-1)     m1mesh,m2mesh = torch.meshgrid(m1ticks,m2ticks,indexing=\"ij\")     ms = torch.stack([m1mesh.flatten(),m2mesh.flatten()],dim=-1)     _,times_d10,_,_ = run(lambda l,x: x.sum(-1),10,[0,1],2**ms,iterations=1,trials=5,fit_std_gp=False,fit_gpytorch_gp=False,fit_fgpdnet=True,fit_fgplat=False,n_ref=2**3,n_test=2**2)     _,times_d100,_,_ = run(lambda l,x: x.sum(-1),100,[0,1],2**ms,fit_std_gp=False,trials=5,fit_gpytorch_gp=False,fit_fgpdnet=True,fit_fgplat=False,n_ref=2**3,n_test=2**2)     torch.save({\"m1ticks\":m1ticks,\"m2ticks\":m2ticks,\"times_d10\":times_d10,\"times_d100\":times_d100},\"./timing.pt\") In\u00a0[128]: Copied! <pre>data_timing = torch.load(\"./timing.pt\")\nm1ticks = data_timing[\"m1ticks\"]\nm2ticks = data_timing[\"m2ticks\"]\ntimes_d10 = data_timing[\"times_d10\"]\ntimes_d100 = data_timing[\"times_d100\"]\nfig,ax = pyplot.subplots(nrows=1,ncols=2,figsize=(PW,PW/2),sharex=False,sharey=False)\ncmap = sns.color_palette(\"Blues\", as_cmap=True) # https://seaborn.pydata.org/tutorial/color_palettes.html\n# cmap = sns.cubehelix_palette(start=1/2, rot=-1/2, reverse=False, as_cmap=True)\n# cmap = sns.color_palette(\"ch:start=.2,rot=-.3\", as_cmap=True)\nfgp_timemesh_d10 = times_d10[0,].nanmedian(-1).values.reshape((len(m1ticks),len(m2ticks)))\nfgp_timemesh_d100 = times_d100[0,].nanmedian(-1).values.reshape((len(m1ticks),len(m2ticks)))\nheatmap_d10 = ax[0].pcolor(torch.log10(fgp_timemesh_d10).T,cmap=cmap) \nheatmap_d100 = ax[1].pcolor(torch.log10(fgp_timemesh_d100).T,cmap=cmap) \nfor j in range(2):\n    ax[j].set_xticks(np.arange(len(m1ticks))+1/2)\n    ax[j].set_xticklabels([r\"$2^{%d}$\"%m for m in m1ticks])\n    ax[j].set_yticks(np.arange(len(m2ticks))+1/2)\n    ax[j].set_yticklabels([r\"$2^{%d}$\"%m for m in m2ticks])\n    ax[j].invert_xaxis()\n    ax[j].invert_yaxis()\n    ax[j].tick_params(axis='both',which='major',pad=15)\n    ax[j].set_xlabel(r\"$n_1$\")\n    ax[j].set_ylabel(r\"$n_2$\")\nax[0].set_title(r\"$d=10$\")\nax[1].set_title(r\"$d=100$\")\nfor i in range(len(m1ticks)):\n    for j in range(len(m2ticks)):\n        ax[0].text(i+1/2,j+1/2,\"%.2f\"%np.log10(fgp_timemesh_d10[i,j].item()),ha=\"center\",va=\"center\",color=\"black\")\n        ax[1].text(i+1/2,j+1/2,\"%.2f\"%np.log10(fgp_timemesh_d100[i,j].item()),ha=\"center\",va=\"center\",color=\"black\")\ncax_d10 = fig.add_axes([ax[0].get_position().x0,ax[0].get_position().y0-.14,ax[0].get_position().x1-ax[0].get_position().x0,0.05])\ncax_d100 = fig.add_axes([ax[1].get_position().x0,ax[1].get_position().y0-.14,ax[1].get_position().x1-ax[1].get_position().x0,0.05])\nfig.colorbar(heatmap_d10,cax_d10,location=\"bottom\",label=r\"$\\log_{10}(\\text{time per opt step}) $\",extend=\"neither\")\nfig.colorbar(heatmap_d100,cax_d100,location=\"bottom\",label=r\"$\\log_{10}(\\text{time per opt step})$\",extend=\"neither\")\nfig.savefig(\"./timing.pdf\",bbox_inches=\"tight\")\n</pre> data_timing = torch.load(\"./timing.pt\") m1ticks = data_timing[\"m1ticks\"] m2ticks = data_timing[\"m2ticks\"] times_d10 = data_timing[\"times_d10\"] times_d100 = data_timing[\"times_d100\"] fig,ax = pyplot.subplots(nrows=1,ncols=2,figsize=(PW,PW/2),sharex=False,sharey=False) cmap = sns.color_palette(\"Blues\", as_cmap=True) # https://seaborn.pydata.org/tutorial/color_palettes.html # cmap = sns.cubehelix_palette(start=1/2, rot=-1/2, reverse=False, as_cmap=True) # cmap = sns.color_palette(\"ch:start=.2,rot=-.3\", as_cmap=True) fgp_timemesh_d10 = times_d10[0,].nanmedian(-1).values.reshape((len(m1ticks),len(m2ticks))) fgp_timemesh_d100 = times_d100[0,].nanmedian(-1).values.reshape((len(m1ticks),len(m2ticks))) heatmap_d10 = ax[0].pcolor(torch.log10(fgp_timemesh_d10).T,cmap=cmap)  heatmap_d100 = ax[1].pcolor(torch.log10(fgp_timemesh_d100).T,cmap=cmap)  for j in range(2):     ax[j].set_xticks(np.arange(len(m1ticks))+1/2)     ax[j].set_xticklabels([r\"$2^{%d}$\"%m for m in m1ticks])     ax[j].set_yticks(np.arange(len(m2ticks))+1/2)     ax[j].set_yticklabels([r\"$2^{%d}$\"%m for m in m2ticks])     ax[j].invert_xaxis()     ax[j].invert_yaxis()     ax[j].tick_params(axis='both',which='major',pad=15)     ax[j].set_xlabel(r\"$n_1$\")     ax[j].set_ylabel(r\"$n_2$\") ax[0].set_title(r\"$d=10$\") ax[1].set_title(r\"$d=100$\") for i in range(len(m1ticks)):     for j in range(len(m2ticks)):         ax[0].text(i+1/2,j+1/2,\"%.2f\"%np.log10(fgp_timemesh_d10[i,j].item()),ha=\"center\",va=\"center\",color=\"black\")         ax[1].text(i+1/2,j+1/2,\"%.2f\"%np.log10(fgp_timemesh_d100[i,j].item()),ha=\"center\",va=\"center\",color=\"black\") cax_d10 = fig.add_axes([ax[0].get_position().x0,ax[0].get_position().y0-.14,ax[0].get_position().x1-ax[0].get_position().x0,0.05]) cax_d100 = fig.add_axes([ax[1].get_position().x0,ax[1].get_position().y0-.14,ax[1].get_position().x1-ax[1].get_position().x0,0.05]) fig.colorbar(heatmap_d10,cax_d10,location=\"bottom\",label=r\"$\\log_{10}(\\text{time per opt step}) $\",extend=\"neither\") fig.colorbar(heatmap_d100,cax_d100,location=\"bottom\",label=r\"$\\log_{10}(\\text{time per opt step})$\",extend=\"neither\") fig.savefig(\"./timing.pdf\",bbox_inches=\"tight\") In\u00a0[\u00a0]: Copied! <pre>if True: # uncomment to run simulations\n    # name,func,d,levelops,ms,trials = (\"Currin\",currin,2,[0,1],[13,12,11,10,9,8,7,6],5)\n    # name,func,d,levelops,ms,trials = (\"Park1\",park1,4,[0,1],[13,12,11,10,9,8,7,6],5)\n    # name,func,d,levelops,ms,trials = (\"Park2\",park2,4,[0,1],[13,12,11,10,9,8,7,6],5)\n    # name,func,d,levelops,ms,trials = (\"Ackley\",ackley,4,[0,1],[16,15,14,13,12,11,10,9,8,7,6,5],5)\n    # name,func,d,levelops,ms,trials = (\"Borehole\",borehole,8,[0,1],[16,15,14,13,12,11,10,9,8,7,6,5],5)\n    # name,func,d,levelops,ms,trials = (\"Elliptic\",elliptic,16,[4,5,6],[15,13,11,9,7,5],5)\n    # name,func,d,levelops,ms,trials = (\"Asian Option\",asian_option,32,[8,16,32,64],5)\n    name,func,d,levelops,ms,trials = (\"Cookie\",cookie_pregen,8,[1,2,3,4],[13,11,9,7,5],1)\n    meshes = torch.meshgrid(*[torch.tensor(ms,dtype=int) for l in range(len(levelops))],indexing=\"ij\")\n    ns = 2**torch.stack([mesh.flatten() for mesh in meshes],dim=-1)\n    print(name)\n    if name==\"Cookie\":\n        seeds = [7,8,9,10]\n        cookie_data_test = torch.load(\"./cookie_data.test.pt\")\n        test_data = (cookie_data_test[4][0][-2**12:],cookie_data_test[4][1][-2**12:])\n        ref_mu = 0.064202350667514\n    else:\n        seeds = None \n        test_data = None\n        ref_mu = None\n    if name==\"Asian Option\":\n        tf_output = lambda x: torch.maximum(x,torch.zeros(1))\n    else:\n        tf_output = lambda x: x\n    gptypes,times,l2rerrors,bcerrors = run(func,d,levelops,ns,trials=trials,seeds=seeds,test_data=test_data,ref_mu=ref_mu,tf_output=tf_output)\n    torch.save({\"ns\":ns,\"gptypes\":gptypes,\"times\":times,\"l2rerrors\":l2rerrors,\"bcerrors\":bcerrors},\"./runs.%s.pt\"%name)\n</pre> if True: # uncomment to run simulations     # name,func,d,levelops,ms,trials = (\"Currin\",currin,2,[0,1],[13,12,11,10,9,8,7,6],5)     # name,func,d,levelops,ms,trials = (\"Park1\",park1,4,[0,1],[13,12,11,10,9,8,7,6],5)     # name,func,d,levelops,ms,trials = (\"Park2\",park2,4,[0,1],[13,12,11,10,9,8,7,6],5)     # name,func,d,levelops,ms,trials = (\"Ackley\",ackley,4,[0,1],[16,15,14,13,12,11,10,9,8,7,6,5],5)     # name,func,d,levelops,ms,trials = (\"Borehole\",borehole,8,[0,1],[16,15,14,13,12,11,10,9,8,7,6,5],5)     # name,func,d,levelops,ms,trials = (\"Elliptic\",elliptic,16,[4,5,6],[15,13,11,9,7,5],5)     # name,func,d,levelops,ms,trials = (\"Asian Option\",asian_option,32,[8,16,32,64],5)     name,func,d,levelops,ms,trials = (\"Cookie\",cookie_pregen,8,[1,2,3,4],[13,11,9,7,5],1)     meshes = torch.meshgrid(*[torch.tensor(ms,dtype=int) for l in range(len(levelops))],indexing=\"ij\")     ns = 2**torch.stack([mesh.flatten() for mesh in meshes],dim=-1)     print(name)     if name==\"Cookie\":         seeds = [7,8,9,10]         cookie_data_test = torch.load(\"./cookie_data.test.pt\")         test_data = (cookie_data_test[4][0][-2**12:],cookie_data_test[4][1][-2**12:])         ref_mu = 0.064202350667514     else:         seeds = None          test_data = None         ref_mu = None     if name==\"Asian Option\":         tf_output = lambda x: torch.maximum(x,torch.zeros(1))     else:         tf_output = lambda x: x     gptypes,times,l2rerrors,bcerrors = run(func,d,levelops,ns,trials=trials,seeds=seeds,test_data=test_data,ref_mu=ref_mu,tf_output=tf_output)     torch.save({\"ns\":ns,\"gptypes\":gptypes,\"times\":times,\"l2rerrors\":l2rerrors,\"bcerrors\":bcerrors},\"./runs.%s.pt\"%name) <pre>Elliptic\n\tgptypes: ['Cholesky SE', 'CG SE (GPyTorch)', 'Fast DSI (ours)']\n\n\tns\n\t\t0: [32768 32768 32768]\n\t\t1: [32768 32768  8192]\n\t\t2: [32768 32768  2048]\n\t\t3: [32768 32768   512]\n\t\t4: [32768 32768   128]\n\t\t5: [32768 32768    32]\n\t\t6: [32768  8192 32768]\n\t\t7: [32768  8192  8192]\n\t\t8: [32768  8192  2048]\n\t\t9: [32768  8192   512]\n\t\t10: [32768  8192   128]\n\t\t11: [32768  8192    32]\n\t\t12: [32768  2048 32768]\n\t\t13: [32768  2048  8192]\n\t\t14: [32768  2048  2048]\n\t\t15: [32768  2048   512]\n\t\t16: [32768  2048   128]\n\t\t17: [32768  2048    32]\n\t\t18: [32768   512 32768]\n\t\t19: [32768   512  8192]\n\t\t20: [32768   512  2048]\n\t\t21: [32768   512   512]\n\t\t22: [32768   512   128]\n\t\t23: [32768   512    32]\n\t\t24: [32768   128 32768]\n\t\t25: [32768   128  8192]\n\t\t26: [32768   128  2048]\n\t\t27: [32768   128   512]\n\t\t28: [32768   128   128]\n\t\t29: [32768   128    32]\n\t\t30: [32768    32 32768]\n\t\t31: [32768    32  8192]\n\t\t32: [32768    32  2048]\n\t\t33: [32768    32   512]\n\t\t34: [32768    32   128]\n\t\t35: [32768    32    32]\n\t\t36: [ 8192 32768 32768]\n\t\t37: [ 8192 32768  8192]\n\t\t38: [ 8192 32768  2048]\n\t\t39: [ 8192 32768   512]\n\t\t40: [ 8192 32768   128]\n\t\t41: [ 8192 32768    32]\n\t\t42: [ 8192  8192 32768]\n\t\t43: [8192 8192 8192]\n\t\t44: [8192 8192 2048]\n\t\t45: [8192 8192  512]\n\t\t46: [8192 8192  128]\n\t\t47: [8192 8192   32]\n\t\t48: [ 8192  2048 32768]\n\t\t49: [8192 2048 8192]\n\t\t50: [8192 2048 2048]\n\t\t51: [8192 2048  512]\n\t\t52: [8192 2048  128]\n\t\t53: [8192 2048   32]\n\t\t54: [ 8192   512 32768]\n\t\t55: [8192  512 8192]\n\t\t56: [8192  512 2048]\n\t\t57: [8192  512  512]\n\t\t58: [8192  512  128]\n\t\t59: [8192  512   32]\n\t\t60: [ 8192   128 32768]\n\t\t61: [8192  128 8192]\n\t\t62: [8192  128 2048]\n\t\t63: [8192  128  512]\n\t\t64: [8192  128  128]\n\t\t65: [8192  128   32]\n\t\t66: [ 8192    32 32768]\n\t\t67: [8192   32 8192]\n\t\t68: [8192   32 2048]\n\t\t69: [8192   32  512]\n\t\t70: [8192   32  128]\n\t\t71: [8192   32   32]\n\t\t72: [ 2048 32768 32768]\n\t\t73: [ 2048 32768  8192]\n\t\t74: [ 2048 32768  2048]\n\t\t75: [ 2048 32768   512]\n\t\t76: [ 2048 32768   128]\n\t\t77: [ 2048 32768    32]\n\t\t78: [ 2048  8192 32768]\n\t\t79: [2048 8192 8192]\n\t\t80: [2048 8192 2048]\n\t\t81: [2048 8192  512]\n\t\t82: [2048 8192  128]\n\t\t83: [2048 8192   32]\n\t\t84: [ 2048  2048 32768]\n\t\t85: [2048 2048 8192]\n\t\t86: [2048 2048 2048]\n\t\t87: [2048 2048  512]\n\t\t88: [2048 2048  128]\n\t\t89: [2048 2048   32]\n\t\t90: [ 2048   512 32768]\n\t\t91: [2048  512 8192]\n\t\t92: [2048  512 2048]\n\t\t93: [2048  512  512]\n\t\t94: [2048  512  128]\n\t\t95: [2048  512   32]\n\t\t96: [ 2048   128 32768]\n\t\t97: [2048  128 8192]\n\t\t98: [2048  128 2048]\n\t\t99: [2048  128  512]\n\t\t100: [2048  128  128]\n\t\t101: [2048  128   32]\n\t\t102: [ 2048    32 32768]\n\t\t103: [2048   32 8192]\n\t\t104: [2048   32 2048]\n\t\t105: [2048   32  512]\n\t\t106: [2048   32  128]\n\t\t107: [2048   32   32]\n\t\t108: [  512 32768 32768]\n\t\t109: [  512 32768  8192]\n\t\t110: [  512 32768  2048]\n\t\t111: [  512 32768   512]\n\t\t112: [  512 32768   128]\n\t\t113: [  512 32768    32]\n\t\t114: [  512  8192 32768]\n\t\t115: [ 512 8192 8192]\n\t\t116: [ 512 8192 2048]\n\t\t117: [ 512 8192  512]\n\t\t118: [ 512 8192  128]\n\t\t119: [ 512 8192   32]\n\t\t120: [  512  2048 32768]\n\t\t121: [ 512 2048 8192]\n\t\t122: [ 512 2048 2048]\n\t\t123: [ 512 2048  512]\n\t\t124: [ 512 2048  128]\n\t\t125: [ 512 2048   32]\n\t\t126: [  512   512 32768]\n\t\t127: [ 512  512 8192]\n\t\t128: [ 512  512 2048]\n\t\t129: [512 512 512]\n\t\t130: [512 512 128]\n\t\t131: [512 512  32]\n\t\t132: [  512   128 32768]\n\t\t133: [ 512  128 8192]\n\t\t134: [ 512  128 2048]\n\t\t135: [512 128 512]\n\t\t136: [512 128 128]\n\t\t137: [512 128  32]\n\t\t138: [  512    32 32768]\n\t\t139: [ 512   32 8192]\n\t\t140: [ 512   32 2048]\n\t\t141: [512  32 512]\n\t\t142: [512  32 128]\n\t\t143: [512  32  32]\n\t\t144: [  128 32768 32768]\n\t\t145: [  128 32768  8192]\n\t\t146: [  128 32768  2048]\n\t\t147: [  128 32768   512]\n\t\t148: [  128 32768   128]\n\t\t149: [  128 32768    32]\n\t\t150: [  128  8192 32768]\n\t\t151: [ 128 8192 8192]\n\t\t152: [ 128 8192 2048]\n\t\t153: [ 128 8192  512]\n\t\t154: [ 128 8192  128]\n\t\t155: [ 128 8192   32]\n\t\t156: [  128  2048 32768]\n\t\t157: [ 128 2048 8192]\n\t\t158: [ 128 2048 2048]\n\t\t159: [ 128 2048  512]\n\t\t160: [ 128 2048  128]\n\t\t161: [ 128 2048   32]\n\t\t162: [  128   512 32768]\n\t\t163: [ 128  512 8192]\n\t\t164: [ 128  512 2048]\n\t\t165: [128 512 512]\n\t\t166: [128 512 128]\n\t\t167: [128 512  32]\n\t\t168: [  128   128 32768]\n\t\t169: [ 128  128 8192]\n\t\t170: [ 128  128 2048]\n\t\t171: [128 128 512]\n\t\t172: [128 128 128]\n\t\t173: [128 128  32]\n\t\t174: [  128    32 32768]\n\t\t175: [ 128   32 8192]\n\t\t176: [ 128   32 2048]\n\t\t177: [128  32 512]\n\t\t178: [128  32 128]\n\t\t179: [128  32  32]\n\t\t180: [   32 32768 32768]\n\t\t181: [   32 32768  8192]\n\t\t182: [   32 32768  2048]\n\t\t183: [   32 32768   512]\n\t\t184: [   32 32768   128]\n\t\t185: [   32 32768    32]\n\t\t186: [   32  8192 32768]\n\t\t187: [  32 8192 8192]\n\t\t188: [  32 8192 2048]\n\t\t189: [  32 8192  512]\n\t\t190: [  32 8192  128]\n\t\t191: [  32 8192   32]\n\t\t192: [   32  2048 32768]\n\t\t193: [  32 2048 8192]\n\t\t194: [  32 2048 2048]\n\t\t195: [  32 2048  512]\n\t\t196: [  32 2048  128]\n\t\t197: [  32 2048   32]\n\t\t198: [   32   512 32768]\n\t\t199: [  32  512 8192]\n\t\t200: [  32  512 2048]\n\t\t201: [ 32 512 512]\n\t\t202: [ 32 512 128]\n\t\t203: [ 32 512  32]\n\t\t204: [   32   128 32768]\n\t\t205: [  32  128 8192]\n\t\t206: [  32  128 2048]\n\t\t207: [ 32 128 512]\n\t\t208: [ 32 128 128]\n\t\t209: [ 32 128  32]\n\t\t210: [   32    32 32768]\n\t\t211: [  32   32 8192]\n\t\t212: [  32   32 2048]\n\t\t213: [ 32  32 512]\n\t\t214: [ 32  32 128]\n\t\t215: [32 32 32]\n\n\t~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n\tns[0] = [32768 32768 32768]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          172.2                [nan nan 1.2e-01]                   [nan nan 2.1e-02]                   [nan nan 2.6e-05]                  \n\t\t1          328.6                [nan nan 1.1e-01]                   [nan nan 3.2e-02]                   [nan nan 1.8e-05]                  \n\t\t2          496.6                [nan nan 1.2e-01]                   [nan nan 2.6e-02]                   [nan nan 1.8e-05]                  \n\t\t3          667.4                [nan nan 1.2e-01]                   [nan nan 2.1e-02]                   [nan nan 5.2e-06]                  \n\t\t4          835.7                [nan nan 1.2e-01]                   [nan nan 2.1e-02]                   [nan nan 6.3e-06]                  \n\n\tns[1] = [32768 32768  8192]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          946.7                [nan nan 1.1e-01]                   [nan nan 3.5e-02]                   [nan nan 2.6e-05]                  \n\t\t1          1063.3               [nan nan 1.1e-01]                   [nan nan 2.6e-02]                   [nan nan 2.0e-06]                  \n\t\t2          1174.3               [nan nan 1.1e-01]                   [nan nan 2.6e-02]                   [nan nan 5.4e-07]                  \n\t\t3          1287.2               [nan nan 1.1e-01]                   [nan nan 2.6e-02]                   [nan nan 5.9e-06]                  \n\t\t4          1393.7               [nan nan 1.1e-01]                   [nan nan 3.3e-02]                   [nan nan 2.3e-05]                  \n\n\tns[2] = [32768 32768  2048]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          1508.2               [nan nan 1.1e-01]                   [nan nan 3.2e-02]                   [nan nan 4.5e-05]                  \n\t\t1          1622.3               [nan nan 1.1e-01]                   [nan nan 2.6e-02]                   [nan nan 2.0e-05]                  \n\t\t2          1735.8               [nan nan 1.1e-01]                   [nan nan 2.9e-02]                   [nan nan 1.5e-05]                  \n\t\t3          1841.6               [nan nan 1.1e-01]                   [nan nan 3.1e-02]                   [nan nan 1.1e-04]                  \n\t\t4          1957.1               [nan nan 1.1e-01]                   [nan nan 2.6e-02]                   [nan nan 2.0e-04]                  \n\n\tns[3] = [32768 32768   512]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          2071.9               [nan nan 1.3e-01]                   [nan nan 3.0e-02]                   [nan nan 1.4e-04]                  \n\t\t1          2180.6               [nan nan 1.3e-01]                   [nan nan 3.3e-02]                   [nan nan 3.1e-05]                  \n\t\t2          2295.9               [nan nan 1.3e-01]                   [nan nan 2.7e-02]                   [nan nan 1.2e-04]                  \n\t\t3          2411.4               [nan nan 1.3e-01]                   [nan nan 3.0e-02]                   [nan nan 1.1e-04]                  \n\t\t4          2523.3               [nan nan 1.3e-01]                   [nan nan 3.0e-02]                   [nan nan 2.8e-05]                  \n\n\tns[4] = [32768 32768   128]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          2654.5               [nan nan 2.6e-01]                   [nan nan 2.3e-02]                   [nan nan 2.0e-04]                  \n\t\t1          2784.0               [nan nan 2.5e-01]                   [nan nan 2.7e-02]                   [nan nan 5.2e-04]                  \n\t\t2          2908.7               [nan nan 2.5e-01]                   [nan nan 2.5e-02]                   [nan nan 8.7e-05]                  \n\t\t3          3037.6               [nan nan 2.5e-01]                   [nan nan 3.3e-02]                   [nan nan 3.1e-04]                  \n\t\t4          3165.6               [nan nan 2.5e-01]                   [nan nan 5.0e-02]                   [nan nan 6.0e-04]                  \n\n\tns[5] = [32768 32768    32]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          3302.8               [nan nan 7.8e-01]                   [nan nan 6.7e-02]                   [nan nan 6.8e-04]                  \n\t\t1          3489.0               [nan nan 8.0e-01]                   [nan nan 3.2e-02]                   [nan nan 8.1e-05]                  \n\t\t2          3674.7               [nan nan 8.2e-01]                   [nan nan 3.0e-02]                   [nan nan 4.9e-04]                  \n\t\t3          3861.1               [nan nan 8.2e-01]                   [nan nan 2.5e-02]                   [nan nan 2.4e-04]                  \n\t\t4          4049.7               [nan nan 8.2e-01]                   [nan nan 4.7e-02]                   [nan nan 2.3e-03]                  \n\n\tns[6] = [32768  8192 32768]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          4166.9               [nan nan 1.1e-01]                   [nan nan 2.4e-02]                   [nan nan 7.7e-06]                  \n\t\t1          4288.2               [nan nan 1.1e-01]                   [nan nan 3.9e-02]                   [nan nan 4.5e-05]                  \n\t\t2          4406.4               [nan nan 1.1e-01]                   [nan nan 2.3e-02]                   [nan nan 8.8e-06]                  \n\t\t3          4523.6               [nan nan 1.1e-01]                   [nan nan 2.8e-02]                   [nan nan 8.8e-06]                  \n\t\t4          4633.4               [nan nan 1.0e-01]                   [nan nan 2.8e-02]                   [nan nan 3.4e-05]                  \n\n\tns[7] = [32768  8192  8192]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          4701.7               [nan nan 9.0e-02]                   [nan nan 4.0e-02]                   [nan nan 6.6e-06]                  \n\t\t1          4770.0               [nan nan 8.9e-02]                   [nan nan 2.9e-02]                   [nan nan 3.5e-05]                  \n\t\t2          4832.5               [nan nan 8.8e-02]                   [nan nan 3.3e-02]                   [nan nan 1.5e-05]                  \n\t\t3          4893.7               [nan nan 9.1e-02]                   [nan nan 3.8e-02]                   [nan nan 1.3e-05]                  \n\t\t4          4962.8               [nan nan 9.0e-02]                   [nan nan 2.6e-02]                   [nan nan 3.4e-05]                  \n\n\tns[8] = [32768  8192  2048]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          5022.3               [nan nan 8.9e-02]                   [nan nan 3.9e-02]                   [nan nan 6.4e-05]                  \n\t\t1          5079.1               [nan nan 8.6e-02]                   [nan nan 5.7e-02]                   [nan nan 1.6e-05]                  \n\t\t2          5145.3               [nan nan 9.2e-02]                   [nan nan 2.7e-02]                   [nan nan 7.3e-05]                  \n\t\t3          5204.1               [nan nan 9.0e-02]                   [nan nan 3.7e-02]                   [nan nan 4.1e-05]                  \n\t\t4          5270.4               [nan nan 8.9e-02]                   [nan nan 3.9e-02]                   [nan nan 5.1e-05]                  \n\n\tns[9] = [32768  8192   512]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          5335.8               [nan nan 9.6e-02]                   [nan nan 3.1e-02]                   [nan nan 1.3e-05]                  \n\t\t1          5400.0               [nan nan 9.7e-02]                   [nan nan 3.1e-02]                   [nan nan 2.6e-04]                  \n\t\t2          5464.7               [nan nan 9.6e-02]                   [nan nan 4.5e-02]                   [nan nan 5.6e-05]                  \n\t\t3          5528.7               [nan nan 9.4e-02]                   [nan nan 3.0e-02]                   [nan nan 1.8e-04]                  \n\t\t4          5586.6               [nan nan 9.5e-02]                   [nan nan 3.0e-02]                   [nan nan 3.0e-04]                  \n\n\tns[10] = [32768  8192   128]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          5645.3               [nan nan 1.4e-01]                   [nan nan 2.2e-01]                   [nan nan 5.6e-03]                  \n\t\t1          5712.8               [nan nan 1.4e-01]                   [nan nan 2.8e-02]                   [nan nan 2.8e-04]                  \n\t\t2          5781.9               [nan nan 1.4e-01]                   [nan nan 3.9e-02]                   [nan nan 3.5e-04]                  \n\t\t3          5841.2               [nan nan 1.4e-01]                   [nan nan 7.3e-02]                   [nan nan 1.9e-04]                  \n\t\t4          5900.2               [nan nan 1.4e-01]                   [nan nan 6.6e-02]                   [nan nan 2.7e-04]                  \n\n\tns[11] = [32768  8192    32]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          5966.7               [nan nan 3.0e-01]                   [nan nan 2.0e-01]                   [nan nan 3.9e-03]                  \n\t\t1          6051.5               [nan nan 3.0e-01]                   [nan nan 3.5e-02]                   [nan nan 8.1e-04]                  \n\t\t2          6117.8               [nan nan 3.0e-01]                   [nan nan 2.4e-01]                   [nan nan 6.8e-03]                  \n\t\t3          6187.3               [nan nan 3.1e-01]                   [nan nan 6.8e-02]                   [nan nan 2.0e-03]                  \n\t\t4          6254.0               [nan nan 3.1e-01]                   [nan nan 5.9e-02]                   [nan nan 6.7e-04]                  \n\n\tns[12] = [32768  2048 32768]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          6368.4               [nan nan 1.1e-01]                   [nan nan 2.8e-02]                   [nan nan 3.2e-05]                  \n\t\t1          6472.6               [nan nan 1.1e-01]                   [nan nan 2.8e-02]                   [nan nan 2.5e-05]                  \n\t\t2          6585.4               [nan nan 1.1e-01]                   [nan nan 2.4e-02]                   [nan nan 5.5e-06]                  \n\t\t3          6700.7               [nan nan 1.0e-01]                   [nan nan 2.9e-02]                   [nan nan 1.9e-05]                  \n\t\t4          6816.0               [nan nan 1.1e-01]                   [nan nan 2.4e-02]                   [nan nan 1.3e-05]                  \n\n\tns[13] = [32768  2048  8192]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          6880.5               [nan nan 8.7e-02]                   [nan nan 2.8e-02]                   [nan nan 1.7e-05]                  \n\t\t1          6947.1               [nan nan 9.2e-02]                   [nan nan 3.6e-02]                   [nan nan 6.7e-05]                  \n\t\t2          7008.9               [nan nan 9.0e-02]                   [nan nan 3.5e-02]                   [nan nan 5.5e-06]                  \n\t\t3          7073.1               [nan nan 9.0e-02]                   [nan nan 2.7e-02]                   [nan nan 1.1e-05]                  \n\t\t4          7139.1               [nan nan 9.1e-02]                   [nan nan 3.0e-02]                   [nan nan 1.4e-06]                  \n\n\tns[14] = [32768  2048  2048]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          7193.5               [nan nan 8.1e-02]                   [nan nan 5.5e-02]                   [nan nan 3.0e-05]                  \n\t\t1          7249.1               [nan nan 8.3e-02]                   [nan nan 6.9e-02]                   [nan nan 7.4e-05]                  \n\t\t2          7302.4               [nan nan 8.7e-02]                   [nan nan 8.8e-02]                   [nan nan 8.9e-05]                  \n\t\t3          7362.5               [nan nan 8.5e-02]                   [nan nan 3.1e-02]                   [nan nan 2.9e-06]                  \n\t\t4          7423.2               [nan nan 8.5e-02]                   [nan nan 3.0e-02]                   [nan nan 8.5e-05]                  \n\n\tns[15] = [32768  2048   512]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          7478.5               [nan nan 8.8e-02]                   [nan nan 3.5e-02]                   [nan nan 1.5e-04]                  \n\t\t1          7532.0               [nan nan 8.8e-02]                   [nan nan 7.3e-02]                   [nan nan 2.0e-04]                  \n\t\t2          7591.9               [nan nan 9.2e-02]                   [nan nan 4.4e-02]                   [nan nan 1.4e-05]                  \n\t\t3          7645.7               [nan nan 9.2e-02]                   [nan nan 5.5e-02]                   [nan nan 5.2e-05]                  \n\t\t4          7706.2               [nan nan 9.0e-02]                   [nan nan 3.5e-02]                   [nan nan 2.0e-04]                  \n\n\tns[16] = [32768  2048   128]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          7767.7               [nan nan 1.1e-01]                   [nan nan 4.0e-02]                   [nan nan 4.5e-04]                  \n\t\t1          7830.5               [nan nan 1.1e-01]                   [nan nan 4.9e-02]                   [nan nan 7.5e-05]                  \n\t\t2          7892.5               [nan nan 1.1e-01]                   [nan nan 3.8e-02]                   [nan nan 3.0e-04]                  \n\t\t3          7948.7               [nan nan 1.1e-01]                   [nan nan 7.8e-02]                   [nan nan 7.9e-04]                  \n\t\t4          8009.7               [nan nan 1.1e-01]                   [nan nan 3.8e-02]                   [nan nan 8.3e-05]                  \n\n\tns[17] = [32768  2048    32]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          8086.8               [nan nan 2.6e-01]                   [nan nan 4.8e-02]                   [nan nan 1.9e-03]                  \n\t\t1          8144.5               [nan nan 2.4e-01]                   [nan nan 1.6e-01]                   [nan nan 7.4e-04]                  \n\t\t2          8204.2               [nan nan 2.5e-01]                   [nan nan 1.1e-01]                   [nan nan 2.0e-03]                  \n\t\t3          8280.9               [nan nan 2.5e-01]                   [nan nan 5.9e-02]                   [nan nan 1.6e-03]                  \n\t\t4          8357.6               [nan nan 2.5e-01]                   [nan nan 3.6e-02]                   [nan nan 6.8e-04]                  \n\n\tns[18] = [32768   512 32768]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          8469.5               [nan nan 1.2e-01]                   [nan nan 2.8e-02]                   [nan nan 1.1e-05]                  \n\t\t1          8578.7               [nan nan 1.2e-01]                   [nan nan 2.9e-02]                   [nan nan 1.7e-05]                  \n\t\t2          8690.6               [nan nan 1.2e-01]                   [nan nan 2.7e-02]                   [nan nan 2.6e-05]                  \n\t\t3          8800.7               [nan nan 1.2e-01]                   [nan nan 2.9e-02]                   [nan nan 1.6e-05]                  \n\t\t4          8914.7               [nan nan 1.2e-01]                   [nan nan 2.7e-02]                   [nan nan 7.1e-06]                  \n\n\tns[19] = [32768   512  8192]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          8973.4               [nan nan 9.3e-02]                   [nan nan 5.1e-02]                   [nan nan 7.3e-06]                  \n\t\t1          9036.2               [nan nan 9.5e-02]                   [nan nan 3.0e-02]                   [nan nan 6.1e-06]                  \n\t\t2          9092.4               [nan nan 9.4e-02]                   [nan nan 3.6e-02]                   [nan nan 6.5e-05]                  \n\t\t3          9151.9               [nan nan 9.3e-02]                   [nan nan 2.9e-02]                   [nan nan 4.4e-06]                  \n\t\t4          9210.4               [nan nan 9.5e-02]                   [nan nan 3.4e-02]                   [nan nan 8.9e-06]                  \n\n\tns[20] = [32768   512  2048]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          9269.9               [nan nan 9.1e-02]                   [nan nan 4.2e-02]                   [nan nan 1.4e-05]                  \n\t\t1          9328.2               [nan nan 9.1e-02]                   [nan nan 3.6e-02]                   [nan nan 4.3e-05]                  \n\t\t2          9388.1               [nan nan 9.1e-02]                   [nan nan 4.4e-02]                   [nan nan 1.4e-04]                  \n\t\t3          9447.5               [nan nan 9.3e-02]                   [nan nan 3.0e-02]                   [nan nan 1.1e-04]                  \n\t\t4          9499.0               [nan nan 8.8e-02]                   [nan nan 7.9e-02]                   [nan nan 9.5e-05]                  \n\n\tns[21] = [32768   512   512]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          9558.4               [nan nan 9.7e-02]                   [nan nan 4.1e-02]                   [nan nan 3.4e-04]                  \n\t\t1          9617.7               [nan nan 9.5e-02]                   [nan nan 3.5e-02]                   [nan nan 8.6e-05]                  \n\t\t2          9681.1               [nan nan 9.5e-02]                   [nan nan 3.6e-02]                   [nan nan 2.5e-04]                  \n\t\t3          9741.5               [nan nan 9.9e-02]                   [nan nan 4.2e-02]                   [nan nan 2.3e-04]                  \n\t\t4          9801.3               [nan nan 9.8e-02]                   [nan nan 4.3e-02]                   [nan nan 3.8e-04]                  \n\n\tns[22] = [32768   512   128]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          9854.9               [nan nan 1.3e-01]                   [nan nan 1.3e-01]                   [nan nan 4.4e-04]                  \n\t\t1          9909.1               [nan nan 1.2e-01]                   [nan nan 1.0e-01]                   [nan nan 5.0e-04]                  \n\t\t2          9972.5               [nan nan 1.2e-01]                   [nan nan 3.8e-02]                   [nan nan 2.1e-04]                  \n\t\t3          10027.5              [nan nan 1.2e-01]                   [nan nan 1.3e-01]                   [nan nan 1.8e-03]                  \n\t\t4          10090.8              [nan nan 1.2e-01]                   [nan nan 4.7e-02]                   [nan nan 1.3e-03]                  \n\n\tns[23] = [32768   512    32]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          10151.2              [nan nan 2.7e-01]                   [nan nan 1.3e-01]                   [nan nan 3.7e-04]                  \n\t\t1          10211.1              [nan nan 2.6e-01]                   [nan nan 1.2e-01]                   [nan nan 3.5e-03]                  \n\t\t2          10270.7              [nan nan 2.6e-01]                   [nan nan 1.3e-01]                   [nan nan 1.4e-03]                  \n\t\t3          10332.4              [nan nan 2.6e-01]                   [nan nan 1.1e-01]                   [nan nan 1.2e-03]                  \n\t\t4          10398.5              [nan nan 2.6e-01]                   [nan nan 8.5e-02]                   [nan nan 5.5e-04]                  \n\n\tns[24] = [32768   128 32768]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          10527.0              [nan nan 2.5e-01]                   [nan nan 7.0e-02]                   [nan nan 5.4e-05]                  \n\t\t1          10650.1              [nan nan 2.5e-01]                   [nan nan 2.7e-02]                   [nan nan 3.6e-05]                  \n\t\t2          10776.5              [nan nan 2.5e-01]                   [nan nan 2.7e-02]                   [nan nan 1.8e-05]                  \n\t\t3          10903.5              [nan nan 2.5e-01]                   [nan nan 2.8e-02]                   [nan nan 3.6e-05]                  \n\t\t4          11027.8              [nan nan 2.4e-01]                   [nan nan 2.6e-02]                   [nan nan 3.9e-05]                  \n\n\tns[25] = [32768   128  8192]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          11086.2              [nan nan 1.4e-01]                   [nan nan 7.3e-02]                   [nan nan 7.7e-05]                  \n\t\t1          11153.7              [nan nan 1.4e-01]                   [nan nan 4.8e-02]                   [nan nan 3.0e-05]                  \n\t\t2          11212.0              [nan nan 1.4e-01]                   [nan nan 4.8e-02]                   [nan nan 6.4e-06]                  \n\t\t3          11280.5              [nan nan 1.4e-01]                   [nan nan 3.3e-02]                   [nan nan 3.0e-05]                  \n\t\t4          11350.9              [nan nan 1.4e-01]                   [nan nan 2.9e-02]                   [nan nan 5.4e-05]                  \n\n\tns[26] = [32768   128  2048]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          11413.1              [nan nan 1.1e-01]                   [nan nan 3.7e-02]                   [nan nan 2.6e-05]                  \n\t\t1          11475.6              [nan nan 1.1e-01]                   [nan nan 3.8e-02]                   [nan nan 2.9e-05]                  \n\t\t2          11532.7              [nan nan 1.1e-01]                   [nan nan 6.0e-02]                   [nan nan 4.9e-05]                  \n\t\t3          11592.3              [nan nan 1.1e-01]                   [nan nan 3.6e-02]                   [nan nan 1.3e-06]                  \n\t\t4          11654.0              [nan nan 1.1e-01]                   [nan nan 3.0e-02]                   [nan nan 3.1e-05]                  \n\n\tns[27] = [32768   128   512]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          11708.8              [nan nan 1.2e-01]                   [nan nan 1.3e-01]                   [nan nan 4.3e-04]                  \n\t\t1          11771.3              [nan nan 1.2e-01]                   [nan nan 4.7e-02]                   [nan nan 2.7e-04]                  \n\t\t2          11834.4              [nan nan 1.2e-01]                   [nan nan 9.5e-02]                   [nan nan 6.6e-04]                  \n\t\t3          11896.8              [nan nan 1.2e-01]                   [nan nan 3.7e-02]                   [nan nan 4.5e-04]                  \n\t\t4          11961.3              [nan nan 1.2e-01]                   [nan nan 4.1e-02]                   [nan nan 9.3e-05]                  \n\n\tns[28] = [32768   128   128]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          12031.2              [nan nan 2.0e-01]                   [nan nan 8.8e-02]                   [nan nan 4.7e-04]                  \n\t\t1          12102.1              [nan nan 2.0e-01]                   [nan nan 9.1e-02]                   [nan nan 2.5e-04]                  \n\t\t2          12172.1              [nan nan 2.0e-01]                   [nan nan 7.0e-02]                   [nan nan 1.0e-03]                  \n\t\t3          12241.9              [nan nan 2.0e-01]                   [nan nan 9.7e-02]                   [nan nan 1.4e-03]                  \n\t\t4          12310.5              [nan nan 2.0e-01]                   [nan nan 1.1e-01]                   [nan nan 1.7e-03]                  \n\n\tns[29] = [32768   128    32]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          12376.6              [nan nan 3.5e-01]                   [nan nan 1.7e-01]                   [nan nan 3.3e-03]                  \n\t\t1          12439.1              [nan nan 3.4e-01]                   [nan nan 1.9e-01]                   [nan nan 5.8e-03]                  \n\t\t2          12501.3              [nan nan 3.3e-01]                   [nan nan 1.9e-01]                   [nan nan 3.5e-03]                  \n\t\t3          12587.2              [nan nan 3.4e-01]                   [nan nan 5.9e-02]                   [nan nan 2.1e-03]                  \n\t\t4          12649.0              [nan nan 3.4e-01]                   [nan nan 1.5e-01]                   [nan nan 1.1e-03]                  \n\n\tns[30] = [32768    32 32768]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          12833.5              [nan nan 8.1e-01]                   [nan nan 3.1e-02]                   [nan nan 2.5e-05]                  \n\t\t1          12982.5              [nan nan 8.0e-01]                   [nan nan 3.2e-02]                   [nan nan 2.2e-05]                  \n\t\t2          13163.8              [nan nan 8.1e-01]                   [nan nan 3.7e-02]                   [nan nan 3.6e-05]                  \n\t\t3          13343.9              [nan nan 8.1e-01]                   [nan nan 2.7e-02]                   [nan nan 7.6e-06]                  \n\t\t4          13525.5              [nan nan 8.1e-01]                   [nan nan 2.8e-02]                   [nan nan 4.4e-06]                  \n\n\tns[31] = [32768    32  8192]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          13612.4              [nan nan 3.2e-01]                   [nan nan 3.1e-02]                   [nan nan 4.1e-05]                  \n\t\t1          13677.7              [nan nan 3.0e-01]                   [nan nan 4.2e-02]                   [nan nan 7.3e-05]                  \n\t\t2          13741.9              [nan nan 3.0e-01]                   [nan nan 6.5e-02]                   [nan nan 4.7e-05]                  \n\t\t3          13807.8              [nan nan 3.1e-01]                   [nan nan 6.4e-02]                   [nan nan 4.0e-06]                  \n\t\t4          13887.0              [nan nan 3.0e-01]                   [nan nan 2.9e-02]                   [nan nan 3.2e-05]                  \n\n\tns[32] = [32768    32  2048]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          13945.2              [nan nan 2.5e-01]                   [nan nan 8.2e-02]                   [nan nan 1.2e-04]                  \n\t\t1          14006.3              [nan nan 2.5e-01]                   [nan nan 5.6e-02]                   [nan nan 2.9e-04]                  \n\t\t2          14079.4              [nan nan 2.5e-01]                   [nan nan 4.2e-02]                   [nan nan 6.5e-06]                  \n\t\t3          14138.8              [nan nan 2.5e-01]                   [nan nan 7.7e-02]                   [nan nan 1.0e-04]                  \n\t\t4          14214.7              [nan nan 2.5e-01]                   [nan nan 3.9e-02]                   [nan nan 7.5e-05]                  \n\n\tns[33] = [32768    32   512]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          14290.2              [nan nan 2.6e-01]                   [nan nan 4.4e-02]                   [nan nan 5.7e-06]                  \n\t\t1          14349.3              [nan nan 2.5e-01]                   [nan nan 1.1e-01]                   [nan nan 1.3e-03]                  \n\t\t2          14408.7              [nan nan 2.6e-01]                   [nan nan 1.2e-01]                   [nan nan 3.5e-04]                  \n\t\t3          14484.5              [nan nan 2.6e-01]                   [nan nan 3.5e-02]                   [nan nan 1.0e-04]                  \n\t\t4          14542.9              [nan nan 2.6e-01]                   [nan nan 1.4e-01]                   [nan nan 5.0e-04]                  \n\n\tns[34] = [32768    32   128]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          14626.5              [nan nan 3.3e-01]                   [nan nan 1.2e-01]                   [nan nan 7.5e-04]                  \n\t\t1          14687.4              [nan nan 3.3e-01]                   [nan nan 1.9e-01]                   [nan nan 2.7e-03]                  \n\t\t2          14747.4              [nan nan 3.3e-01]                   [nan nan 1.6e-01]                   [nan nan 8.9e-04]                  \n\t\t3          14812.5              [nan nan 3.3e-01]                   [nan nan 1.6e-01]                   [nan nan 8.1e-04]                  \n\t\t4          14872.9              [nan nan 3.3e-01]                   [nan nan 1.7e-01]                   [nan nan 2.1e-03]                  \n\n\tns[35] = [32768    32    32]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          14946.3              [nan nan 6.3e-01]                   [nan nan 2.3e-01]                   [nan nan 2.4e-03]                  \n\t\t1          15021.0              [nan nan 6.2e-01]                   [nan nan 2.6e-01]                   [nan nan 6.4e-03]                  \n\t\t2          15134.4              [nan nan 6.3e-01]                   [nan nan 1.8e-01]                   [nan nan 5.9e-03]                  \n\t\t3          15246.9              [nan nan 6.3e-01]                   [nan nan 1.7e-01]                   [nan nan 7.8e-04]                  \n\t\t4          15360.1              [nan nan 6.3e-01]                   [nan nan 1.5e-01]                   [nan nan 2.0e-03]                  \n\n\tns[36] = [ 8192 32768 32768]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          15475.5              [nan nan 1.1e-01]                   [nan nan 2.0e-02]                   [nan nan 2.5e-06]                  \n\t\t1          15591.7              [nan nan 1.1e-01]                   [nan nan 2.2e-02]                   [nan nan 6.4e-06]                  \n\t\t2          15706.7              [nan nan 1.1e-01]                   [nan nan 2.6e-02]                   [nan nan 9.0e-06]                  \n\t\t3          15824.1              [nan nan 1.1e-01]                   [nan nan 2.1e-02]                   [nan nan 7.1e-07]                  \n\t\t4          15939.8              [nan nan 1.1e-01]                   [nan nan 3.0e-02]                   [nan nan 5.2e-06]                  \n\n\tns[37] = [ 8192 32768  8192]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          16005.8              [nan nan 8.9e-02]                   [nan nan 3.0e-02]                   [nan nan 6.0e-06]                  \n\t\t1          16068.9              [nan nan 8.8e-02]                   [nan nan 1.3e-01]                   [nan nan 1.4e-04]                  \n\t\t2          16138.7              [nan nan 9.1e-02]                   [nan nan 3.1e-02]                   [nan nan 6.8e-06]                  \n\t\t3          16203.0              [nan nan 9.3e-02]                   [nan nan 2.5e-02]                   [nan nan 1.3e-05]                  \n\t\t4          16272.9              [nan nan 9.2e-02]                   [nan nan 2.9e-02]                   [nan nan 1.3e-05]                  \n\n\tns[38] = [ 8192 32768  2048]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          16332.6              [nan nan 8.6e-02]                   [nan nan 6.5e-02]                   [nan nan 5.4e-05]                  \n\t\t1          16390.8              [nan nan 9.2e-02]                   [nan nan 4.8e-02]                   [nan nan 1.3e-04]                  \n\t\t2          16455.3              [nan nan 8.9e-02]                   [nan nan 2.9e-02]                   [nan nan 1.2e-04]                  \n\t\t3          16519.3              [nan nan 9.2e-02]                   [nan nan 2.9e-02]                   [nan nan 1.3e-06]                  \n\t\t4          16577.0              [nan nan 8.9e-02]                   [nan nan 6.2e-02]                   [nan nan 4.1e-05]                  \n\n\tns[39] = [ 8192 32768   512]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          16635.5              [nan nan 9.6e-02]                   [nan nan 3.0e-02]                   [nan nan 1.7e-04]                  \n\t\t1          16692.9              [nan nan 9.5e-02]                   [nan nan 5.9e-02]                   [nan nan 3.7e-04]                  \n\t\t2          16753.3              [nan nan 9.5e-02]                   [nan nan 4.2e-02]                   [nan nan 2.7e-04]                  \n\t\t3          16817.9              [nan nan 9.7e-02]                   [nan nan 3.5e-02]                   [nan nan 2.3e-04]                  \n\t\t4          16881.1              [nan nan 9.6e-02]                   [nan nan 2.9e-02]                   [nan nan 1.8e-04]                  \n\n\tns[40] = [ 8192 32768   128]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          16939.0              [nan nan 1.4e-01]                   [nan nan 2.3e-01]                   [nan nan 1.1e-03]                  \n\t\t1          17004.6              [nan nan 1.5e-01]                   [nan nan 3.4e-02]                   [nan nan 4.4e-04]                  \n\t\t2          17072.9              [nan nan 1.5e-01]                   [nan nan 3.6e-02]                   [nan nan 1.0e-03]                  \n\t\t3          17132.5              [nan nan 1.4e-01]                   [nan nan 2.4e-01]                   [nan nan 1.7e-03]                  \n\t\t4          17202.1              [nan nan 1.4e-01]                   [nan nan 3.1e-02]                   [nan nan 6.3e-04]                  \n\n\tns[41] = [ 8192 32768    32]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          17272.6              [nan nan 3.1e-01]                   [nan nan 3.7e-02]                   [nan nan 2.5e-04]                  \n\t\t1          17338.1              [nan nan 3.0e-01]                   [nan nan 4.2e-01]                   [nan nan 1.3e-02]                  \n\t\t2          17403.3              [nan nan 3.0e-01]                   [nan nan 6.9e-02]                   [nan nan 2.5e-03]                  \n\t\t3          17468.8              [nan nan 3.1e-01]                   [nan nan 1.6e-01]                   [nan nan 3.3e-03]                  \n\t\t4          17532.7              [nan nan 3.1e-01]                   [nan nan 3.1e-01]                   [nan nan 1.2e-03]                  \n\n\tns[42] = [ 8192  8192 32768]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          17602.7              [nan nan 9.2e-02]                   [nan nan 3.0e-02]                   [nan nan 7.1e-06]                  \n\t\t1          17671.5              [nan nan 8.7e-02]                   [nan nan 2.3e-02]                   [nan nan 9.3e-06]                  \n\t\t2          17739.9              [nan nan 8.8e-02]                   [nan nan 3.3e-02]                   [nan nan 2.4e-05]                  \n\t\t3          17809.7              [nan nan 8.8e-02]                   [nan nan 2.6e-02]                   [nan nan 9.2e-06]                  \n\t\t4          17878.4              [nan nan 8.8e-02]                   [nan nan 2.9e-02]                   [nan nan 6.4e-06]                  \n\n\tns[43] = [8192 8192 8192]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          17895.5              [nan nan 5.4e-02]                   [nan nan 5.7e-02]                   [nan nan 1.0e-05]                  \n\t\t1          17916.1              [nan nan 5.5e-02]                   [nan nan 3.8e-02]                   [nan nan 3.3e-05]                  \n\t\t2          17934.5              [nan nan 5.6e-02]                   [nan nan 4.9e-02]                   [nan nan 2.0e-05]                  \n\t\t3          17955.2              [nan nan 5.6e-02]                   [nan nan 3.9e-02]                   [nan nan 5.0e-05]                  \n\t\t4          17972.5              [nan nan 5.6e-02]                   [nan nan 4.6e-02]                   [nan nan 5.1e-05]                  \n\n\tns[44] = [8192 8192 2048]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          17985.6              [nan nan 5.3e-02]                   [nan nan 4.9e-02]                   [nan nan 2.0e-04]                  \n\t\t1          17999.2              [nan nan 5.1e-02]                   [nan nan 5.3e-02]                   [nan nan 3.0e-05]                  \n\t\t2          18015.4              [nan nan 5.0e-02]                   [nan nan 5.6e-02]                   [nan nan 7.8e-05]                  \n\t\t3          18028.6              [nan nan 5.2e-02]                   [nan nan 4.6e-02]                   [nan nan 1.5e-05]                  \n\t\t4          18045.0              [nan nan 5.2e-02]                   [nan nan 6.5e-02]                   [nan nan 6.1e-06]                  \n\n\tns[45] = [8192 8192  512]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          18057.3              [nan nan 5.3e-02]                   [nan nan 6.4e-02]                   [nan nan 5.6e-05]                  \n\t\t1          18069.5              [nan nan 5.4e-02]                   [nan nan 7.8e-02]                   [nan nan 1.0e-04]                  \n\t\t2          18081.8              [nan nan 5.4e-02]                   [nan nan 5.4e-02]                   [nan nan 2.8e-04]                  \n\t\t3          18097.5              [nan nan 5.4e-02]                   [nan nan 6.4e-02]                   [nan nan 1.3e-04]                  \n\t\t4          18110.6              [nan nan 5.3e-02]                   [nan nan 6.5e-02]                   [nan nan 3.7e-06]                  \n\n\tns[46] = [8192 8192  128]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          18126.6              [nan nan 5.8e-02]                   [nan nan 4.6e-02]                   [nan nan 9.4e-06]                  \n\t\t1          18138.8              [nan nan 5.8e-02]                   [nan nan 6.6e-02]                   [nan nan 1.4e-03]                  \n\t\t2          18152.8              [nan nan 5.7e-02]                   [nan nan 7.4e-02]                   [nan nan 7.6e-04]                  \n\t\t3          18165.0              [nan nan 5.8e-02]                   [nan nan 6.4e-02]                   [nan nan 7.0e-04]                  \n\t\t4          18181.0              [nan nan 5.8e-02]                   [nan nan 6.1e-02]                   [nan nan 2.7e-04]                  \n\n\tns[47] = [8192 8192   32]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          18194.1              [nan nan 8.3e-02]                   [nan nan 3.2e-01]                   [nan nan 3.8e-03]                  \n\t\t1          18212.3              [nan nan 8.1e-02]                   [nan nan 4.2e-02]                   [nan nan 1.1e-03]                  \n\t\t2          18225.4              [nan nan 8.1e-02]                   [nan nan 4.6e-01]                   [nan nan 7.7e-03]                  \n\t\t3          18238.4              [nan nan 8.1e-02]                   [nan nan 7.4e-01]                   [nan nan 2.7e-02]                  \n\t\t4          18251.4              [nan nan 8.1e-02]                   [nan nan 7.0e-02]                   [nan nan 7.9e-04]                  \n\n\tns[48] = [ 8192  2048 32768]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          18317.5              [nan nan 9.1e-02]                   [nan nan 3.0e-02]                   [nan nan 2.0e-05]                  \n\t\t1          18383.6              [nan nan 9.0e-02]                   [nan nan 2.4e-02]                   [nan nan 9.6e-06]                  \n\t\t2          18449.9              [nan nan 8.8e-02]                   [nan nan 2.7e-02]                   [nan nan 9.3e-06]                  \n\t\t3          18515.0              [nan nan 8.5e-02]                   [nan nan 2.9e-02]                   [nan nan 4.0e-06]                  \n\t\t4          18580.8              [nan nan 8.8e-02]                   [nan nan 3.4e-02]                   [nan nan 3.4e-05]                  \n\n\tns[49] = [8192 2048 8192]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          18597.1              [nan nan 4.9e-02]                   [nan nan 6.9e-02]                   [nan nan 1.1e-04]                  \n\t\t1          18610.1              [nan nan 5.0e-02]                   [nan nan 5.5e-02]                   [nan nan 4.3e-05]                  \n\t\t2          18626.4              [nan nan 5.1e-02]                   [nan nan 5.6e-02]                   [nan nan 3.6e-05]                  \n\t\t3          18642.8              [nan nan 5.1e-02]                   [nan nan 4.2e-02]                   [nan nan 1.6e-04]                  \n\t\t4          18659.3              [nan nan 5.1e-02]                   [nan nan 5.9e-02]                   [nan nan 7.7e-05]                  \n\n\tns[50] = [8192 2048 2048]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          18668.4              [nan nan 3.9e-02]                   [nan nan 7.2e-02]                   [nan nan 1.2e-04]                  \n\t\t1          18677.2              [nan nan 3.8e-02]                   [nan nan 7.0e-02]                   [nan nan 4.1e-05]                  \n\t\t2          18685.8              [nan nan 3.9e-02]                   [nan nan 6.1e-02]                   [nan nan 3.2e-04]                  \n\t\t3          18694.6              [nan nan 3.9e-02]                   [nan nan 4.8e-02]                   [nan nan 6.8e-05]                  \n\t\t4          18703.3              [nan nan 3.9e-02]                   [nan nan 5.4e-02]                   [nan nan 1.1e-04]                  \n\n\tns[51] = [8192 2048  512]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          18713.6              [nan nan 3.9e-02]                   [nan nan 7.6e-02]                   [nan nan 2.0e-04]                  \n\t\t1          18723.8              [nan nan 3.9e-02]                   [nan nan 6.6e-02]                   [nan nan 1.5e-04]                  \n\t\t2          18732.5              [nan nan 3.9e-02]                   [nan nan 7.5e-02]                   [nan nan 7.8e-06]                  \n\t\t3          18740.2              [nan nan 3.9e-02]                   [nan nan 7.9e-02]                   [nan nan 5.5e-06]                  \n\t\t4          18747.7              [nan nan 3.9e-02]                   [nan nan 8.5e-02]                   [nan nan 2.0e-05]                  \n\n\tns[52] = [8192 2048  128]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          18758.2              [nan nan 4.2e-02]                   [nan nan 6.8e-02]                   [nan nan 7.0e-06]                  \n\t\t1          18766.1              [nan nan 4.4e-02]                   [nan nan 6.4e-02]                   [nan nan 6.3e-04]                  \n\t\t2          18773.9              [nan nan 4.4e-02]                   [nan nan 1.1e-01]                   [nan nan 4.5e-04]                  \n\t\t3          18781.8              [nan nan 4.4e-02]                   [nan nan 8.3e-02]                   [nan nan 5.5e-05]                  \n\t\t4          18789.2              [nan nan 4.3e-02]                   [nan nan 9.0e-02]                   [nan nan 1.9e-05]                  \n\n\tns[53] = [8192 2048   32]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          18797.3              [nan nan 5.3e-02]                   [nan nan 5.2e-02]                   [nan nan 6.8e-04]                  \n\t\t1          18808.8              [nan nan 5.3e-02]                   [nan nan 7.8e-02]                   [nan nan 2.3e-03]                  \n\t\t2          18817.3              [nan nan 5.2e-02]                   [nan nan 6.5e-02]                   [nan nan 1.3e-03]                  \n\t\t3          18825.4              [nan nan 5.3e-02]                   [nan nan 9.2e-02]                   [nan nan 1.5e-03]                  \n\t\t4          18833.6              [nan nan 5.2e-02]                   [nan nan 4.1e-01]                   [nan nan 8.2e-03]                  \n\n\tns[54] = [ 8192   512 32768]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          18900.7              [nan nan 9.6e-02]                   [nan nan 2.7e-02]                   [nan nan 1.9e-05]                  \n\t\t1          18965.7              [nan nan 9.4e-02]                   [nan nan 3.2e-02]                   [nan nan 2.6e-05]                  \n\t\t2          19029.7              [nan nan 9.4e-02]                   [nan nan 3.1e-02]                   [nan nan 1.8e-05]                  \n\t\t3          19090.5              [nan nan 9.4e-02]                   [nan nan 2.4e-02]                   [nan nan 1.7e-05]                  \n\t\t4          19157.1              [nan nan 9.4e-02]                   [nan nan 2.6e-02]                   [nan nan 1.2e-05]                  \n\n\tns[55] = [8192  512 8192]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          19169.3              [nan nan 5.2e-02]                   [nan nan 6.7e-02]                   [nan nan 5.2e-05]                  \n\t\t1          19181.4              [nan nan 5.1e-02]                   [nan nan 6.1e-02]                   [nan nan 2.0e-05]                  \n\t\t2          19193.6              [nan nan 5.2e-02]                   [nan nan 4.0e-02]                   [nan nan 8.7e-05]                  \n\t\t3          19209.2              [nan nan 5.3e-02]                   [nan nan 4.3e-02]                   [nan nan 3.7e-05]                  \n\t\t4          19222.1              [nan nan 5.3e-02]                   [nan nan 5.0e-02]                   [nan nan 5.8e-05]                  \n\n\tns[56] = [8192  512 2048]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          19229.9              [nan nan 3.9e-02]                   [nan nan 9.1e-02]                   [nan nan 3.2e-04]                  \n\t\t1          19238.6              [nan nan 3.9e-02]                   [nan nan 6.9e-02]                   [nan nan 2.2e-04]                  \n\t\t2          19246.1              [nan nan 3.8e-02]                   [nan nan 7.6e-02]                   [nan nan 1.7e-05]                  \n\t\t3          19253.7              [nan nan 3.8e-02]                   [nan nan 8.4e-02]                   [nan nan 1.9e-04]                  \n\t\t4          19261.5              [nan nan 4.0e-02]                   [nan nan 7.3e-02]                   [nan nan 5.9e-05]                  \n\n\tns[57] = [8192  512  512]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          19268.4              [nan nan 3.8e-02]                   [nan nan 6.6e-02]                   [nan nan 2.0e-04]                  \n\t\t1          19275.3              [nan nan 3.8e-02]                   [nan nan 9.2e-02]                   [nan nan 4.7e-04]                  \n\t\t2          19282.1              [nan nan 3.7e-02]                   [nan nan 9.9e-02]                   [nan nan 7.1e-04]                  \n\t\t3          19291.4              [nan nan 3.8e-02]                   [nan nan 4.4e-02]                   [nan nan 5.4e-04]                  \n\t\t4          19298.2              [nan nan 3.8e-02]                   [nan nan 9.1e-02]                   [nan nan 2.6e-04]                  \n\n\tns[58] = [8192  512  128]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          19305.1              [nan nan 4.1e-02]                   [nan nan 1.1e-01]                   [nan nan 1.0e-04]                  \n\t\t1          19311.9              [nan nan 4.1e-02]                   [nan nan 8.7e-02]                   [nan nan 6.7e-04]                  \n\t\t2          19319.9              [nan nan 4.2e-02]                   [nan nan 5.4e-02]                   [nan nan 6.9e-04]                  \n\t\t3          19326.7              [nan nan 4.1e-02]                   [nan nan 9.7e-02]                   [nan nan 3.1e-04]                  \n\t\t4          19336.1              [nan nan 4.1e-02]                   [nan nan 7.0e-02]                   [nan nan 3.9e-04]                  \n\n\tns[59] = [8192  512   32]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          19343.2              [nan nan 4.9e-02]                   [nan nan 1.8e-01]                   [nan nan 4.0e-03]                  \n\t\t1          19350.2              [nan nan 4.9e-02]                   [nan nan 1.1e-01]                   [nan nan 1.6e-04]                  \n\t\t2          19357.3              [nan nan 4.8e-02]                   [nan nan 1.1e-01]                   [nan nan 1.9e-03]                  \n\t\t3          19364.4              [nan nan 4.8e-02]                   [nan nan 1.3e-01]                   [nan nan 1.2e-03]                  \n\t\t4          19374.7              [nan nan 4.8e-02]                   [nan nan 8.0e-02]                   [nan nan 4.0e-04]                  \n\n\tns[60] = [ 8192   128 32768]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          19445.8              [nan nan 1.5e-01]                   [nan nan 3.8e-02]                   [nan nan 2.5e-05]                  \n\t\t1          19516.1              [nan nan 1.4e-01]                   [nan nan 4.0e-02]                   [nan nan 3.5e-05]                  \n\t\t2          19588.5              [nan nan 1.4e-01]                   [nan nan 3.0e-02]                   [nan nan 2.2e-05]                  \n\t\t3          19659.1              [nan nan 1.4e-01]                   [nan nan 2.5e-02]                   [nan nan 5.7e-06]                  \n\t\t4          19727.3              [nan nan 1.4e-01]                   [nan nan 3.0e-02]                   [nan nan 4.4e-06]                  \n\n\tns[61] = [8192  128 8192]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          19740.7              [nan nan 5.9e-02]                   [nan nan 5.3e-02]                   [nan nan 3.3e-05]                  \n\t\t1          19753.7              [nan nan 5.6e-02]                   [nan nan 6.9e-02]                   [nan nan 9.4e-05]                  \n\t\t2          19766.1              [nan nan 5.6e-02]                   [nan nan 4.6e-02]                   [nan nan 1.2e-04]                  \n\t\t3          19780.0              [nan nan 5.7e-02]                   [nan nan 5.6e-02]                   [nan nan 3.3e-05]                  \n\t\t4          19792.2              [nan nan 5.7e-02]                   [nan nan 5.8e-02]                   [nan nan 3.9e-05]                  \n\n\tns[62] = [8192  128 2048]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          19802.6              [nan nan 4.2e-02]                   [nan nan 5.7e-02]                   [nan nan 9.5e-05]                  \n\t\t1          19810.5              [nan nan 4.4e-02]                   [nan nan 5.5e-02]                   [nan nan 1.3e-04]                  \n\t\t2          19818.3              [nan nan 4.3e-02]                   [nan nan 8.8e-02]                   [nan nan 2.0e-04]                  \n\t\t3          19826.1              [nan nan 4.4e-02]                   [nan nan 5.6e-02]                   [nan nan 1.3e-04]                  \n\t\t4          19834.2              [nan nan 4.5e-02]                   [nan nan 6.2e-02]                   [nan nan 3.1e-04]                  \n\n\tns[63] = [8192  128  512]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          19841.1              [nan nan 4.2e-02]                   [nan nan 7.6e-02]                   [nan nan 1.3e-04]                  \n\t\t1          19848.7              [nan nan 4.1e-02]                   [nan nan 9.3e-02]                   [nan nan 5.9e-04]                  \n\t\t2          19855.5              [nan nan 4.2e-02]                   [nan nan 1.0e-01]                   [nan nan 3.3e-04]                  \n\t\t3          19864.9              [nan nan 4.2e-02]                   [nan nan 7.2e-02]                   [nan nan 5.5e-05]                  \n\t\t4          19871.8              [nan nan 4.1e-02]                   [nan nan 1.0e-01]                   [nan nan 5.4e-04]                  \n\n\tns[64] = [8192  128  128]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          19882.0              [nan nan 5.0e-02]                   [nan nan 6.6e-02]                   [nan nan 7.6e-04]                  \n\t\t1          19888.8              [nan nan 4.8e-02]                   [nan nan 1.4e-01]                   [nan nan 1.9e-03]                  \n\t\t2          19895.5              [nan nan 4.6e-02]                   [nan nan 1.3e-01]                   [nan nan 9.5e-04]                  \n\t\t3          19902.3              [nan nan 4.5e-02]                   [nan nan 1.5e-01]                   [nan nan 1.3e-03]                  \n\t\t4          19909.1              [nan nan 4.7e-02]                   [nan nan 1.4e-01]                   [nan nan 3.7e-04]                  \n\n\tns[65] = [8192  128   32]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          19916.0              [nan nan 5.1e-02]                   [nan nan 1.4e-01]                   [nan nan 8.1e-04]                  \n\t\t1          19923.8              [nan nan 5.3e-02]                   [nan nan 1.4e-01]                   [nan nan 4.3e-04]                  \n\t\t2          19930.7              [nan nan 5.2e-02]                   [nan nan 1.7e-01]                   [nan nan 2.8e-03]                  \n\t\t3          19937.7              [nan nan 5.3e-02]                   [nan nan 1.5e-01]                   [nan nan 4.7e-03]                  \n\t\t4          19944.7              [nan nan 5.2e-02]                   [nan nan 1.9e-01]                   [nan nan 1.7e-03]                  \n\n\tns[66] = [ 8192    32 32768]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          20031.1              [nan nan 3.2e-01]                   [nan nan 2.7e-02]                   [nan nan 1.7e-05]                  \n\t\t1          20117.1              [nan nan 3.0e-01]                   [nan nan 2.9e-02]                   [nan nan 9.2e-06]                  \n\t\t2          20202.3              [nan nan 3.0e-01]                   [nan nan 3.1e-02]                   [nan nan 1.7e-05]                  \n\t\t3          20273.3              [nan nan 3.0e-01]                   [nan nan 5.5e-02]                   [nan nan 5.2e-05]                  \n\t\t4          20360.0              [nan nan 3.0e-01]                   [nan nan 3.0e-02]                   [nan nan 6.7e-06]                  \n\n\tns[67] = [8192   32 8192]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          20372.9              [nan nan 7.9e-02]                   [nan nan 6.3e-02]                   [nan nan 3.9e-05]                  \n\t\t1          20387.8              [nan nan 7.9e-02]                   [nan nan 6.5e-02]                   [nan nan 1.0e-05]                  \n\t\t2          20406.0              [nan nan 8.0e-02]                   [nan nan 6.7e-02]                   [nan nan 1.9e-05]                  \n\t\t3          20419.0              [nan nan 8.0e-02]                   [nan nan 4.7e-02]                   [nan nan 1.1e-04]                  \n\t\t4          20431.9              [nan nan 7.9e-02]                   [nan nan 7.8e-02]                   [nan nan 1.2e-04]                  \n\n\tns[68] = [8192   32 2048]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          20439.6              [nan nan 5.1e-02]                   [nan nan 8.8e-02]                   [nan nan 5.1e-05]                  \n\t\t1          20447.5              [nan nan 5.1e-02]                   [nan nan 6.9e-02]                   [nan nan 8.0e-05]                  \n\t\t2          20458.8              [nan nan 5.2e-02]                   [nan nan 7.4e-02]                   [nan nan 5.2e-05]                  \n\t\t3          20466.8              [nan nan 5.1e-02]                   [nan nan 7.4e-02]                   [nan nan 1.2e-04]                  \n\t\t4          20474.8              [nan nan 5.2e-02]                   [nan nan 9.6e-02]                   [nan nan 1.6e-04]                  \n\n\tns[69] = [8192   32  512]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          20484.8              [nan nan 4.8e-02]                   [nan nan 7.7e-02]                   [nan nan 1.5e-04]                  \n\t\t1          20492.9              [nan nan 4.9e-02]                   [nan nan 5.6e-02]                   [nan nan 2.1e-04]                  \n\t\t2          20500.4              [nan nan 4.8e-02]                   [nan nan 8.6e-02]                   [nan nan 2.0e-04]                  \n\t\t3          20507.3              [nan nan 4.8e-02]                   [nan nan 1.1e-01]                   [nan nan 4.0e-04]                  \n\t\t4          20514.3              [nan nan 4.9e-02]                   [nan nan 1.1e-01]                   [nan nan 6.5e-04]                  \n\n\tns[70] = [8192   32  128]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          20524.4              [nan nan 5.0e-02]                   [nan nan 9.3e-02]                   [nan nan 3.4e-04]                  \n\t\t1          20531.6              [nan nan 5.2e-02]                   [nan nan 9.4e-02]                   [nan nan 2.2e-03]                  \n\t\t2          20538.5              [nan nan 5.0e-02]                   [nan nan 1.2e-01]                   [nan nan 6.6e-04]                  \n\t\t3          20545.4              [nan nan 5.1e-02]                   [nan nan 1.6e-01]                   [nan nan 1.3e-03]                  \n\t\t4          20555.8              [nan nan 5.3e-02]                   [nan nan 7.4e-02]                   [nan nan 4.9e-04]                  \n\n\tns[71] = [8192   32   32]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          20567.4              [nan nan 6.6e-02]                   [nan nan 1.7e-01]                   [nan nan 3.4e-03]                  \n\t\t1          20575.0              [nan nan 6.8e-02]                   [nan nan 1.6e-01]                   [nan nan 3.2e-03]                  \n\t\t2          20582.5              [nan nan 6.7e-02]                   [nan nan 2.3e-01]                   [nan nan 4.7e-03]                  \n\t\t3          20589.9              [nan nan 6.6e-02]                   [nan nan 2.0e-01]                   [nan nan 2.2e-03]                  \n\t\t4          20597.3              [nan nan 6.8e-02]                   [nan nan 2.0e-01]                   [nan nan 2.3e-03]                  \n\n\tns[72] = [ 2048 32768 32768]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          20709.9              [nan nan 1.1e-01]                   [nan nan 2.6e-02]                   [nan nan 4.0e-06]                  \n\t\t1          20819.0              [nan nan 1.1e-01]                   [nan nan 2.8e-02]                   [nan nan 2.3e-05]                  \n\t\t2          20928.6              [nan nan 1.1e-01]                   [nan nan 3.0e-02]                   [nan nan 3.6e-05]                  \n\t\t3          21039.6              [nan nan 1.1e-01]                   [nan nan 2.4e-02]                   [nan nan 2.5e-05]                  \n\t\t4          21151.9              [nan nan 1.1e-01]                   [nan nan 3.8e-02]                   [nan nan 4.4e-05]                  \n\n\tns[73] = [ 2048 32768  8192]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          21216.8              [nan nan 8.8e-02]                   [nan nan 2.6e-02]                   [nan nan 2.8e-05]                  \n\t\t1          21279.3              [nan nan 9.0e-02]                   [nan nan 2.8e-02]                   [nan nan 6.2e-05]                  \n\t\t2          21336.1              [nan nan 8.7e-02]                   [nan nan 4.9e-02]                   [nan nan 1.7e-05]                  \n\t\t3          21394.2              [nan nan 9.1e-02]                   [nan nan 4.6e-02]                   [nan nan 4.1e-06]                  \n\t\t4          21458.6              [nan nan 9.0e-02]                   [nan nan 2.6e-02]                   [nan nan 5.7e-05]                  \n\n\tns[74] = [ 2048 32768  2048]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          21513.1              [nan nan 8.4e-02]                   [nan nan 5.4e-02]                   [nan nan 2.7e-04]                  \n\t\t1          21568.3              [nan nan 8.2e-02]                   [nan nan 5.8e-02]                   [nan nan 3.4e-06]                  \n\t\t2          21628.5              [nan nan 8.6e-02]                   [nan nan 4.9e-02]                   [nan nan 8.0e-05]                  \n\t\t3          21687.4              [nan nan 8.6e-02]                   [nan nan 2.9e-02]                   [nan nan 8.4e-05]                  \n\t\t4          21741.2              [nan nan 8.2e-02]                   [nan nan 7.3e-02]                   [nan nan 1.1e-04]                  \n\n\tns[75] = [ 2048 32768   512]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          21798.9              [nan nan 8.2e-02]                   [nan nan 4.7e-02]                   [nan nan 1.9e-04]                  \n\t\t1          21859.3              [nan nan 9.1e-02]                   [nan nan 3.1e-02]                   [nan nan 1.8e-04]                  \n\t\t2          21911.6              [nan nan 9.2e-02]                   [nan nan 8.2e-02]                   [nan nan 6.1e-04]                  \n\t\t3          21964.9              [nan nan 8.8e-02]                   [nan nan 7.2e-02]                   [nan nan 2.2e-04]                  \n\t\t4          22024.7              [nan nan 9.0e-02]                   [nan nan 3.1e-02]                   [nan nan 3.9e-04]                  \n\n\tns[76] = [ 2048 32768   128]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          22087.5              [nan nan 1.1e-01]                   [nan nan 2.9e-02]                   [nan nan 4.2e-04]                  \n\t\t1          22150.7              [nan nan 1.1e-01]                   [nan nan 3.9e-02]                   [nan nan 4.1e-04]                  \n\t\t2          22212.9              [nan nan 1.1e-01]                   [nan nan 4.2e-02]                   [nan nan 5.6e-04]                  \n\t\t3          22274.2              [nan nan 1.2e-01]                   [nan nan 4.5e-02]                   [nan nan 6.5e-04]                  \n\t\t4          22335.3              [nan nan 1.1e-01]                   [nan nan 3.8e-02]                   [nan nan 5.9e-04]                  \n\n\tns[77] = [ 2048 32768    32]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          22412.6              [nan nan 2.6e-01]                   [nan nan 4.7e-02]                   [nan nan 7.3e-04]                  \n\t\t1          22472.3              [nan nan 2.4e-01]                   [nan nan 8.2e-02]                   [nan nan 3.7e-04]                  \n\t\t2          22548.3              [nan nan 2.5e-01]                   [nan nan 3.6e-02]                   [nan nan 8.6e-05]                  \n\t\t3          22608.6              [nan nan 2.4e-01]                   [nan nan 1.0e-01]                   [nan nan 2.4e-03]                  \n\t\t4          22683.9              [nan nan 2.5e-01]                   [nan nan 3.5e-02]                   [nan nan 8.0e-04]                  \n\n\tns[78] = [ 2048  8192 32768]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          22742.0              [nan nan 8.8e-02]                   [nan nan 2.6e-02]                   [nan nan 1.9e-05]                  \n\t\t1          22807.8              [nan nan 9.1e-02]                   [nan nan 2.8e-02]                   [nan nan 4.0e-06]                  \n\t\t2          22871.6              [nan nan 8.8e-02]                   [nan nan 2.7e-02]                   [nan nan 2.6e-05]                  \n\t\t3          22934.5              [nan nan 8.9e-02]                   [nan nan 2.5e-02]                   [nan nan 1.5e-05]                  \n\t\t4          23000.2              [nan nan 9.1e-02]                   [nan nan 3.6e-02]                   [nan nan 1.9e-05]                  \n\n\tns[79] = [2048 8192 8192]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          23016.5              [nan nan 4.8e-02]                   [nan nan 5.0e-02]                   [nan nan 9.4e-05]                  \n\t\t1          23032.7              [nan nan 5.0e-02]                   [nan nan 6.8e-02]                   [nan nan 3.7e-05]                  \n\t\t2          23049.0              [nan nan 5.0e-02]                   [nan nan 3.2e-02]                   [nan nan 3.9e-05]                  \n\t\t3          23065.2              [nan nan 5.1e-02]                   [nan nan 4.4e-02]                   [nan nan 4.9e-05]                  \n\t\t4          23078.7              [nan nan 5.1e-02]                   [nan nan 3.9e-02]                   [nan nan 1.1e-04]                  \n\n\tns[80] = [2048 8192 2048]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          23089.8              [nan nan 3.8e-02]                   [nan nan 6.3e-02]                   [nan nan 2.7e-05]                  \n\t\t1          23098.6              [nan nan 3.8e-02]                   [nan nan 6.6e-02]                   [nan nan 6.4e-05]                  \n\t\t2          23109.8              [nan nan 3.9e-02]                   [nan nan 6.9e-02]                   [nan nan 1.3e-04]                  \n\t\t3          23118.4              [nan nan 3.9e-02]                   [nan nan 6.9e-02]                   [nan nan 4.9e-05]                  \n\t\t4          23129.5              [nan nan 3.8e-02]                   [nan nan 6.0e-02]                   [nan nan 6.2e-05]                  \n\n\tns[81] = [2048 8192  512]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          23138.3              [nan nan 3.9e-02]                   [nan nan 3.6e-02]                   [nan nan 4.4e-04]                  \n\t\t1          23146.2              [nan nan 4.0e-02]                   [nan nan 6.6e-02]                   [nan nan 5.9e-04]                  \n\t\t2          23154.0              [nan nan 3.8e-02]                   [nan nan 7.8e-02]                   [nan nan 3.8e-04]                  \n\t\t3          23161.8              [nan nan 3.9e-02]                   [nan nan 8.8e-02]                   [nan nan 2.5e-04]                  \n\t\t4          23169.5              [nan nan 3.9e-02]                   [nan nan 7.4e-02]                   [nan nan 2.0e-04]                  \n\n\tns[82] = [2048 8192  128]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          23178.4              [nan nan 4.2e-02]                   [nan nan 8.1e-02]                   [nan nan 6.3e-04]                  \n\t\t1          23186.2              [nan nan 4.4e-02]                   [nan nan 5.6e-02]                   [nan nan 1.1e-03]                  \n\t\t2          23194.0              [nan nan 4.3e-02]                   [nan nan 8.4e-02]                   [nan nan 5.2e-04]                  \n\t\t3          23204.6              [nan nan 4.4e-02]                   [nan nan 7.7e-02]                   [nan nan 1.2e-03]                  \n\t\t4          23212.3              [nan nan 4.3e-02]                   [nan nan 9.8e-02]                   [nan nan 4.2e-05]                  \n\n\tns[83] = [2048 8192   32]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          23220.4              [nan nan 5.2e-02]                   [nan nan 1.0e-01]                   [nan nan 2.7e-03]                  \n\t\t1          23228.5              [nan nan 5.3e-02]                   [nan nan 9.8e-02]                   [nan nan 2.4e-03]                  \n\t\t2          23236.6              [nan nan 5.2e-02]                   [nan nan 1.1e-01]                   [nan nan 4.1e-04]                  \n\t\t3          23247.3              [nan nan 5.2e-02]                   [nan nan 6.8e-02]                   [nan nan 1.8e-03]                  \n\t\t4          23255.4              [nan nan 5.3e-02]                   [nan nan 7.4e-02]                   [nan nan 1.4e-03]                  \n\n\tns[84] = [ 2048  2048 32768]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          23316.8              [nan nan 8.9e-02]                   [nan nan 2.7e-02]                   [nan nan 1.0e-05]                  \n\t\t1          23377.7              [nan nan 8.5e-02]                   [nan nan 3.0e-02]                   [nan nan 3.4e-06]                  \n\t\t2          23438.5              [nan nan 8.3e-02]                   [nan nan 3.1e-02]                   [nan nan 3.3e-05]                  \n\t\t3          23500.0              [nan nan 8.6e-02]                   [nan nan 2.5e-02]                   [nan nan 5.3e-06]                  \n\t\t4          23561.0              [nan nan 8.3e-02]                   [nan nan 2.9e-02]                   [nan nan 1.9e-05]                  \n\n\tns[85] = [2048 2048 8192]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          23572.1              [nan nan 3.7e-02]                   [nan nan 6.8e-02]                   [nan nan 1.1e-05]                  \n\t\t1          23583.4              [nan nan 3.8e-02]                   [nan nan 7.0e-02]                   [nan nan 9.6e-05]                  \n\t\t2          23594.7              [nan nan 3.9e-02]                   [nan nan 6.9e-02]                   [nan nan 3.2e-05]                  \n\t\t3          23605.8              [nan nan 3.8e-02]                   [nan nan 5.8e-02]                   [nan nan 6.0e-05]                  \n\t\t4          23614.5              [nan nan 3.7e-02]                   [nan nan 4.3e-02]                   [nan nan 1.0e-04]                  \n\n\tns[86] = [2048 2048 2048]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          23618.3              [nan nan 1.6e-02]                   [nan nan 9.7e-02]                   [nan nan 3.3e-04]                  \n\t\t1          23623.3              [nan nan 1.6e-02]                   [nan nan 7.4e-02]                   [nan nan 1.7e-05]                  \n\t\t2          23627.1              [nan nan 1.6e-02]                   [nan nan 7.9e-02]                   [nan nan 4.4e-04]                  \n\t\t3          23631.0              [nan nan 1.6e-02]                   [nan nan 9.8e-02]                   [nan nan 2.5e-04]                  \n\t\t4          23635.0              [nan nan 1.6e-02]                   [nan nan 1.0e-01]                   [nan nan 2.1e-04]                  \n\n\tns[87] = [2048 2048  512]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          23638.0              [nan nan 1.5e-02]                   [nan nan 1.0e-01]                   [nan nan 8.7e-04]                  \n\t\t1          23641.1              [nan nan 1.6e-02]                   [nan nan 8.9e-02]                   [nan nan 2.4e-04]                  \n\t\t2          23644.0              [nan nan 1.5e-02]                   [nan nan 1.1e-01]                   [nan nan 2.4e-04]                  \n\t\t3          23646.9              [nan nan 1.5e-02]                   [nan nan 1.1e-01]                   [nan nan 3.0e-04]                  \n\t\t4          23650.1              [nan nan 1.5e-02]                   [nan nan 9.9e-02]                   [nan nan 5.2e-04]                  \n\n\tns[88] = [2048 2048  128]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          23653.1              [nan nan 1.6e-02]                   [nan nan 1.0e-01]                   [nan nan 7.7e-04]                  \n\t\t1          23656.0              [nan nan 1.6e-02]                   [nan nan 1.0e-01]                   [nan nan 1.3e-04]                  \n\t\t2          23659.1              [nan nan 1.6e-02]                   [nan nan 1.1e-01]                   [nan nan 2.0e-04]                  \n\t\t3          23661.9              [nan nan 1.6e-02]                   [nan nan 1.2e-01]                   [nan nan 9.5e-04]                  \n\t\t4          23664.7              [nan nan 1.6e-02]                   [nan nan 8.8e-02]                   [nan nan 3.7e-04]                  \n\n\tns[89] = [2048 2048   32]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          23667.9              [nan nan 1.9e-02]                   [nan nan 8.9e-02]                   [nan nan 4.7e-04]                  \n\t\t1          23670.8              [nan nan 2.0e-02]                   [nan nan 1.2e-01]                   [nan nan 1.7e-03]                  \n\t\t2          23674.0              [nan nan 2.0e-02]                   [nan nan 8.3e-02]                   [nan nan 8.2e-04]                  \n\t\t3          23677.1              [nan nan 2.0e-02]                   [nan nan 6.4e-02]                   [nan nan 7.8e-04]                  \n\t\t4          23680.7              [nan nan 2.0e-02]                   [nan nan 9.9e-02]                   [nan nan 4.7e-04]                  \n\n\tns[90] = [ 2048   512 32768]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          23739.5              [nan nan 8.2e-02]                   [nan nan 2.5e-02]                   [nan nan 1.2e-05]                  \n\t\t1          23797.6              [nan nan 9.0e-02]                   [nan nan 3.1e-02]                   [nan nan 1.2e-05]                  \n\t\t2          23857.4              [nan nan 9.0e-02]                   [nan nan 3.4e-02]                   [nan nan 4.9e-08]                  \n\t\t3          23917.3              [nan nan 8.9e-02]                   [nan nan 3.2e-02]                   [nan nan 2.6e-05]                  \n\t\t4          23978.2              [nan nan 9.0e-02]                   [nan nan 2.6e-02]                   [nan nan 1.1e-05]                  \n\n\tns[91] = [2048  512 8192]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          23986.1              [nan nan 3.7e-02]                   [nan nan 5.1e-02]                   [nan nan 1.1e-04]                  \n\t\t1          23993.9              [nan nan 3.7e-02]                   [nan nan 6.4e-02]                   [nan nan 3.0e-05]                  \n\t\t2          24001.7              [nan nan 3.8e-02]                   [nan nan 4.3e-02]                   [nan nan 7.5e-05]                  \n\t\t3          24009.7              [nan nan 3.8e-02]                   [nan nan 6.2e-02]                   [nan nan 4.1e-05]                  \n\t\t4          24018.2              [nan nan 3.9e-02]                   [nan nan 7.2e-02]                   [nan nan 3.5e-05]                  \n\n\tns[92] = [2048  512 2048]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          24022.2              [nan nan 1.6e-02]                   [nan nan 9.7e-02]                   [nan nan 2.5e-04]                  \n\t\t1          24025.3              [nan nan 1.6e-02]                   [nan nan 7.9e-02]                   [nan nan 1.5e-04]                  \n\t\t2          24028.2              [nan nan 1.6e-02]                   [nan nan 7.6e-02]                   [nan nan 2.6e-04]                  \n\t\t3          24031.2              [nan nan 1.6e-02]                   [nan nan 7.2e-02]                   [nan nan 7.1e-04]                  \n\t\t4          24034.3              [nan nan 1.5e-02]                   [nan nan 9.6e-02]                   [nan nan 4.3e-05]                  \n\n\tns[93] = [2048  512  512]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          24049.8              [nan 1.3e-01 1.2e-02]               [nan 1.3e-01 1.1e-01]               [nan 1.1e-03 3.0e-04]              \n\t\t1          24065.5              [nan 1.3e-01 1.1e-02]               [nan 1.2e-01 1.1e-01]               [nan 1.4e-03 1.1e-04]              \n\t\t2          24081.2              [nan 1.3e-01 1.1e-02]               [nan 1.4e-01 1.1e-01]               [nan 6.2e-04 5.9e-04]              \n\t\t3          24096.5              [nan 1.3e-01 1.1e-02]               [nan 1.3e-01 9.2e-02]               [nan 2.9e-03 7.1e-04]              \n\t\t4          24112.2              [nan 1.3e-01 1.1e-02]               [nan 1.3e-01 1.1e-01]               [nan 9.4e-05 4.8e-04]              \n\n\tns[94] = [2048  512  128]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          24123.2              [nan 9.0e-02 1.2e-02]               [nan 1.5e-01 1.4e-01]               [nan 2.1e-03 1.1e-03]              \n\t\t1          24134.3              [nan 9.0e-02 1.3e-02]               [nan 2.2e-01 1.3e-01]               [nan 2.9e-03 8.0e-04]              \n\t\t2          24145.3              [nan 8.9e-02 1.3e-02]               [nan 1.2e-01 1.1e-01]               [nan 5.4e-03 1.0e-03]              \n\t\t3          24156.3              [nan 8.9e-02 1.2e-02]               [nan 1.2e-01 1.2e-01]               [nan 1.1e-03 7.1e-05]              \n\t\t4          24167.3              [nan 9.0e-02 1.2e-02]               [nan 1.3e-01 1.2e-01]               [nan 3.4e-03 4.9e-05]              \n\n\tns[95] = [2048  512   32]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          24181.3              [nan 1.2e-01 1.3e-02]               [nan 2.5e-01 9.2e-02]               [nan 6.3e-03 1.6e-03]              \n\t\t1          24192.0              [nan 8.5e-02 1.4e-02]               [nan 5.0e-01 1.3e-01]               [nan 2.7e-03 1.4e-03]              \n\t\t2          24202.9              [nan 8.8e-02 1.4e-02]               [nan 2.2e-01 1.1e-01]               [nan 1.4e-03 4.8e-04]              \n\t\t3          24214.3              [nan 8.4e-02 1.4e-02]               [nan 5.1e-01 9.6e-02]               [nan 8.4e-03 1.0e-03]              \n\t\t4          24224.7              [nan 8.3e-02 1.4e-02]               [nan 3.0e-01 1.5e-01]               [nan 2.8e-03 2.8e-03]              \n\n\tns[96] = [ 2048   128 32768]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          24285.6              [nan nan 1.1e-01]                   [nan nan 3.2e-02]                   [nan nan 1.2e-05]                  \n\t\t1          24347.4              [nan nan 1.1e-01]                   [nan nan 2.8e-02]                   [nan nan 1.3e-05]                  \n\t\t2          24409.6              [nan nan 1.1e-01]                   [nan nan 3.0e-02]                   [nan nan 1.5e-05]                  \n\t\t3          24472.2              [nan nan 1.1e-01]                   [nan nan 3.8e-02]                   [nan nan 2.9e-05]                  \n\t\t4          24528.3              [nan nan 1.1e-01]                   [nan nan 3.1e-02]                   [nan nan 3.0e-05]                  \n\n\tns[97] = [2048  128 8192]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          24536.5              [nan nan 4.9e-02]                   [nan nan 4.9e-02]                   [nan nan 5.3e-05]                  \n\t\t1          24547.0              [nan nan 4.2e-02]                   [nan nan 6.3e-02]                   [nan nan 7.7e-06]                  \n\t\t2          24555.4              [nan nan 4.3e-02]                   [nan nan 6.6e-02]                   [nan nan 1.3e-05]                  \n\t\t3          24566.0              [nan nan 4.3e-02]                   [nan nan 7.3e-02]                   [nan nan 9.7e-05]                  \n\t\t4          24573.8              [nan nan 4.0e-02]                   [nan nan 6.9e-02]                   [nan nan 1.1e-04]                  \n\n\tns[98] = [2048  128 2048]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          24576.7              [nan nan 1.6e-02]                   [nan nan 9.1e-02]                   [nan nan 1.5e-05]                  \n\t\t1          24579.5              [nan nan 1.6e-02]                   [nan nan 8.6e-02]                   [nan nan 3.4e-05]                  \n\t\t2          24582.4              [nan nan 1.6e-02]                   [nan nan 9.1e-02]                   [nan nan 2.9e-04]                  \n\t\t3          24585.3              [nan nan 1.6e-02]                   [nan nan 6.2e-02]                   [nan nan 4.9e-04]                  \n\t\t4          24588.4              [nan nan 1.7e-02]                   [nan nan 9.3e-02]                   [nan nan 9.4e-05]                  \n\n\tns[99] = [2048  128  512]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          24599.6              [nan 9.0e-02 1.2e-02]               [nan 1.3e-01 9.6e-02]               [nan 8.1e-05 9.3e-05]              \n\t\t1          24611.1              [nan 9.1e-02 1.3e-02]               [nan 1.3e-01 1.0e-01]               [nan 7.4e-05 5.0e-04]              \n\t\t2          24622.2              [nan 9.1e-02 1.2e-02]               [nan 1.3e-01 1.2e-01]               [nan 1.9e-03 2.3e-05]              \n\t\t3          24634.7              [nan 1.0e-01 1.3e-02]               [nan 1.3e-01 1.1e-01]               [nan 1.1e-03 1.8e-04]              \n\t\t4          24646.2              [nan 9.2e-02 1.3e-02]               [nan 1.4e-01 1.0e-01]               [nan 1.5e-03 1.9e-04]              \n\n\tns[100] = [2048  128  128]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          24655.2              [nan 7.1e-02 1.4e-02]               [nan 1.6e-01 1.5e-01]               [nan 2.0e-03 3.2e-03]              \n\t\t1          24664.1              [nan 7.1e-02 1.4e-02]               [nan 1.4e-01 1.5e-01]               [nan 1.2e-03 6.0e-04]              \n\t\t2          24673.0              [nan 7.0e-02 1.4e-02]               [nan 1.6e-01 1.4e-01]               [nan 7.6e-04 5.4e-04]              \n\t\t3          24682.0              [nan 7.1e-02 1.3e-02]               [nan 1.4e-01 1.5e-01]               [nan 7.3e-04 8.7e-04]              \n\t\t4          24691.0              [nan 7.1e-02 1.4e-02]               [nan 1.4e-01 1.5e-01]               [nan 4.2e-04 1.3e-03]              \n\n\tns[101] = [2048  128   32]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          24700.3              [nan 7.4e-02 1.4e-02]               [nan 3.1e-01 1.8e-01]               [nan 3.1e-03 2.3e-03]              \n\t\t1          24709.0              [nan 6.7e-02 1.4e-02]               [nan 1.7e-01 1.1e-01]               [nan 6.2e-04 1.8e-03]              \n\t\t2          24719.1              [nan 8.2e-02 1.4e-02]               [nan 3.4e-01 1.5e-01]               [nan 2.4e-03 1.8e-03]              \n\t\t3          24727.7              [nan 6.7e-02 1.5e-02]               [nan 4.9e-01 1.5e-01]               [nan 6.6e-06 6.6e-04]              \n\t\t4          24737.2              [nan 7.3e-02 1.5e-02]               [nan 2.1e-01 1.1e-01]               [nan 3.8e-03 2.0e-03]              \n\n\tns[102] = [ 2048    32 32768]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          24817.1              [nan nan 2.6e-01]                   [nan nan 3.6e-02]                   [nan nan 2.6e-05]                  \n\t\t1          24892.7              [nan nan 2.5e-01]                   [nan nan 3.3e-02]                   [nan nan 5.0e-05]                  \n\t\t2          24968.1              [nan nan 2.5e-01]                   [nan nan 2.9e-02]                   [nan nan 1.2e-05]                  \n\t\t3          25045.0              [nan nan 2.5e-01]                   [nan nan 4.8e-02]                   [nan nan 2.9e-05]                  \n\t\t4          25121.7              [nan nan 2.5e-01]                   [nan nan 3.2e-02]                   [nan nan 1.8e-05]                  \n\n\tns[103] = [2048   32 8192]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          25133.5              [nan nan 4.8e-02]                   [nan nan 6.4e-02]                   [nan nan 1.2e-06]                  \n\t\t1          25141.5              [nan nan 5.1e-02]                   [nan nan 4.7e-02]                   [nan nan 1.6e-04]                  \n\t\t2          25149.5              [nan nan 5.1e-02]                   [nan nan 6.8e-02]                   [nan nan 2.2e-05]                  \n\t\t3          25158.6              [nan nan 5.1e-02]                   [nan nan 4.2e-02]                   [nan nan 6.4e-05]                  \n\t\t4          25166.6              [nan nan 5.1e-02]                   [nan nan 7.0e-02]                   [nan nan 3.5e-05]                  \n\n\tns[104] = [2048   32 2048]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          25169.8              [nan nan 2.0e-02]                   [nan nan 9.8e-02]                   [nan nan 1.1e-04]                  \n\t\t1          25172.7              [nan nan 1.9e-02]                   [nan nan 1.0e-01]                   [nan nan 7.1e-05]                  \n\t\t2          25175.8              [nan nan 2.0e-02]                   [nan nan 8.0e-02]                   [nan nan 3.0e-04]                  \n\t\t3          25178.7              [nan nan 2.0e-02]                   [nan nan 1.1e-01]                   [nan nan 1.8e-04]                  \n\t\t4          25181.7              [nan nan 2.0e-02]                   [nan nan 1.0e-01]                   [nan nan 2.0e-04]                  \n\n\tns[105] = [2048   32  512]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          25192.2              [nan 8.3e-02 1.4e-02]               [nan 1.2e-01 1.1e-01]               [nan 4.3e-04 6.1e-06]              \n\t\t1          25203.2              [nan 8.5e-02 1.3e-02]               [nan 1.4e-01 1.1e-01]               [nan 3.7e-04 5.5e-05]              \n\t\t2          25214.2              [nan 8.8e-02 1.5e-02]               [nan 2.0e-01 1.3e-01]               [nan 6.5e-05 1.6e-03]              \n\t\t3          25225.0              [nan 8.6e-02 1.3e-02]               [nan 1.4e-01 1.2e-01]               [nan 3.8e-04 4.4e-04]              \n\t\t4          25235.5              [nan 8.5e-02 1.4e-02]               [nan 1.4e-01 1.1e-01]               [nan 7.3e-03 1.4e-04]              \n\n\tns[106] = [2048   32  128]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          25244.1              [nan 6.6e-02 1.4e-02]               [nan 1.7e-01 1.6e-01]               [nan 8.8e-05 6.7e-04]              \n\t\t1          25252.4              [nan 6.5e-02 1.4e-02]               [nan 1.9e-01 1.3e-01]               [nan 1.8e-02 2.9e-04]              \n\t\t2          25261.6              [nan 7.3e-02 1.4e-02]               [nan 1.2e-01 1.6e-01]               [nan 4.7e-03 1.8e-03]              \n\t\t3          25269.9              [nan 6.4e-02 1.4e-02]               [nan 1.3e-01 1.5e-01]               [nan 2.7e-03 1.2e-03]              \n\t\t4          25278.4              [nan 6.8e-02 1.4e-02]               [nan 1.7e-01 1.7e-01]               [nan 1.6e-03 1.8e-03]              \n\n\tns[107] = [2048   32   32]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          25286.3              [nan 6.0e-02 1.5e-02]               [nan 2.5e-01 1.3e-01]               [nan 8.5e-04 1.6e-03]              \n\t\t1          25293.9              [nan 5.9e-02 1.5e-02]               [nan 1.4e-01 2.6e-01]               [nan 7.8e-04 5.7e-03]              \n\t\t2          25301.7              [nan 5.9e-02 1.5e-02]               [nan 2.4e-01 2.2e-01]               [nan 6.1e-03 6.8e-03]              \n\t\t3          25310.0              [nan 6.4e-02 1.5e-02]               [nan 2.1e-01 1.5e-01]               [nan 5.4e-03 4.3e-03]              \n\t\t4          25318.1              [nan 6.2e-02 1.6e-02]               [nan 2.1e-01 1.4e-01]               [nan 2.3e-03 1.4e-03]              \n\n\tns[108] = [  512 32768 32768]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          25431.3              [nan nan 1.3e-01]                   [nan nan 2.8e-02]                   [nan nan 1.5e-05]                  \n\t\t1          25544.5              [nan nan 1.2e-01]                   [nan nan 4.8e-02]                   [nan nan 4.7e-05]                  \n\t\t2          25657.2              [nan nan 1.2e-01]                   [nan nan 2.1e-02]                   [nan nan 2.0e-05]                  \n\t\t3          25767.8              [nan nan 1.2e-01]                   [nan nan 2.4e-02]                   [nan nan 7.4e-06]                  \n\t\t4          25880.7              [nan nan 1.2e-01]                   [nan nan 2.3e-02]                   [nan nan 1.1e-05]                  \n\n\tns[109] = [  512 32768  8192]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          25944.1              [nan nan 9.5e-02]                   [nan nan 3.7e-02]                   [nan nan 1.5e-05]                  \n\t\t1          26001.9              [nan nan 9.3e-02]                   [nan nan 4.6e-02]                   [nan nan 3.2e-05]                  \n\t\t2          26064.7              [nan nan 9.4e-02]                   [nan nan 2.8e-02]                   [nan nan 6.1e-05]                  \n\t\t3          26121.7              [nan nan 9.5e-02]                   [nan nan 4.6e-02]                   [nan nan 2.3e-05]                  \n\t\t4          26186.0              [nan nan 9.6e-02]                   [nan nan 3.1e-02]                   [nan nan 1.1e-05]                  \n\n\tns[110] = [  512 32768  2048]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          26239.7              [nan nan 8.9e-02]                   [nan nan 7.3e-02]                   [nan nan 1.0e-04]                  \n\t\t1          26291.1              [nan nan 8.1e-02]                   [nan nan 8.4e-02]                   [nan nan 3.2e-04]                  \n\t\t2          26350.9              [nan nan 9.2e-02]                   [nan nan 3.3e-02]                   [nan nan 7.8e-05]                  \n\t\t3          26410.5              [nan nan 9.1e-02]                   [nan nan 2.9e-02]                   [nan nan 5.0e-05]                  \n\t\t4          26471.0              [nan nan 9.2e-02]                   [nan nan 3.1e-02]                   [nan nan 1.0e-04]                  \n\n\tns[111] = [  512 32768   512]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          26530.7              [nan nan 1.0e-01]                   [nan nan 2.8e-02]                   [nan nan 7.5e-05]                  \n\t\t1          26581.8              [nan nan 1.0e-01]                   [nan nan 1.1e-01]                   [nan nan 1.8e-05]                  \n\t\t2          26633.5              [nan nan 1.0e-01]                   [nan nan 1.2e-01]                   [nan nan 6.8e-04]                  \n\t\t3          26685.8              [nan nan 1.0e-01]                   [nan nan 9.9e-02]                   [nan nan 4.9e-04]                  \n\t\t4          26739.6              [nan nan 1.0e-01]                   [nan nan 8.9e-02]                   [nan nan 5.3e-05]                  \n\n\tns[112] = [  512 32768   128]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          26800.4              [nan nan 1.2e-01]                   [nan nan 4.1e-02]                   [nan nan 5.7e-04]                  \n\t\t1          26862.3              [nan nan 1.2e-01]                   [nan nan 5.0e-02]                   [nan nan 7.7e-04]                  \n\t\t2          26924.7              [nan nan 1.2e-01]                   [nan nan 4.0e-02]                   [nan nan 9.8e-05]                  \n\t\t3          26979.5              [nan nan 1.3e-01]                   [nan nan 8.7e-02]                   [nan nan 1.3e-03]                  \n\t\t4          27043.2              [nan nan 1.2e-01]                   [nan nan 9.7e-02]                   [nan nan 8.2e-04]                  \n\n\tns[113] = [  512 32768    32]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          27104.8              [nan nan 2.7e-01]                   [nan nan 1.2e-01]                   [nan nan 4.9e-04]                  \n\t\t1          27165.8              [nan nan 2.6e-01]                   [nan nan 1.4e-01]                   [nan nan 3.9e-04]                  \n\t\t2          27241.4              [nan nan 2.5e-01]                   [nan nan 7.4e-02]                   [nan nan 1.4e-03]                  \n\t\t3          27301.1              [nan nan 2.5e-01]                   [nan nan 1.4e-01]                   [nan nan 7.1e-04]                  \n\t\t4          27377.0              [nan nan 2.5e-01]                   [nan nan 4.9e-02]                   [nan nan 9.8e-05]                  \n\n\tns[114] = [  512  8192 32768]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          27435.0              [nan nan 9.6e-02]                   [nan nan 2.9e-02]                   [nan nan 2.8e-05]                  \n\t\t1          27499.2              [nan nan 9.7e-02]                   [nan nan 2.7e-02]                   [nan nan 6.2e-06]                  \n\t\t2          27564.8              [nan nan 9.4e-02]                   [nan nan 2.8e-02]                   [nan nan 1.6e-05]                  \n\t\t3          27630.0              [nan nan 9.4e-02]                   [nan nan 2.7e-02]                   [nan nan 2.3e-05]                  \n\t\t4          27695.4              [nan nan 9.6e-02]                   [nan nan 2.9e-02]                   [nan nan 2.8e-05]                  \n\n\tns[115] = [ 512 8192 8192]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          27708.2              [nan nan 5.0e-02]                   [nan nan 7.0e-02]                   [nan nan 8.5e-05]                  \n\t\t1          27720.2              [nan nan 5.2e-02]                   [nan nan 4.0e-02]                   [nan nan 1.6e-04]                  \n\t\t2          27735.7              [nan nan 5.2e-02]                   [nan nan 5.2e-02]                   [nan nan 2.4e-05]                  \n\t\t3          27751.2              [nan nan 5.3e-02]                   [nan nan 6.4e-02]                   [nan nan 1.2e-04]                  \n\t\t4          27764.1              [nan nan 5.2e-02]                   [nan nan 3.6e-02]                   [nan nan 1.3e-04]                  \n\n\tns[116] = [ 512 8192 2048]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          27771.9              [nan nan 3.9e-02]                   [nan nan 7.1e-02]                   [nan nan 5.9e-06]                  \n\t\t1          27779.6              [nan nan 3.9e-02]                   [nan nan 8.3e-02]                   [nan nan 9.6e-05]                  \n\t\t2          27787.5              [nan nan 4.0e-02]                   [nan nan 5.5e-02]                   [nan nan 2.1e-04]                  \n\t\t3          27795.1              [nan nan 3.8e-02]                   [nan nan 7.4e-02]                   [nan nan 3.6e-05]                  \n\t\t4          27802.8              [nan nan 3.9e-02]                   [nan nan 5.3e-02]                   [nan nan 3.3e-04]                  \n\n\tns[117] = [ 512 8192  512]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          27810.0              [nan nan 3.7e-02]                   [nan nan 7.2e-02]                   [nan nan 4.5e-04]                  \n\t\t1          27817.0              [nan nan 3.8e-02]                   [nan nan 9.9e-02]                   [nan nan 3.0e-05]                  \n\t\t2          27826.3              [nan nan 3.9e-02]                   [nan nan 8.2e-02]                   [nan nan 1.6e-04]                  \n\t\t3          27833.1              [nan nan 3.7e-02]                   [nan nan 8.3e-02]                   [nan nan 4.6e-04]                  \n\t\t4          27842.3              [nan nan 3.8e-02]                   [nan nan 7.6e-02]                   [nan nan 3.9e-05]                  \n\n\tns[118] = [ 512 8192  128]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          27849.1              [nan nan 4.2e-02]                   [nan nan 1.2e-01]                   [nan nan 3.7e-05]                  \n\t\t1          27855.9              [nan nan 4.2e-02]                   [nan nan 9.8e-02]                   [nan nan 1.8e-05]                  \n\t\t2          27862.6              [nan nan 4.1e-02]                   [nan nan 1.0e-01]                   [nan nan 3.5e-04]                  \n\t\t3          27871.6              [nan nan 4.1e-02]                   [nan nan 7.3e-02]                   [nan nan 6.6e-04]                  \n\t\t4          27878.3              [nan nan 4.1e-02]                   [nan nan 9.4e-02]                   [nan nan 5.9e-04]                  \n\n\tns[119] = [ 512 8192   32]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          27885.3              [nan nan 4.9e-02]                   [nan nan 9.6e-02]                   [nan nan 1.7e-03]                  \n\t\t1          27895.5              [nan nan 4.8e-02]                   [nan nan 7.6e-02]                   [nan nan 1.2e-04]                  \n\t\t2          27902.5              [nan nan 4.8e-02]                   [nan nan 9.3e-02]                   [nan nan 7.4e-05]                  \n\t\t3          27912.7              [nan nan 4.9e-02]                   [nan nan 7.1e-02]                   [nan nan 7.4e-04]                  \n\t\t4          27922.9              [nan nan 4.9e-02]                   [nan nan 8.0e-02]                   [nan nan 1.1e-03]                  \n\n\tns[120] = [  512  2048 32768]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          27981.5              [nan nan 8.2e-02]                   [nan nan 2.8e-02]                   [nan nan 1.2e-05]                  \n\t\t1          28042.4              [nan nan 9.1e-02]                   [nan nan 2.7e-02]                   [nan nan 1.2e-05]                  \n\t\t2          28101.7              [nan nan 9.1e-02]                   [nan nan 3.7e-02]                   [nan nan 3.3e-05]                  \n\t\t3          28162.2              [nan nan 8.8e-02]                   [nan nan 4.1e-02]                   [nan nan 3.0e-05]                  \n\t\t4          28221.2              [nan nan 9.2e-02]                   [nan nan 2.8e-02]                   [nan nan 4.6e-06]                  \n\n\tns[121] = [ 512 2048 8192]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          28231.3              [nan nan 3.6e-02]                   [nan nan 4.6e-02]                   [nan nan 1.0e-04]                  \n\t\t1          28239.0              [nan nan 3.7e-02]                   [nan nan 7.2e-02]                   [nan nan 1.1e-04]                  \n\t\t2          28249.2              [nan nan 3.8e-02]                   [nan nan 5.5e-02]                   [nan nan 5.1e-05]                  \n\t\t3          28257.0              [nan nan 3.9e-02]                   [nan nan 5.6e-02]                   [nan nan 7.1e-05]                  \n\t\t4          28267.4              [nan nan 3.7e-02]                   [nan nan 7.1e-02]                   [nan nan 8.6e-05]                  \n\n\tns[122] = [ 512 2048 2048]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          28270.4              [nan nan 1.5e-02]                   [nan nan 1.0e-01]                   [nan nan 1.9e-04]                  \n\t\t1          28273.5              [nan nan 1.6e-02]                   [nan nan 8.3e-02]                   [nan nan 2.8e-05]                  \n\t\t2          28277.5              [nan nan 1.5e-02]                   [nan nan 7.4e-02]                   [nan nan 4.3e-05]                  \n\t\t3          28280.5              [nan nan 1.5e-02]                   [nan nan 6.3e-02]                   [nan nan 4.4e-04]                  \n\t\t4          28283.5              [nan nan 1.5e-02]                   [nan nan 8.2e-02]                   [nan nan 1.3e-04]                  \n\n\tns[123] = [ 512 2048  512]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          28299.8              [nan 1.4e-01 1.1e-02]               [nan 1.3e-01 1.3e-01]               [nan 7.0e-04 5.3e-04]              \n\t\t1          28316.0              [nan 1.3e-01 1.1e-02]               [nan 1.3e-01 1.1e-01]               [nan 1.8e-03 3.2e-04]              \n\t\t2          28332.1              [nan 1.3e-01 1.1e-02]               [nan 1.2e-01 1.0e-01]               [nan 9.2e-04 1.0e-05]              \n\t\t3          28348.2              [nan 1.3e-01 1.1e-02]               [nan 1.3e-01 1.0e-01]               [nan 4.2e-03 2.6e-04]              \n\t\t4          28364.2              [nan 1.3e-01 1.1e-02]               [nan 1.2e-01 8.5e-02]               [nan 1.2e-04 2.9e-04]              \n\n\tns[124] = [ 512 2048  128]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          28375.3              [nan 9.1e-02 1.3e-02]               [nan 1.5e-01 1.3e-01]               [nan 1.9e-03 1.2e-03]              \n\t\t1          28386.5              [nan 9.0e-02 1.3e-02]               [nan 1.4e-01 1.4e-01]               [nan 4.9e-04 3.1e-04]              \n\t\t2          28398.6              [nan 9.2e-02 1.2e-02]               [nan 1.6e-01 8.3e-02]               [nan 4.2e-03 1.7e-03]              \n\t\t3          28409.7              [nan 9.1e-02 1.2e-02]               [nan 1.5e-01 1.4e-01]               [nan 2.5e-03 5.8e-04]              \n\t\t4          28421.6              [nan 8.9e-02 1.3e-02]               [nan 1.4e-01 9.9e-02]               [nan 3.3e-03 1.7e-04]              \n\n\tns[125] = [ 512 2048   32]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          28432.0              [nan 8.5e-02 1.3e-02]               [nan 2.2e-01 1.2e-01]               [nan 2.6e-03 4.6e-04]              \n\t\t1          28443.8              [nan 9.6e-02 1.4e-02]               [nan 1.7e-01 1.3e-01]               [nan 1.3e-03 6.1e-04]              \n\t\t2          28456.7              [nan 1.1e-01 1.4e-02]               [nan 1.9e-01 1.3e-01]               [nan 3.8e-03 1.6e-03]              \n\t\t3          28469.5              [nan 1.1e-01 1.4e-02]               [nan 3.7e-01 1.1e-01]               [nan 6.4e-03 3.2e-03]              \n\t\t4          28485.5              [nan 1.3e-01 1.4e-02]               [nan 3.4e-01 1.1e-01]               [nan 7.0e-03 1.0e-03]              \n\n\tns[126] = [  512   512 32768]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          28545.2              [nan nan 9.9e-02]                   [nan nan 3.7e-02]                   [nan nan 2.0e-05]                  \n\t\t1          28604.9              [nan nan 9.9e-02]                   [nan nan 3.2e-02]                   [nan nan 2.7e-05]                  \n\t\t2          28666.0              [nan nan 1.0e-01]                   [nan nan 3.6e-02]                   [nan nan 7.1e-06]                  \n\t\t3          28726.1              [nan nan 9.9e-02]                   [nan nan 4.7e-02]                   [nan nan 6.6e-05]                  \n\t\t4          28787.2              [nan nan 1.0e-01]                   [nan nan 3.6e-02]                   [nan nan 2.3e-06]                  \n\n\tns[127] = [ 512  512 8192]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          28794.1              [nan nan 3.6e-02]                   [nan nan 6.0e-02]                   [nan nan 3.0e-05]                  \n\t\t1          28800.9              [nan nan 3.6e-02]                   [nan nan 7.7e-02]                   [nan nan 1.0e-04]                  \n\t\t2          28807.6              [nan nan 3.6e-02]                   [nan nan 6.9e-02]                   [nan nan 1.4e-06]                  \n\t\t3          28814.4              [nan nan 3.6e-02]                   [nan nan 5.6e-02]                   [nan nan 2.0e-04]                  \n\t\t4          28823.5              [nan nan 3.6e-02]                   [nan nan 7.0e-02]                   [nan nan 7.6e-05]                  \n\n\tns[128] = [ 512  512 2048]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          28838.6              [nan 1.3e-01 1.1e-02]               [nan 1.2e-01 1.1e-01]               [nan 1.0e-03 4.7e-04]              \n\t\t1          28854.4              [nan 1.3e-01 1.1e-02]               [nan 1.2e-01 1.0e-01]               [nan 9.3e-04 3.6e-05]              \n\t\t2          28869.6              [nan 1.3e-01 1.2e-02]               [nan 1.2e-01 8.5e-02]               [nan 2.8e-04 1.2e-04]              \n\t\t3          28885.0              [nan 1.3e-01 1.2e-02]               [nan 1.2e-01 1.1e-01]               [nan 2.5e-05 2.1e-04]              \n\t\t4          28901.1              [nan 1.4e-01 1.2e-02]               [nan 1.1e-01 8.7e-02]               [nan 4.7e-04 1.0e-04]              \n\n\tns[129] = [512 512 512]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          28916.4              [2.3e-01 5.0e-02 7.7e-03]           [1.4e-01 1.3e-01 1.1e-01]           [7.5e-05 9.4e-05 4.4e-05]          \n\t\t1          28928.1              [2.3e-01 4.4e-02 7.9e-03]           [1.4e-01 1.5e-01 1.3e-01]           [5.8e-04 9.7e-04 4.8e-04]          \n\t\t2          28942.3              [2.3e-01 5.3e-02 7.7e-03]           [1.4e-01 1.3e-01 1.3e-01]           [2.2e-04 4.6e-04 6.4e-04]          \n\t\t3          28956.0              [2.3e-01 4.4e-02 8.1e-03]           [1.5e-01 1.6e-01 1.2e-01]           [9.5e-04 2.7e-03 3.2e-05]          \n\t\t4          28969.6              [2.3e-01 4.7e-02 7.8e-03]           [1.4e-01 1.5e-01 1.3e-01]           [6.0e-04 7.3e-04 1.0e-03]          \n\n\tns[130] = [512 512 128]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          28977.9              [1.3e-01 2.6e-02 7.5e-03]           [1.6e-01 1.7e-01 1.3e-01]           [5.9e-04 2.0e-03 1.2e-04]          \n\t\t1          28986.5              [1.3e-01 3.2e-02 7.8e-03]           [1.7e-01 1.7e-01 1.6e-01]           [6.6e-04 1.2e-03 3.0e-04]          \n\t\t2          28995.6              [1.3e-01 3.6e-02 7.7e-03]           [1.9e-01 2.1e-01 1.1e-01]           [5.3e-04 8.9e-04 1.5e-04]          \n\t\t3          29003.6              [1.2e-01 3.0e-02 7.6e-03]           [1.3e-01 1.3e-01 1.3e-01]           [7.7e-04 7.4e-04 5.6e-04]          \n\t\t4          29011.4              [1.3e-01 3.4e-02 7.5e-03]           [1.4e-01 1.5e-01 1.3e-01]           [6.7e-04 6.1e-04 8.6e-04]          \n\n\tns[131] = [512 512  32]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          29018.6              [1.1e-01 2.7e-02 8.1e-03]           [1.5e-01 1.7e-01 1.3e-01]           [8.7e-04 8.7e-04 1.0e-03]          \n\t\t1          29025.7              [1.1e-01 2.6e-02 8.0e-03]           [1.5e-01 1.6e-01 1.4e-01]           [1.2e-03 4.9e-03 1.3e-03]          \n\t\t2          29033.1              [1.1e-01 2.8e-02 8.2e-03]           [1.7e-01 1.7e-01 1.5e-01]           [2.8e-03 2.4e-03 1.4e-03]          \n\t\t3          29040.3              [1.1e-01 2.6e-02 8.3e-03]           [1.7e-01 1.7e-01 1.6e-01]           [2.0e-03 3.1e-03 3.0e-03]          \n\t\t4          29047.6              [1.1e-01 2.7e-02 8.1e-03]           [1.4e-01 1.4e-01 1.3e-01]           [2.0e-03 2.0e-04 2.0e-03]          \n\n\tns[132] = [  512   128 32768]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          29111.2              [nan nan 1.2e-01]                   [nan nan 4.1e-02]                   [nan nan 2.5e-05]                  \n\t\t1          29173.8              [nan nan 1.2e-01]                   [nan nan 3.4e-02]                   [nan nan 8.3e-06]                  \n\t\t2          29234.9              [nan nan 1.2e-01]                   [nan nan 2.8e-02]                   [nan nan 1.5e-05]                  \n\t\t3          29296.6              [nan nan 1.2e-01]                   [nan nan 2.8e-02]                   [nan nan 1.6e-07]                  \n\t\t4          29358.9              [nan nan 1.2e-01]                   [nan nan 3.7e-02]                   [nan nan 3.3e-05]                  \n\n\tns[133] = [ 512  128 8192]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          29366.7              [nan nan 3.9e-02]                   [nan nan 4.8e-02]                   [nan nan 1.9e-04]                  \n\t\t1          29376.0              [nan nan 4.0e-02]                   [nan nan 6.7e-02]                   [nan nan 3.3e-05]                  \n\t\t2          29385.4              [nan nan 4.0e-02]                   [nan nan 6.2e-02]                   [nan nan 2.7e-05]                  \n\t\t3          29394.6              [nan nan 3.9e-02]                   [nan nan 7.1e-02]                   [nan nan 8.5e-05]                  \n\t\t4          29404.0              [nan nan 4.1e-02]                   [nan nan 7.0e-02]                   [nan nan 5.3e-05]                  \n\n\tns[134] = [ 512  128 2048]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          29415.5              [nan 9.2e-02 1.2e-02]               [nan 1.3e-01 1.0e-01]               [nan 6.4e-04 3.7e-05]              \n\t\t1          29427.3              [nan 8.9e-02 1.2e-02]               [nan 1.2e-01 7.7e-02]               [nan 3.8e-04 2.9e-04]              \n\t\t2          29438.4              [nan 8.9e-02 1.2e-02]               [nan 1.2e-01 9.7e-02]               [nan 5.7e-04 2.6e-04]              \n\t\t3          29450.2              [nan 9.3e-02 1.2e-02]               [nan 1.8e-01 1.2e-01]               [nan 6.4e-04 1.2e-03]              \n\t\t4          29463.1              [nan 1.1e-01 1.2e-02]               [nan 9.5e-02 8.6e-02]               [nan 5.1e-04 2.7e-04]              \n\n\tns[135] = [512 128 512]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          29471.2              [1.2e-01 2.8e-02 7.7e-03]           [1.5e-01 1.5e-01 1.2e-01]           [8.2e-05 2.8e-05 4.4e-04]          \n\t\t1          29480.2              [1.3e-01 3.6e-02 8.1e-03]           [1.6e-01 1.6e-01 1.3e-01]           [1.7e-04 1.2e-03 5.5e-04]          \n\t\t2          29489.1              [1.3e-01 3.7e-02 7.5e-03]           [2.6e-01 2.4e-01 1.5e-01]           [9.0e-04 1.1e-04 2.4e-03]          \n\t\t3          29496.7              [1.3e-01 2.8e-02 7.5e-03]           [1.4e-01 1.5e-01 1.3e-01]           [2.8e-04 4.4e-04 8.7e-04]          \n\t\t4          29505.1              [1.3e-01 3.1e-02 7.8e-03]           [1.5e-01 1.5e-01 1.2e-01]           [1.8e-04 9.7e-04 2.2e-04]          \n\n\tns[136] = [512 128 128]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          29509.4              [6.5e-02 1.5e-02 7.6e-03]           [1.6e-01 1.4e-01 1.4e-01]           [1.0e-03 1.4e-03 2.2e-03]          \n\t\t1          29514.1              [6.3e-02 1.8e-02 7.2e-03]           [1.3e-01 1.2e-01 1.2e-01]           [1.3e-04 5.0e-04 7.9e-05]          \n\t\t2          29518.4              [6.8e-02 1.6e-02 7.3e-03]           [1.7e-01 1.4e-01 1.5e-01]           [1.4e-03 1.3e-03 2.0e-03]          \n\t\t3          29523.1              [6.5e-02 1.8e-02 7.7e-03]           [1.5e-01 1.4e-01 1.5e-01]           [3.9e-04 1.5e-04 4.5e-04]          \n\t\t4          29527.8              [6.6e-02 1.8e-02 7.8e-03]           [1.5e-01 1.5e-01 1.5e-01]           [3.1e-04 2.3e-05 1.4e-03]          \n\n\tns[137] = [512 128  32]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          29531.1              [5.1e-02 1.2e-02 7.2e-03]           [2.2e-01 1.8e-01 1.5e-01]           [1.6e-03 5.2e-03 1.8e-04]          \n\t\t1          29534.4              [5.1e-02 1.3e-02 7.9e-03]           [1.7e-01 1.4e-01 1.9e-01]           [1.6e-03 1.8e-04 7.0e-04]          \n\t\t2          29538.1              [5.2e-02 1.2e-02 8.1e-03]           [1.6e-01 1.4e-01 1.6e-01]           [3.0e-04 2.2e-03 7.4e-04]          \n\t\t3          29541.9              [4.9e-02 1.4e-02 7.3e-03]           [1.9e-01 2.3e-01 1.2e-01]           [2.1e-03 2.5e-03 2.7e-05]          \n\t\t4          29546.0              [5.7e-02 1.5e-02 7.4e-03]           [1.8e-01 1.5e-01 1.5e-01]           [1.4e-03 2.2e-04 1.3e-03]          \n\n\tns[138] = [  512    32 32768]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          29624.9              [nan nan 2.6e-01]                   [nan nan 3.8e-02]                   [nan nan 7.1e-07]                  \n\t\t1          29700.2              [nan nan 2.5e-01]                   [nan nan 3.4e-02]                   [nan nan 1.1e-05]                  \n\t\t2          29776.1              [nan nan 2.5e-01]                   [nan nan 3.0e-02]                   [nan nan 1.0e-05]                  \n\t\t3          29851.9              [nan nan 2.5e-01]                   [nan nan 3.4e-02]                   [nan nan 3.4e-06]                  \n\t\t4          29927.1              [nan nan 2.5e-01]                   [nan nan 3.7e-02]                   [nan nan 4.0e-06]                  \n\n\tns[139] = [ 512   32 8192]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          29934.5              [nan nan 4.3e-02]                   [nan nan 7.5e-02]                   [nan nan 1.6e-05]                  \n\t\t1          29943.0              [nan nan 4.4e-02]                   [nan nan 4.6e-02]                   [nan nan 7.3e-05]                  \n\t\t2          29949.9              [nan nan 4.5e-02]                   [nan nan 5.2e-02]                   [nan nan 1.8e-04]                  \n\t\t3          29956.8              [nan nan 4.6e-02]                   [nan nan 4.7e-02]                   [nan nan 1.1e-04]                  \n\t\t4          29966.7              [nan nan 4.6e-02]                   [nan nan 7.3e-02]                   [nan nan 6.3e-05]                  \n\n\tns[140] = [ 512   32 2048]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          29978.0              [nan 9.1e-02 1.3e-02]               [nan 1.0e-01 1.1e-01]               [nan 5.0e-04 1.4e-04]              \n\t\t1          29990.8              [nan 9.8e-02 1.4e-02]               [nan 1.1e-01 9.0e-02]               [nan 7.4e-05 1.2e-04]              \n\t\t2          30002.6              [nan 9.8e-02 1.4e-02]               [nan 1.2e-01 1.1e-01]               [nan 7.2e-05 2.8e-04]              \n\t\t3          30013.6              [nan 9.0e-02 1.2e-02]               [nan 1.2e-01 1.0e-01]               [nan 5.8e-04 2.2e-04]              \n\t\t4          30023.8              [nan 8.2e-02 1.2e-02]               [nan 1.4e-01 1.1e-01]               [nan 5.6e-04 1.7e-04]              \n\n\tns[141] = [512  32 512]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          30030.8              [1.0e-01 2.5e-02 8.7e-03]           [1.5e-01 1.5e-01 1.2e-01]           [6.9e-04 2.8e-05 2.6e-04]          \n\t\t1          30037.4              [1.0e-01 2.1e-02 8.5e-03]           [1.5e-01 1.5e-01 1.2e-01]           [8.5e-04 7.8e-05 2.3e-04]          \n\t\t2          30043.5              [1.1e-01 2.4e-02 8.4e-03]           [1.6e-01 1.4e-01 1.7e-01]           [6.2e-04 2.3e-04 9.1e-04]          \n\t\t3          30049.9              [1.0e-01 2.2e-02 7.9e-03]           [1.7e-01 1.5e-01 1.3e-01]           [6.7e-04 5.1e-04 2.4e-04]          \n\t\t4          30055.5              [1.1e-01 2.0e-02 8.2e-03]           [1.6e-01 1.6e-01 1.7e-01]           [5.5e-04 7.4e-04 9.0e-04]          \n\n\tns[142] = [512  32 128]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          30059.4              [5.2e-02 1.3e-02 7.5e-03]           [1.5e-01 1.4e-01 1.5e-01]           [4.3e-04 1.1e-03 6.6e-04]          \n\t\t1          30063.2              [5.1e-02 1.3e-02 7.1e-03]           [1.7e-01 1.4e-01 1.5e-01]           [7.9e-04 7.8e-05 5.8e-04]          \n\t\t2          30066.8              [4.8e-02 1.3e-02 7.3e-03]           [1.8e-01 1.7e-01 1.3e-01]           [7.8e-04 1.7e-04 1.6e-03]          \n\t\t3          30071.0              [5.1e-02 1.3e-02 7.3e-03]           [1.6e-01 1.5e-01 1.4e-01]           [1.8e-04 5.0e-04 8.7e-04]          \n\t\t4          30074.4              [5.0e-02 1.0e-02 7.2e-03]           [1.7e-01 1.6e-01 1.6e-01]           [2.3e-03 2.1e-03 1.5e-03]          \n\n\tns[143] = [512  32  32]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          30077.4              [3.8e-02 1.1e-02 7.0e-03]           [2.4e-01 1.9e-01 2.5e-01]           [5.0e-03 2.9e-03 9.4e-03]          \n\t\t1          30080.5              [3.6e-02 1.1e-02 7.3e-03]           [2.2e-01 1.5e-01 1.4e-01]           [3.9e-03 2.4e-03 1.2e-03]          \n\t\t2          30083.2              [3.8e-02 1.0e-02 6.8e-03]           [2.2e-01 1.6e-01 1.9e-01]           [2.8e-03 1.5e-04 4.0e-03]          \n\t\t3          30085.9              [3.5e-02 9.6e-03 7.6e-03]           [1.7e-01 1.5e-01 2.2e-01]           [2.2e-03 2.1e-03 9.9e-03]          \n\t\t4          30089.2              [4.1e-02 1.2e-02 7.3e-03]           [1.8e-01 1.6e-01 1.6e-01]           [2.6e-03 3.1e-03 4.6e-03]          \n\n\tns[144] = [  128 32768 32768]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          30215.6              [nan nan 2.5e-01]                   [nan nan 2.6e-02]                   [nan nan 1.3e-05]                  \n\t\t1          30320.4              [nan nan 2.4e-01]                   [nan nan 3.3e-02]                   [nan nan 4.5e-05]                  \n\t\t2          30435.7              [nan nan 2.4e-01]                   [nan nan 2.1e-02]                   [nan nan 2.6e-05]                  \n\t\t3          30557.8              [nan nan 2.4e-01]                   [nan nan 2.9e-02]                   [nan nan 1.8e-05]                  \n\t\t4          30685.2              [nan nan 2.4e-01]                   [nan nan 3.0e-02]                   [nan nan 1.5e-05]                  \n\n\tns[145] = [  128 32768  8192]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          30744.4              [nan nan 1.4e-01]                   [nan nan 5.6e-02]                   [nan nan 1.0e-04]                  \n\t\t1          30814.0              [nan nan 1.4e-01]                   [nan nan 3.3e-02]                   [nan nan 2.8e-05]                  \n\t\t2          30883.1              [nan nan 1.4e-01]                   [nan nan 2.3e-02]                   [nan nan 1.2e-05]                  \n\t\t3          30942.4              [nan nan 1.4e-01]                   [nan nan 5.5e-02]                   [nan nan 6.2e-05]                  \n\t\t4          31009.5              [nan nan 1.4e-01]                   [nan nan 3.7e-02]                   [nan nan 8.4e-05]                  \n\n\tns[146] = [  128 32768  2048]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          31064.9              [nan nan 1.1e-01]                   [nan nan 6.1e-02]                   [nan nan 1.6e-04]                  \n\t\t1          31125.5              [nan nan 1.1e-01]                   [nan nan 2.7e-02]                   [nan nan 9.5e-05]                  \n\t\t2          31181.4              [nan nan 1.2e-01]                   [nan nan 7.1e-02]                   [nan nan 2.3e-04]                  \n\t\t3          31241.6              [nan nan 1.1e-01]                   [nan nan 4.5e-02]                   [nan nan 6.7e-05]                  \n\t\t4          31301.4              [nan nan 1.1e-01]                   [nan nan 2.7e-02]                   [nan nan 1.2e-05]                  \n\n\tns[147] = [  128 32768   512]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          31354.6              [nan nan 1.2e-01]                   [nan nan 9.3e-02]                   [nan nan 2.0e-04]                  \n\t\t1          31406.5              [nan nan 1.2e-01]                   [nan nan 1.3e-01]                   [nan nan 4.9e-04]                  \n\t\t2          31460.7              [nan nan 1.2e-01]                   [nan nan 1.4e-01]                   [nan nan 8.1e-04]                  \n\t\t3          31516.9              [nan nan 1.2e-01]                   [nan nan 9.5e-02]                   [nan nan 3.7e-04]                  \n\t\t4          31571.5              [nan nan 1.2e-01]                   [nan nan 1.2e-01]                   [nan nan 4.6e-04]                  \n\n\tns[148] = [  128 32768   128]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          31641.7              [nan nan 2.0e-01]                   [nan nan 6.3e-02]                   [nan nan 1.4e-03]                  \n\t\t1          31698.0              [nan nan 2.0e-01]                   [nan nan 1.9e-01]                   [nan nan 1.2e-03]                  \n\t\t2          31756.5              [nan nan 2.0e-01]                   [nan nan 1.6e-01]                   [nan nan 8.0e-05]                  \n\t\t3          31815.5              [nan nan 2.0e-01]                   [nan nan 1.5e-01]                   [nan nan 1.3e-03]                  \n\t\t4          31884.6              [nan nan 2.0e-01]                   [nan nan 9.0e-02]                   [nan nan 8.4e-04]                  \n\n\tns[149] = [  128 32768    32]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          31946.2              [nan nan 3.4e-01]                   [nan nan 1.6e-01]                   [nan nan 5.5e-04]                  \n\t\t1          32029.1              [nan nan 3.3e-01]                   [nan nan 1.4e-01]                   [nan nan 3.2e-03]                  \n\t\t2          32094.2              [nan nan 3.2e-01]                   [nan nan 1.8e-01]                   [nan nan 3.7e-03]                  \n\t\t3          32176.2              [nan nan 3.2e-01]                   [nan nan 1.4e-01]                   [nan nan 1.9e-03]                  \n\t\t4          32237.4              [nan nan 3.3e-01]                   [nan nan 1.6e-01]                   [nan nan 2.9e-03]                  \n\n\tns[150] = [  128  8192 32768]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          32295.5              [nan nan 1.3e-01]                   [nan nan 3.9e-02]                   [nan nan 3.8e-05]                  \n\t\t1          32365.8              [nan nan 1.4e-01]                   [nan nan 2.4e-02]                   [nan nan 1.5e-05]                  \n\t\t2          32433.6              [nan nan 1.4e-01]                   [nan nan 2.6e-02]                   [nan nan 1.8e-06]                  \n\t\t3          32502.1              [nan nan 1.4e-01]                   [nan nan 2.4e-02]                   [nan nan 2.6e-05]                  \n\t\t4          32570.9              [nan nan 1.4e-01]                   [nan nan 3.8e-02]                   [nan nan 2.9e-05]                  \n\n\tns[151] = [ 128 8192 8192]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          32586.5              [nan nan 5.5e-02]                   [nan nan 5.6e-02]                   [nan nan 9.2e-05]                  \n\t\t1          32598.6              [nan nan 5.5e-02]                   [nan nan 7.1e-02]                   [nan nan 9.2e-05]                  \n\t\t2          32614.4              [nan nan 5.6e-02]                   [nan nan 3.2e-02]                   [nan nan 6.2e-05]                  \n\t\t3          32626.5              [nan nan 5.6e-02]                   [nan nan 4.0e-02]                   [nan nan 1.6e-04]                  \n\t\t4          32638.9              [nan nan 5.6e-02]                   [nan nan 4.5e-02]                   [nan nan 1.4e-05]                  \n\n\tns[152] = [ 128 8192 2048]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          32646.6              [nan nan 4.1e-02]                   [nan nan 8.4e-02]                   [nan nan 1.4e-04]                  \n\t\t1          32654.3              [nan nan 4.3e-02]                   [nan nan 9.1e-02]                   [nan nan 6.9e-05]                  \n\t\t2          32662.0              [nan nan 4.2e-02]                   [nan nan 5.8e-02]                   [nan nan 7.4e-05]                  \n\t\t3          32672.4              [nan nan 4.2e-02]                   [nan nan 5.4e-02]                   [nan nan 4.6e-05]                  \n\t\t4          32680.0              [nan nan 4.1e-02]                   [nan nan 7.9e-02]                   [nan nan 7.0e-05]                  \n\n\tns[153] = [ 128 8192  512]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          32691.2              [nan nan 5.8e-02]                   [nan nan 5.3e-02]                   [nan nan 3.1e-04]                  \n\t\t1          32698.1              [nan nan 4.5e-02]                   [nan nan 1.1e-01]                   [nan nan 7.8e-04]                  \n\t\t2          32705.1              [nan nan 4.8e-02]                   [nan nan 6.2e-02]                   [nan nan 1.8e-04]                  \n\t\t3          32712.0              [nan nan 4.7e-02]                   [nan nan 7.7e-02]                   [nan nan 1.8e-06]                  \n\t\t4          32722.0              [nan nan 4.6e-02]                   [nan nan 5.1e-02]                   [nan nan 1.0e-04]                  \n\n\tns[154] = [ 128 8192  128]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          32728.8              [nan nan 4.7e-02]                   [nan nan 1.5e-01]                   [nan nan 1.5e-03]                  \n\t\t1          32738.7              [nan nan 4.7e-02]                   [nan nan 8.1e-02]                   [nan nan 1.2e-03]                  \n\t\t2          32745.6              [nan nan 4.9e-02]                   [nan nan 1.3e-01]                   [nan nan 9.7e-04]                  \n\t\t3          32752.5              [nan nan 4.8e-02]                   [nan nan 1.5e-01]                   [nan nan 6.3e-04]                  \n\t\t4          32760.0              [nan nan 4.7e-02]                   [nan nan 1.2e-01]                   [nan nan 4.1e-04]                  \n\n\tns[155] = [ 128 8192   32]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          32767.1              [nan nan 5.5e-02]                   [nan nan 1.6e-01]                   [nan nan 1.4e-03]                  \n\t\t1          32774.1              [nan nan 5.4e-02]                   [nan nan 1.5e-01]                   [nan nan 2.6e-03]                  \n\t\t2          32781.9              [nan nan 5.6e-02]                   [nan nan 1.5e-01]                   [nan nan 3.4e-03]                  \n\t\t3          32788.9              [nan nan 5.3e-02]                   [nan nan 2.0e-01]                   [nan nan 3.3e-03]                  \n\t\t4          32796.0              [nan nan 5.5e-02]                   [nan nan 1.6e-01]                   [nan nan 8.9e-04]                  \n\n\tns[156] = [  128  2048 32768]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          32859.8              [nan nan 1.1e-01]                   [nan nan 3.1e-02]                   [nan nan 2.2e-05]                  \n\t\t1          32921.8              [nan nan 1.1e-01]                   [nan nan 3.7e-02]                   [nan nan 2.9e-05]                  \n\t\t2          32983.9              [nan nan 1.2e-01]                   [nan nan 3.5e-02]                   [nan nan 1.2e-05]                  \n\t\t3          33046.0              [nan nan 1.1e-01]                   [nan nan 3.6e-02]                   [nan nan 2.1e-05]                  \n\t\t4          33108.0              [nan nan 1.1e-01]                   [nan nan 2.7e-02]                   [nan nan 1.8e-05]                  \n\n\tns[157] = [ 128 2048 8192]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          33115.9              [nan nan 4.2e-02]                   [nan nan 5.9e-02]                   [nan nan 2.6e-05]                  \n\t\t1          33127.0              [nan nan 4.5e-02]                   [nan nan 6.0e-02]                   [nan nan 4.3e-05]                  \n\t\t2          33137.8              [nan nan 4.5e-02]                   [nan nan 5.5e-02]                   [nan nan 2.8e-06]                  \n\t\t3          33148.7              [nan nan 4.6e-02]                   [nan nan 5.9e-02]                   [nan nan 6.6e-06]                  \n\t\t4          33156.7              [nan nan 4.7e-02]                   [nan nan 4.8e-02]                   [nan nan 2.1e-04]                  \n\n\tns[158] = [ 128 2048 2048]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          33160.9              [nan nan 1.9e-02]                   [nan nan 7.8e-02]                   [nan nan 2.0e-04]                  \n\t\t1          33164.0              [nan nan 1.8e-02]                   [nan nan 1.0e-01]                   [nan nan 2.0e-04]                  \n\t\t2          33166.9              [nan nan 1.9e-02]                   [nan nan 9.9e-02]                   [nan nan 1.9e-04]                  \n\t\t3          33171.1              [nan nan 1.8e-02]                   [nan nan 9.7e-02]                   [nan nan 2.7e-04]                  \n\t\t4          33174.3              [nan nan 1.8e-02]                   [nan nan 6.7e-02]                   [nan nan 1.5e-04]                  \n\n\tns[159] = [ 128 2048  512]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          33186.1              [nan 9.4e-02 1.6e-02]               [nan 1.3e-01 1.1e-01]               [nan 4.1e-04 7.5e-04]              \n\t\t1          33197.9              [nan 9.5e-02 1.5e-02]               [nan 1.2e-01 1.1e-01]               [nan 1.6e-03 1.3e-04]              \n\t\t2          33209.4              [nan 9.4e-02 1.3e-02]               [nan 1.2e-01 9.4e-02]               [nan 1.7e-03 3.0e-04]              \n\t\t3          33221.0              [nan 9.4e-02 1.3e-02]               [nan 1.3e-01 1.1e-01]               [nan 5.7e-03 3.7e-05]              \n\t\t4          33232.3              [nan 9.2e-02 1.4e-02]               [nan 1.2e-01 1.0e-01]               [nan 3.8e-03 4.7e-05]              \n\n\tns[160] = [ 128 2048  128]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          33242.2              [nan 8.0e-02 1.4e-02]               [nan 1.9e-01 1.5e-01]               [nan 6.1e-04 9.0e-04]              \n\t\t1          33251.3              [nan 7.2e-02 1.4e-02]               [nan 1.4e-01 1.5e-01]               [nan 3.5e-03 1.3e-03]              \n\t\t2          33261.1              [nan 7.9e-02 1.4e-02]               [nan 1.4e-01 1.0e-01]               [nan 2.2e-03 3.2e-04]              \n\t\t3          33270.3              [nan 7.1e-02 1.5e-02]               [nan 1.4e-01 1.3e-01]               [nan 2.1e-03 4.8e-04]              \n\t\t4          33279.9              [nan 7.4e-02 1.6e-02]               [nan 1.4e-01 1.1e-01]               [nan 3.5e-04 7.2e-04]              \n\n\tns[161] = [ 128 2048   32]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          33289.5              [nan 7.6e-02 1.5e-02]               [nan 4.7e-01 1.5e-01]               [nan 9.0e-03 1.0e-03]              \n\t\t1          33300.0              [nan 8.5e-02 1.6e-02]               [nan 1.6e-01 1.1e-01]               [nan 2.2e-03 2.5e-03]              \n\t\t2          33309.6              [nan 7.6e-02 1.6e-02]               [nan 1.5e-01 1.6e-01]               [nan 1.2e-03 1.3e-03]              \n\t\t3          33318.5              [nan 6.9e-02 1.5e-02]               [nan 2.0e-01 1.3e-01]               [nan 4.7e-05 1.2e-03]              \n\t\t4          33329.8              [nan 9.3e-02 1.5e-02]               [nan 3.4e-01 1.3e-01]               [nan 3.1e-02 1.9e-03]              \n\n\tns[162] = [  128   512 32768]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          33391.9              [nan nan 1.2e-01]                   [nan nan 3.0e-02]                   [nan nan 2.5e-05]                  \n\t\t1          33452.6              [nan nan 1.2e-01]                   [nan nan 3.4e-02]                   [nan nan 2.4e-05]                  \n\t\t2          33513.3              [nan nan 1.2e-01]                   [nan nan 3.8e-02]                   [nan nan 1.6e-05]                  \n\t\t3          33574.5              [nan nan 1.2e-01]                   [nan nan 4.1e-02]                   [nan nan 2.6e-05]                  \n\t\t4          33636.3              [nan nan 1.2e-01]                   [nan nan 3.6e-02]                   [nan nan 4.2e-05]                  \n\n\tns[163] = [ 128  512 8192]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          33645.7              [nan nan 4.0e-02]                   [nan nan 6.4e-02]                   [nan nan 2.0e-05]                  \n\t\t1          33655.3              [nan nan 4.2e-02]                   [nan nan 5.7e-02]                   [nan nan 2.5e-05]                  \n\t\t2          33665.0              [nan nan 4.3e-02]                   [nan nan 4.2e-02]                   [nan nan 1.0e-04]                  \n\t\t3          33671.8              [nan nan 4.1e-02]                   [nan nan 5.7e-02]                   [nan nan 8.8e-05]                  \n\t\t4          33681.2              [nan nan 4.0e-02]                   [nan nan 5.8e-02]                   [nan nan 1.2e-05]                  \n\n\tns[164] = [ 128  512 2048]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          33692.8              [nan 9.4e-02 1.3e-02]               [nan 1.2e-01 8.0e-02]               [nan 2.4e-04 3.3e-04]              \n\t\t1          33704.1              [nan 9.2e-02 1.3e-02]               [nan 1.3e-01 1.1e-01]               [nan 1.2e-03 1.2e-04]              \n\t\t2          33715.5              [nan 9.3e-02 1.3e-02]               [nan 1.2e-01 8.5e-02]               [nan 1.5e-04 1.5e-04]              \n\t\t3          33727.0              [nan 9.3e-02 1.3e-02]               [nan 1.3e-01 8.6e-02]               [nan 8.7e-04 1.2e-04]              \n\t\t4          33738.5              [nan 9.3e-02 1.3e-02]               [nan 1.2e-01 1.1e-01]               [nan 8.5e-04 2.2e-04]              \n\n\tns[165] = [128 512 512]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          33748.0              [1.3e-01 3.9e-02 7.7e-03]           [1.7e-01 1.8e-01 1.2e-01]           [1.1e-03 1.2e-03 5.7e-04]          \n\t\t1          33756.2              [1.3e-01 3.0e-02 7.9e-03]           [1.6e-01 1.7e-01 1.3e-01]           [3.6e-04 5.8e-04 5.8e-05]          \n\t\t2          33764.5              [1.3e-01 2.9e-02 7.9e-03]           [1.4e-01 1.6e-01 1.6e-01]           [4.6e-04 1.0e-03 4.8e-04]          \n\t\t3          33774.2              [1.3e-01 3.8e-02 7.2e-03]           [2.1e-01 2.1e-01 1.2e-01]           [1.2e-03 1.2e-03 1.7e-03]          \n\t\t4          33782.8              [1.3e-01 3.2e-02 7.9e-03]           [2.5e-01 1.9e-01 1.7e-01]           [1.7e-03 2.1e-03 2.5e-03]          \n\n\tns[166] = [128 512 128]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          33787.6              [7.1e-02 1.9e-02 8.2e-03]           [2.3e-01 2.3e-01 2.5e-01]           [2.9e-03 2.8e-03 3.7e-03]          \n\t\t1          33792.6              [7.2e-02 1.9e-02 7.9e-03]           [1.5e-01 1.4e-01 1.4e-01]           [4.2e-04 9.0e-04 3.4e-04]          \n\t\t2          33797.5              [6.7e-02 1.7e-02 7.9e-03]           [1.6e-01 1.5e-01 1.3e-01]           [3.6e-04 3.8e-04 1.4e-04]          \n\t\t3          33802.6              [7.3e-02 2.0e-02 8.1e-03]           [1.6e-01 1.5e-01 1.6e-01]           [1.7e-03 1.9e-03 2.3e-03]          \n\t\t4          33807.4              [6.8e-02 1.8e-02 8.1e-03]           [1.8e-01 1.4e-01 1.2e-01]           [7.4e-04 4.0e-04 3.7e-04]          \n\n\tns[167] = [128 512  32]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          33811.5              [5.6e-02 1.5e-02 8.1e-03]           [1.7e-01 1.4e-01 1.5e-01]           [2.3e-03 2.3e-03 1.2e-04]          \n\t\t1          33815.1              [5.6e-02 1.5e-02 8.5e-03]           [2.2e-01 1.5e-01 2.1e-01]           [1.2e-03 2.2e-03 8.2e-04]          \n\t\t2          33819.3              [5.5e-02 1.6e-02 8.4e-03]           [1.9e-01 1.5e-01 2.1e-01]           [2.8e-03 2.1e-03 5.1e-03]          \n\t\t3          33822.9              [5.8e-02 1.5e-02 7.8e-03]           [1.7e-01 1.4e-01 1.9e-01]           [1.5e-03 6.6e-04 1.3e-03]          \n\t\t4          33827.2              [5.7e-02 1.6e-02 8.3e-03]           [2.1e-01 1.3e-01 1.5e-01]           [5.7e-03 1.9e-03 3.5e-03]          \n\n\tns[168] = [  128   128 32768]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          33897.1              [nan nan 2.1e-01]                   [nan nan 3.2e-02]                   [nan nan 3.0e-05]                  \n\t\t1          33968.6              [nan nan 2.0e-01]                   [nan nan 3.5e-02]                   [nan nan 1.6e-05]                  \n\t\t2          34024.7              [nan nan 2.1e-01]                   [nan nan 3.7e-02]                   [nan nan 3.8e-05]                  \n\t\t3          34094.4              [nan nan 2.0e-01]                   [nan nan 2.9e-02]                   [nan nan 4.7e-06]                  \n\t\t4          34165.1              [nan nan 2.0e-01]                   [nan nan 9.4e-02]                   [nan nan 8.5e-05]                  \n\n\tns[169] = [ 128  128 8192]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          34172.1              [nan nan 4.7e-02]                   [nan nan 8.0e-02]                   [nan nan 9.4e-05]                  \n\t\t1          34179.1              [nan nan 4.8e-02]                   [nan nan 5.7e-02]                   [nan nan 1.6e-06]                  \n\t\t2          34186.0              [nan nan 4.4e-02]                   [nan nan 7.1e-02]                   [nan nan 6.3e-05]                  \n\t\t3          34193.2              [nan nan 4.9e-02]                   [nan nan 7.1e-02]                   [nan nan 1.7e-05]                  \n\t\t4          34203.5              [nan nan 5.1e-02]                   [nan nan 7.5e-02]                   [nan nan 5.0e-05]                  \n\n\tns[170] = [ 128  128 2048]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          34212.8              [nan 7.3e-02 1.5e-02]               [nan 1.2e-01 9.9e-02]               [nan 3.3e-04 4.4e-05]              \n\t\t1          34222.1              [nan 7.2e-02 1.4e-02]               [nan 1.2e-01 7.3e-02]               [nan 2.4e-04 6.1e-04]              \n\t\t2          34231.2              [nan 7.1e-02 1.5e-02]               [nan 1.3e-01 1.1e-01]               [nan 4.3e-04 5.3e-05]              \n\t\t3          34242.2              [nan 7.9e-02 1.6e-02]               [nan 1.2e-01 1.1e-01]               [nan 4.3e-05 3.0e-04]              \n\t\t4          34251.8              [nan 7.7e-02 1.6e-02]               [nan 1.3e-01 1.1e-01]               [nan 7.3e-04 2.6e-04]              \n\n\tns[171] = [128 128 512]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          34256.4              [6.9e-02 1.8e-02 8.2e-03]           [1.5e-01 1.4e-01 1.4e-01]           [3.5e-04 2.2e-04 1.9e-04]          \n\t\t1          34261.3              [7.1e-02 1.9e-02 8.0e-03]           [1.5e-01 1.4e-01 1.7e-01]           [4.7e-04 2.6e-04 5.8e-04]          \n\t\t2          34266.4              [7.2e-02 1.9e-02 7.7e-03]           [1.4e-01 1.3e-01 1.1e-01]           [2.2e-03 1.6e-04 6.1e-04]          \n\t\t3          34271.3              [7.0e-02 1.8e-02 7.5e-03]           [1.5e-01 1.5e-01 1.6e-01]           [6.8e-04 5.5e-04 9.9e-04]          \n\t\t4          34276.2              [7.2e-02 1.7e-02 7.6e-03]           [1.5e-01 1.5e-01 1.4e-01]           [3.0e-04 3.0e-04 5.6e-04]          \n\n\tns[172] = [128 128 128]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          34278.0              [2.2e-02 7.7e-03 6.6e-03]           [1.9e-01 1.5e-01 1.9e-01]           [1.4e-03 4.1e-04 7.2e-04]          \n\t\t1          34279.8              [2.3e-02 6.1e-03 5.9e-03]           [1.6e-01 1.5e-01 1.9e-01]           [1.7e-03 1.4e-03 2.2e-03]          \n\t\t2          34281.4              [2.1e-02 5.8e-03 5.9e-03]           [1.9e-01 1.8e-01 1.7e-01]           [6.1e-04 1.0e-03 1.4e-03]          \n\t\t3          34283.2              [2.1e-02 6.0e-03 6.0e-03]           [1.6e-01 1.5e-01 1.7e-01]           [6.5e-04 2.6e-04 1.2e-03]          \n\t\t4          34284.8              [2.1e-02 5.9e-03 6.0e-03]           [1.7e-01 1.5e-01 1.8e-01]           [1.0e-03 7.6e-04 1.3e-03]          \n\n\tns[173] = [128 128  32]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          34285.9              [1.3e-02 3.9e-03 5.8e-03]           [1.9e-01 1.6e-01 2.0e-01]           [1.6e-03 3.2e-03 5.8e-04]          \n\t\t1          34287.2              [1.3e-02 4.4e-03 5.9e-03]           [1.8e-01 1.7e-01 1.6e-01]           [5.6e-04 2.2e-03 5.0e-04]          \n\t\t2          34288.6              [1.3e-02 4.6e-03 5.8e-03]           [1.9e-01 1.8e-01 1.7e-01]           [4.0e-03 5.4e-03 3.0e-03]          \n\t\t3          34289.8              [1.4e-02 4.5e-03 5.8e-03]           [1.8e-01 2.0e-01 2.0e-01]           [1.3e-03 2.3e-03 2.5e-03]          \n\t\t4          34291.2              [1.4e-02 4.5e-03 5.7e-03]           [1.9e-01 1.7e-01 1.7e-01]           [1.2e-03 7.5e-04 6.1e-03]          \n\n\tns[174] = [  128    32 32768]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          34376.2              [nan nan 3.4e-01]                   [nan nan 2.8e-02]                   [nan nan 1.4e-05]                  \n\t\t1          34459.6              [nan nan 3.3e-01]                   [nan nan 3.5e-02]                   [nan nan 2.5e-05]                  \n\t\t2          34542.3              [nan nan 3.3e-01]                   [nan nan 3.5e-02]                   [nan nan 2.5e-05]                  \n\t\t3          34625.9              [nan nan 3.3e-01]                   [nan nan 4.2e-02]                   [nan nan 3.0e-05]                  \n\t\t4          34709.2              [nan nan 3.3e-01]                   [nan nan 2.9e-02]                   [nan nan 2.1e-05]                  \n\n\tns[175] = [ 128   32 8192]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          34717.3              [nan nan 5.4e-02]                   [nan nan 7.7e-02]                   [nan nan 2.7e-04]                  \n\t\t1          34724.3              [nan nan 5.1e-02]                   [nan nan 5.2e-02]                   [nan nan 1.9e-04]                  \n\t\t2          34731.8              [nan nan 5.2e-02]                   [nan nan 7.2e-02]                   [nan nan 1.0e-05]                  \n\t\t3          34742.2              [nan nan 5.2e-02]                   [nan nan 7.2e-02]                   [nan nan 3.1e-05]                  \n\t\t4          34749.1              [nan nan 5.2e-02]                   [nan nan 4.4e-02]                   [nan nan 1.4e-04]                  \n\n\tns[176] = [ 128   32 2048]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          34761.9              [nan 1.0e-01 1.5e-02]               [nan 1.3e-01 1.1e-01]               [nan 1.6e-04 8.4e-05]              \n\t\t1          34770.6              [nan 6.7e-02 1.7e-02]               [nan 1.3e-01 1.1e-01]               [nan 6.6e-04 2.1e-04]              \n\t\t2          34780.5              [nan 7.9e-02 1.7e-02]               [nan 1.2e-01 9.6e-02]               [nan 2.1e-04 6.5e-05]              \n\t\t3          34789.2              [nan 6.8e-02 1.6e-02]               [nan 1.1e-01 9.9e-02]               [nan 3.1e-05 1.9e-04]              \n\t\t4          34798.9              [nan 7.2e-02 1.6e-02]               [nan 2.1e-01 1.1e-01]               [nan 3.5e-04 1.3e-03]              \n\n\tns[177] = [128  32 512]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          34802.7              [5.3e-02 1.3e-02 7.2e-03]           [1.6e-01 1.5e-01 1.6e-01]           [7.6e-04 9.7e-04 1.2e-03]          \n\t\t1          34806.7              [5.4e-02 1.5e-02 7.6e-03]           [1.5e-01 1.5e-01 1.4e-01]           [4.3e-04 2.2e-04 6.1e-04]          \n\t\t2          34810.7              [5.4e-02 1.5e-02 7.6e-03]           [2.2e-01 2.2e-01 1.3e-01]           [1.4e-03 3.6e-04 1.1e-03]          \n\t\t3          34814.1              [5.0e-02 1.4e-02 7.6e-03]           [1.7e-01 1.3e-01 1.7e-01]           [9.3e-04 4.3e-04 1.8e-04]          \n\t\t4          34817.8              [5.4e-02 1.3e-02 7.9e-03]           [1.7e-01 1.5e-01 1.6e-01]           [2.5e-04 2.5e-04 5.4e-05]          \n\n\tns[178] = [128  32 128]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          34819.1              [1.5e-02 5.0e-03 5.8e-03]           [1.8e-01 1.5e-01 1.8e-01]           [4.4e-04 5.1e-05 1.2e-03]          \n\t\t1          34820.7              [1.5e-02 4.9e-03 5.6e-03]           [2.0e-01 2.2e-01 1.6e-01]           [1.3e-04 9.3e-06 1.4e-03]          \n\t\t2          34822.0              [1.6e-02 5.0e-03 5.9e-03]           [1.9e-01 1.8e-01 2.0e-01]           [8.7e-04 1.5e-03 1.4e-04]          \n\t\t3          34823.3              [1.4e-02 4.7e-03 5.7e-03]           [1.9e-01 1.7e-01 2.2e-01]           [2.3e-04 3.7e-05 9.1e-04]          \n\t\t4          34824.6              [1.4e-02 5.1e-03 5.7e-03]           [2.0e-01 2.0e-01 1.7e-01]           [1.1e-03 9.7e-04 9.6e-04]          \n\n\tns[179] = [128  32  32]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          34825.5              [8.0e-03 3.4e-03 5.4e-03]           [2.4e-01 2.2e-01 1.8e-01]           [3.5e-03 4.6e-03 2.6e-03]          \n\t\t1          34826.4              [8.2e-03 3.5e-03 5.4e-03]           [2.3e-01 1.8e-01 2.4e-01]           [5.8e-03 5.0e-03 6.0e-03]          \n\t\t2          34827.3              [7.9e-03 3.2e-03 5.4e-03]           [2.1e-01 1.7e-01 1.7e-01]           [3.2e-03 1.0e-03 5.6e-03]          \n\t\t3          34828.2              [8.0e-03 3.1e-03 5.3e-03]           [2.6e-01 2.1e-01 2.0e-01]           [4.0e-03 3.7e-03 4.6e-03]          \n\t\t4          34829.2              [7.9e-03 3.3e-03 5.3e-03]           [2.3e-01 2.5e-01 1.8e-01]           [5.7e-03 8.1e-03 4.6e-03]          \n\n\tns[180] = [   32 32768 32768]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          35009.6              [nan nan 7.8e-01]                   [nan nan 2.2e-02]                   [nan nan 2.5e-05]                  \n\t\t1          35192.3              [nan nan 8.0e-01]                   [nan nan 2.3e-02]                   [nan nan 3.0e-07]                  \n\t\t2          35372.2              [nan nan 8.0e-01]                   [nan nan 2.5e-02]                   [nan nan 1.3e-05]                  \n\t\t3          35555.0              [nan nan 8.1e-01]                   [nan nan 2.4e-02]                   [nan nan 2.1e-05]                  \n\t\t4          35735.3              [nan nan 8.1e-01]                   [nan nan 2.4e-02]                   [nan nan 3.0e-05]                  \n\n\tns[181] = [   32 32768  8192]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          35821.4              [nan nan 3.2e-01]                   [nan nan 1.2e-01]                   [nan nan 1.1e-04]                  \n\t\t1          35906.6              [nan nan 3.0e-01]                   [nan nan 2.7e-02]                   [nan nan 1.7e-05]                  \n\t\t2          35973.9              [nan nan 3.0e-01]                   [nan nan 4.8e-02]                   [nan nan 2.4e-05]                  \n\t\t3          36059.4              [nan nan 3.1e-01]                   [nan nan 2.9e-02]                   [nan nan 4.6e-07]                  \n\t\t4          36144.4              [nan nan 3.1e-01]                   [nan nan 3.2e-02]                   [nan nan 1.5e-05]                  \n\n\tns[182] = [   32 32768  2048]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          36204.6              [nan nan 2.5e-01]                   [nan nan 8.0e-02]                   [nan nan 8.9e-05]                  \n\t\t1          36264.0              [nan nan 2.5e-01]                   [nan nan 6.8e-02]                   [nan nan 1.8e-04]                  \n\t\t2          36339.3              [nan nan 2.6e-01]                   [nan nan 2.9e-02]                   [nan nan 1.4e-04]                  \n\t\t3          36415.4              [nan nan 2.6e-01]                   [nan nan 2.8e-02]                   [nan nan 4.3e-05]                  \n\t\t4          36472.7              [nan nan 2.5e-01]                   [nan nan 4.1e-02]                   [nan nan 1.4e-04]                  \n\n\tns[183] = [   32 32768   512]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          36536.2              [nan nan 2.6e-01]                   [nan nan 8.2e-02]                   [nan nan 5.4e-04]                  \n\t\t1          36594.2              [nan nan 2.6e-01]                   [nan nan 1.3e-01]                   [nan nan 1.0e-04]                  \n\t\t2          36671.0              [nan nan 2.6e-01]                   [nan nan 2.7e-02]                   [nan nan 8.7e-05]                  \n\t\t3          36731.4              [nan nan 2.6e-01]                   [nan nan 1.1e-01]                   [nan nan 1.5e-04]                  \n\t\t4          36789.9              [nan nan 2.6e-01]                   [nan nan 1.2e-01]                   [nan nan 5.3e-04]                  \n\n\tns[184] = [   32 32768   128]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          36871.8              [nan nan 3.3e-01]                   [nan nan 8.9e-02]                   [nan nan 2.1e-03]                  \n\t\t1          36932.8              [nan nan 3.3e-01]                   [nan nan 1.6e-01]                   [nan nan 1.5e-03]                  \n\t\t2          37016.2              [nan nan 3.3e-01]                   [nan nan 9.1e-02]                   [nan nan 9.7e-04]                  \n\t\t3          37076.0              [nan nan 3.3e-01]                   [nan nan 1.3e-01]                   [nan nan 2.3e-05]                  \n\t\t4          37135.8              [nan nan 3.3e-01]                   [nan nan 1.4e-01]                   [nan nan 2.3e-03]                  \n\n\tns[185] = [   32 32768    32]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          37247.8              [nan nan 6.2e-01]                   [nan nan 1.1e-01]                   [nan nan 2.8e-03]                  \n\t\t1          37330.9              [nan nan 6.2e-01]                   [nan nan 2.5e-01]                   [nan nan 1.2e-02]                  \n\t\t2          37443.4              [nan nan 6.2e-01]                   [nan nan 1.7e-01]                   [nan nan 6.1e-04]                  \n\t\t3          37554.0              [nan nan 6.2e-01]                   [nan nan 1.5e-01]                   [nan nan 4.1e-03]                  \n\t\t4          37665.8              [nan nan 6.2e-01]                   [nan nan 1.5e-01]                   [nan nan 4.9e-03]                  \n\n\tns[186] = [   32  8192 32768]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          37751.2              [nan nan 3.0e-01]                   [nan nan 3.7e-02]                   [nan nan 1.9e-05]                  \n\t\t1          37836.0              [nan nan 3.0e-01]                   [nan nan 2.8e-02]                   [nan nan 2.2e-05]                  \n\t\t2          37922.3              [nan nan 3.1e-01]                   [nan nan 3.0e-02]                   [nan nan 5.3e-07]                  \n\t\t3          38010.7              [nan nan 3.1e-01]                   [nan nan 3.9e-02]                   [nan nan 2.4e-05]                  \n\t\t4          38095.4              [nan nan 3.0e-01]                   [nan nan 2.9e-02]                   [nan nan 1.7e-05]                  \n\n\tns[187] = [  32 8192 8192]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          38108.6              [nan nan 8.0e-02]                   [nan nan 4.0e-02]                   [nan nan 4.3e-05]                  \n\t\t1          38126.8              [nan nan 8.0e-02]                   [nan nan 4.5e-02]                   [nan nan 1.2e-05]                  \n\t\t2          38145.1              [nan nan 8.2e-02]                   [nan nan 5.0e-02]                   [nan nan 8.0e-05]                  \n\t\t3          38163.3              [nan nan 8.1e-02]                   [nan nan 7.0e-02]                   [nan nan 3.9e-05]                  \n\t\t4          38181.6              [nan nan 8.1e-02]                   [nan nan 4.7e-02]                   [nan nan 8.4e-05]                  \n\n\tns[188] = [  32 8192 2048]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          38189.9              [nan nan 5.4e-02]                   [nan nan 4.8e-02]                   [nan nan 1.3e-04]                  \n\t\t1          38198.0              [nan nan 5.5e-02]                   [nan nan 7.0e-02]                   [nan nan 1.1e-04]                  \n\t\t2          38209.5              [nan nan 5.3e-02]                   [nan nan 7.0e-02]                   [nan nan 1.4e-04]                  \n\t\t3          38217.8              [nan nan 5.4e-02]                   [nan nan 6.3e-02]                   [nan nan 1.3e-04]                  \n\t\t4          38225.9              [nan nan 5.5e-02]                   [nan nan 6.7e-02]                   [nan nan 5.5e-05]                  \n\n\tns[189] = [  32 8192  512]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          38233.0              [nan nan 5.0e-02]                   [nan nan 8.7e-02]                   [nan nan 6.8e-05]                  \n\t\t1          38240.2              [nan nan 5.2e-02]                   [nan nan 8.1e-02]                   [nan nan 1.9e-04]                  \n\t\t2          38250.6              [nan nan 5.1e-02]                   [nan nan 5.6e-02]                   [nan nan 4.6e-05]                  \n\t\t3          38257.7              [nan nan 5.1e-02]                   [nan nan 9.8e-02]                   [nan nan 6.9e-04]                  \n\t\t4          38265.5              [nan nan 5.1e-02]                   [nan nan 8.2e-02]                   [nan nan 2.1e-04]                  \n\n\tns[190] = [  32 8192  128]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          38276.2              [nan nan 5.6e-02]                   [nan nan 8.0e-02]                   [nan nan 6.0e-04]                  \n\t\t1          38283.9              [nan nan 5.2e-02]                   [nan nan 1.2e-01]                   [nan nan 4.2e-05]                  \n\t\t2          38291.0              [nan nan 5.3e-02]                   [nan nan 1.6e-01]                   [nan nan 2.0e-04]                  \n\t\t3          38298.4              [nan nan 5.3e-02]                   [nan nan 1.2e-01]                   [nan nan 7.8e-04]                  \n\t\t4          38305.4              [nan nan 5.2e-02]                   [nan nan 1.3e-01]                   [nan nan 3.0e-03]                  \n\n\tns[191] = [  32 8192   32]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          38317.2              [nan nan 6.8e-02]                   [nan nan 1.3e-01]                   [nan nan 1.4e-03]                  \n\t\t1          38325.0              [nan nan 6.9e-02]                   [nan nan 2.1e-01]                   [nan nan 5.1e-03]                  \n\t\t2          38333.1              [nan nan 6.7e-02]                   [nan nan 2.2e-01]                   [nan nan 2.0e-03]                  \n\t\t3          38340.6              [nan nan 6.7e-02]                   [nan nan 1.7e-01]                   [nan nan 2.0e-03]                  \n\t\t4          38348.1              [nan nan 6.8e-02]                   [nan nan 2.2e-01]                   [nan nan 2.6e-03]                  \n\n\tns[192] = [   32  2048 32768]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          38427.0              [nan nan 2.7e-01]                   [nan nan 2.9e-02]                   [nan nan 4.2e-06]                  \n\t\t1          38502.4              [nan nan 2.5e-01]                   [nan nan 3.0e-02]                   [nan nan 3.4e-05]                  \n\t\t2          38578.5              [nan nan 2.5e-01]                   [nan nan 3.8e-02]                   [nan nan 2.7e-05]                  \n\t\t3          38652.8              [nan nan 2.5e-01]                   [nan nan 2.6e-02]                   [nan nan 1.6e-05]                  \n\t\t4          38729.3              [nan nan 2.5e-01]                   [nan nan 3.6e-02]                   [nan nan 2.8e-05]                  \n\n\tns[193] = [  32 2048 8192]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          38738.6              [nan nan 5.1e-02]                   [nan nan 6.8e-02]                   [nan nan 2.4e-05]                  \n\t\t1          38747.2              [nan nan 5.3e-02]                   [nan nan 6.2e-02]                   [nan nan 1.9e-05]                  \n\t\t2          38755.3              [nan nan 5.3e-02]                   [nan nan 6.1e-02]                   [nan nan 3.9e-05]                  \n\t\t3          38767.0              [nan nan 5.4e-02]                   [nan nan 6.8e-02]                   [nan nan 5.5e-05]                  \n\t\t4          38775.2              [nan nan 5.3e-02]                   [nan nan 7.6e-02]                   [nan nan 1.5e-04]                  \n\n\tns[194] = [  32 2048 2048]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          38778.2              [nan nan 2.1e-02]                   [nan nan 1.0e-01]                   [nan nan 1.9e-04]                  \n\t\t1          38782.7              [nan nan 2.2e-02]                   [nan nan 1.0e-01]                   [nan nan 3.6e-05]                  \n\t\t2          38787.0              [nan nan 2.1e-02]                   [nan nan 8.6e-02]                   [nan nan 1.8e-04]                  \n\t\t3          38790.1              [nan nan 2.1e-02]                   [nan nan 9.8e-02]                   [nan nan 2.9e-04]                  \n\t\t4          38794.6              [nan nan 2.1e-02]                   [nan nan 6.7e-02]                   [nan nan 9.0e-05]                  \n\n\tns[195] = [  32 2048  512]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          38810.3              [nan 1.3e-01 1.5e-02]               [nan 9.9e-02 1.1e-01]               [nan 6.1e-04 1.9e-04]              \n\t\t1          38822.1              [nan 9.5e-02 1.5e-02]               [nan 1.2e-01 9.6e-02]               [nan 3.0e-04 3.0e-04]              \n\t\t2          38833.7              [nan 9.3e-02 1.4e-02]               [nan 1.3e-01 1.1e-01]               [nan 1.0e-03 8.5e-04]              \n\t\t3          38844.3              [nan 8.5e-02 1.4e-02]               [nan 1.3e-01 1.3e-01]               [nan 6.1e-04 4.8e-04]              \n\t\t4          38855.6              [nan 9.1e-02 1.6e-02]               [nan 1.1e-01 9.9e-02]               [nan 3.6e-04 1.6e-04]              \n\n\tns[196] = [  32 2048  128]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          38864.6              [nan 7.0e-02 1.5e-02]               [nan 1.6e-01 1.4e-01]               [nan 1.0e-03 2.0e-03]              \n\t\t1          38873.7              [nan 7.2e-02 1.6e-02]               [nan 1.8e-01 1.7e-01]               [nan 2.4e-03 3.8e-03]              \n\t\t2          38882.5              [nan 6.9e-02 1.5e-02]               [nan 1.8e-01 1.5e-01]               [nan 1.0e-03 2.8e-04]              \n\t\t3          38891.0              [nan 6.7e-02 1.5e-02]               [nan 1.6e-01 1.6e-01]               [nan 1.6e-03 2.2e-03]              \n\t\t4          38901.0              [nan 8.1e-02 1.5e-02]               [nan 1.6e-01 1.3e-01]               [nan 1.9e-03 1.2e-03]              \n\n\tns[197] = [  32 2048   32]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          38909.3              [nan 6.3e-02 1.7e-02]               [nan 2.1e-01 2.1e-01]               [nan 4.5e-03 5.0e-03]              \n\t\t1          38917.7              [nan 6.6e-02 1.6e-02]               [nan 2.4e-01 2.1e-01]               [nan 1.4e-02 2.1e-04]              \n\t\t2          38926.1              [nan 6.4e-02 1.6e-02]               [nan 2.3e-01 1.7e-01]               [nan 8.5e-04 5.2e-04]              \n\t\t3          38937.1              [nan 9.0e-02 1.6e-02]               [nan 2.3e-01 1.7e-01]               [nan 1.0e-02 9.6e-04]              \n\t\t4          38946.6              [nan 7.6e-02 1.6e-02]               [nan 5.6e-01 2.5e-01]               [nan 3.1e-02 6.4e-03]              \n\n\tns[198] = [   32   512 32768]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          39025.0              [nan nan 2.7e-01]                   [nan nan 4.2e-02]                   [nan nan 3.1e-05]                  \n\t\t1          39098.8              [nan nan 2.6e-01]                   [nan nan 2.7e-02]                   [nan nan 2.4e-05]                  \n\t\t2          39175.8              [nan nan 2.6e-01]                   [nan nan 3.9e-02]                   [nan nan 2.4e-05]                  \n\t\t3          39250.2              [nan nan 2.5e-01]                   [nan nan 4.5e-02]                   [nan nan 8.6e-05]                  \n\t\t4          39325.9              [nan nan 2.6e-01]                   [nan nan 2.7e-02]                   [nan nan 2.0e-05]                  \n\n\tns[199] = [  32  512 8192]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          39335.9              [nan nan 4.6e-02]                   [nan nan 7.1e-02]                   [nan nan 3.8e-05]                  \n\t\t1          39346.0              [nan nan 4.7e-02]                   [nan nan 5.7e-02]                   [nan nan 1.5e-05]                  \n\t\t2          39353.1              [nan nan 4.6e-02]                   [nan nan 6.3e-02]                   [nan nan 3.9e-05]                  \n\t\t3          39360.8              [nan nan 4.8e-02]                   [nan nan 6.3e-02]                   [nan nan 2.2e-05]                  \n\t\t4          39367.9              [nan nan 4.8e-02]                   [nan nan 7.2e-02]                   [nan nan 3.8e-05]                  \n\n\tns[200] = [  32  512 2048]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          39381.5              [nan 1.0e-01 1.4e-02]               [nan 1.1e-01 9.4e-02]               [nan 1.7e-04 5.3e-06]              \n\t\t1          39397.4              [nan 1.3e-01 1.4e-02]               [nan 1.0e-01 8.9e-02]               [nan 6.2e-04 2.3e-04]              \n\t\t2          39410.5              [nan 1.1e-01 1.5e-02]               [nan 1.1e-01 9.9e-02]               [nan 2.6e-04 4.1e-05]              \n\t\t3          39424.3              [nan 1.2e-01 1.5e-02]               [nan 1.2e-01 7.2e-02]               [nan 3.8e-04 6.0e-04]              \n\t\t4          39435.1              [nan 8.3e-02 1.4e-02]               [nan 1.2e-01 1.0e-01]               [nan 1.9e-04 1.1e-04]              \n\n\tns[201] = [ 32 512 512]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          39442.1              [1.1e-01 2.4e-02 8.3e-03]           [1.4e-01 1.4e-01 1.5e-01]           [1.2e-04 2.1e-04 8.0e-04]          \n\t\t1          39450.9              [1.1e-01 3.0e-02 8.7e-03]           [1.7e-01 1.6e-01 1.1e-01]           [4.5e-06 4.5e-04 5.1e-04]          \n\t\t2          39458.3              [1.1e-01 2.7e-02 8.9e-03]           [1.6e-01 1.6e-01 1.4e-01]           [2.3e-04 2.0e-04 4.5e-04]          \n\t\t3          39465.6              [1.1e-01 2.8e-02 8.3e-03]           [1.3e-01 1.4e-01 1.8e-01]           [4.4e-06 4.9e-05 1.5e-04]          \n\t\t4          39471.9              [1.1e-01 2.6e-02 8.4e-03]           [1.6e-01 1.3e-01 1.5e-01]           [1.3e-04 8.7e-06 7.9e-05]          \n\n\tns[202] = [ 32 512 128]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          39475.6              [5.1e-02 1.3e-02 7.3e-03]           [2.3e-01 1.4e-01 1.4e-01]           [1.3e-04 7.5e-05 4.8e-04]          \n\t\t1          39479.5              [5.1e-02 1.4e-02 7.3e-03]           [1.5e-01 1.4e-01 1.4e-01]           [5.6e-04 1.6e-03 7.1e-04]          \n\t\t2          39482.9              [5.1e-02 1.3e-02 8.1e-03]           [1.8e-01 1.4e-01 1.7e-01]           [1.2e-03 8.3e-04 1.6e-03]          \n\t\t3          39486.6              [5.4e-02 1.3e-02 7.4e-03]           [1.7e-01 1.5e-01 1.9e-01]           [1.4e-03 1.5e-03 2.3e-03]          \n\t\t4          39490.6              [5.5e-02 1.4e-02 7.4e-03]           [1.5e-01 1.5e-01 1.6e-01]           [1.3e-04 5.6e-04 9.8e-05]          \n\n\tns[203] = [ 32 512  32]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          39493.9              [4.1e-02 1.2e-02 7.5e-03]           [1.7e-01 1.4e-01 1.4e-01]           [2.1e-03 7.3e-04 2.0e-03]          \n\t\t1          39497.0              [4.2e-02 9.9e-03 7.1e-03]           [2.1e-01 2.0e-01 1.6e-01]           [1.1e-03 2.3e-03 2.5e-03]          \n\t\t2          39500.1              [4.1e-02 1.2e-02 7.9e-03]           [1.8e-01 1.5e-01 2.1e-01]           [6.7e-04 3.2e-04 6.3e-04]          \n\t\t3          39502.9              [3.8e-02 1.2e-02 7.9e-03]           [1.9e-01 1.5e-01 2.3e-01]           [3.8e-03 1.6e-03 7.6e-03]          \n\t\t4          39505.9              [3.8e-02 1.2e-02 7.4e-03]           [1.7e-01 1.4e-01 2.6e-01]           [1.8e-04 1.5e-03 4.4e-03]          \n\n\tns[204] = [   32   128 32768]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          39591.0              [nan nan 3.5e-01]                   [nan nan 4.1e-02]                   [nan nan 3.7e-05]                  \n\t\t1          39674.1              [nan nan 3.3e-01]                   [nan nan 2.8e-02]                   [nan nan 1.3e-05]                  \n\t\t2          39757.2              [nan nan 3.3e-01]                   [nan nan 3.2e-02]                   [nan nan 1.5e-05]                  \n\t\t3          39844.2              [nan nan 3.3e-01]                   [nan nan 4.2e-02]                   [nan nan 2.2e-05]                  \n\t\t4          39932.7              [nan nan 3.5e-01]                   [nan nan 3.3e-02]                   [nan nan 1.5e-05]                  \n\n\tns[205] = [  32  128 8192]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          39939.9              [nan nan 5.4e-02]                   [nan nan 5.2e-02]                   [nan nan 1.1e-04]                  \n\t\t1          39950.8              [nan nan 5.6e-02]                   [nan nan 7.7e-02]                   [nan nan 1.0e-04]                  \n\t\t2          39958.0              [nan nan 5.3e-02]                   [nan nan 4.6e-02]                   [nan nan 1.8e-04]                  \n\t\t3          39965.8              [nan nan 5.2e-02]                   [nan nan 6.4e-02]                   [nan nan 4.8e-05]                  \n\t\t4          39976.3              [nan nan 5.4e-02]                   [nan nan 7.0e-02]                   [nan nan 6.4e-05]                  \n\n\tns[206] = [  32  128 2048]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          39985.5              [nan 7.1e-02 1.6e-02]               [nan 1.2e-01 1.1e-01]               [nan 9.2e-04 1.2e-04]              \n\t\t1          39994.7              [nan 7.1e-02 1.6e-02]               [nan 1.3e-01 7.9e-02]               [nan 4.8e-04 4.1e-04]              \n\t\t2          40005.6              [nan 8.9e-02 1.7e-02]               [nan 1.2e-01 8.6e-02]               [nan 5.1e-04 2.9e-04]              \n\t\t3          40015.0              [nan 7.1e-02 1.6e-02]               [nan 1.3e-01 9.6e-02]               [nan 2.5e-04 6.8e-05]              \n\t\t4          40025.1              [nan 7.0e-02 1.7e-02]               [nan 1.3e-01 1.0e-01]               [nan 4.0e-04 3.7e-05]              \n\n\tns[207] = [ 32 128 512]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          40029.0              [5.5e-02 1.3e-02 8.0e-03]           [1.8e-01 1.7e-01 1.3e-01]           [5.1e-05 4.8e-04 9.7e-04]          \n\t\t1          40032.6              [5.4e-02 1.2e-02 7.7e-03]           [1.5e-01 1.4e-01 1.7e-01]           [4.8e-04 2.6e-04 9.5e-04]          \n\t\t2          40036.4              [5.3e-02 1.3e-02 7.6e-03]           [2.1e-01 1.6e-01 1.5e-01]           [5.7e-04 2.7e-04 8.3e-04]          \n\t\t3          40039.9              [5.4e-02 1.4e-02 7.7e-03]           [1.6e-01 1.4e-01 1.8e-01]           [7.2e-04 1.0e-04 1.9e-04]          \n\t\t4          40043.7              [5.4e-02 1.4e-02 7.6e-03]           [1.8e-01 1.7e-01 1.8e-01]           [1.7e-04 3.8e-04 6.6e-04]          \n\n\tns[208] = [ 32 128 128]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          40045.5              [1.5e-02 4.9e-03 5.7e-03]           [1.8e-01 1.7e-01 1.7e-01]           [8.0e-05 7.0e-04 3.4e-03]          \n\t\t1          40046.8              [1.4e-02 5.2e-03 5.8e-03]           [2.1e-01 2.1e-01 2.0e-01]           [2.7e-04 1.7e-04 2.1e-03]          \n\t\t2          40048.3              [1.6e-02 5.3e-03 6.3e-03]           [1.9e-01 1.8e-01 1.8e-01]           [1.3e-03 2.2e-03 9.0e-04]          \n\t\t3          40049.7              [1.5e-02 5.2e-03 5.9e-03]           [1.9e-01 1.9e-01 1.6e-01]           [8.3e-04 9.2e-04 3.4e-03]          \n\t\t4          40050.9              [1.4e-02 4.6e-03 5.8e-03]           [1.9e-01 1.5e-01 1.6e-01]           [1.1e-03 1.5e-04 2.6e-04]          \n\n\tns[209] = [ 32 128  32]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          40051.9              [8.9e-03 3.5e-03 5.5e-03]           [2.2e-01 1.9e-01 2.2e-01]           [3.2e-03 3.7e-03 2.5e-03]          \n\t\t1          40052.8              [8.8e-03 3.4e-03 5.9e-03]           [2.0e-01 1.9e-01 2.1e-01]           [8.6e-04 1.4e-03 3.4e-03]          \n\t\t2          40053.8              [8.4e-03 3.4e-03 6.1e-03]           [2.0e-01 1.9e-01 1.9e-01]           [4.1e-03 1.4e-03 4.7e-05]          \n\t\t3          40054.8              [9.6e-03 3.8e-03 5.8e-03]           [1.9e-01 1.7e-01 1.8e-01]           [2.4e-03 6.3e-04 2.5e-03]          \n\t\t4          40056.3              [1.0e-02 3.7e-03 5.8e-03]           [1.7e-01 1.6e-01 2.3e-01]           [3.0e-01 8.8e-04 7.0e-03]          \n\n\tns[210] = [   32    32 32768]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          40171.0              [nan nan 6.4e-01]                   [nan nan 2.7e-02]                   [nan nan 1.3e-05]                  \n\t\t1          40286.3              [nan nan 6.3e-01]                   [nan nan 3.1e-02]                   [nan nan 1.3e-05]                  \n\t\t2          40398.5              [nan nan 6.3e-01]                   [nan nan 3.8e-02]                   [nan nan 2.7e-05]                  \n\t\t3          40514.2              [nan nan 6.3e-01]                   [nan nan 4.0e-02]                   [nan nan 3.6e-05]                  \n\t\t4          40626.5              [nan nan 6.2e-01]                   [nan nan 3.1e-02]                   [nan nan 7.2e-06]                  \n\n\tns[211] = [  32   32 8192]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          40634.2              [nan nan 6.7e-02]                   [nan nan 6.4e-02]                   [nan nan 3.5e-05]                  \n\t\t1          40641.9              [nan nan 6.8e-02]                   [nan nan 6.4e-02]                   [nan nan 1.9e-04]                  \n\t\t2          40649.4              [nan nan 6.6e-02]                   [nan nan 5.0e-02]                   [nan nan 1.1e-04]                  \n\t\t3          40657.2              [nan nan 6.7e-02]                   [nan nan 7.7e-02]                   [nan nan 6.0e-05]                  \n\t\t4          40669.3              [nan nan 6.8e-02]                   [nan nan 7.4e-02]                   [nan nan 8.0e-05]                  \n\n\tns[212] = [  32   32 2048]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          40678.6              [nan 7.1e-02 1.7e-02]               [nan 1.1e-01 1.1e-01]               [nan 4.1e-05 1.4e-04]              \n\t\t1          40687.3              [nan 6.7e-02 1.7e-02]               [nan 1.2e-01 1.0e-01]               [nan 3.1e-04 7.0e-05]              \n\t\t2          40695.5              [nan 6.2e-02 1.7e-02]               [nan 1.3e-01 9.0e-02]               [nan 2.1e-04 1.3e-04]              \n\t\t3          40704.4              [nan 6.9e-02 1.7e-02]               [nan 1.2e-01 9.5e-02]               [nan 1.8e-04 1.4e-04]              \n\t\t4          40712.8              [nan 6.3e-02 1.6e-02]               [nan 2.1e-01 8.6e-02]               [nan 3.8e-04 7.6e-04]              \n\n\tns[213] = [ 32  32 512]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          40715.6              [4.0e-02 1.1e-02 7.5e-03]           [1.5e-01 1.4e-01 1.5e-01]           [7.1e-04 3.5e-04 1.0e-03]          \n\t\t1          40718.4              [3.9e-02 1.1e-02 7.3e-03]           [1.5e-01 1.6e-01 1.6e-01]           [1.1e-03 2.8e-04 2.9e-04]          \n\t\t2          40721.0              [3.6e-02 1.1e-02 7.1e-03]           [1.8e-01 1.5e-01 1.8e-01]           [3.6e-05 3.1e-04 5.1e-04]          \n\t\t3          40723.7              [3.9e-02 1.1e-02 7.7e-03]           [3.6e-01 2.3e-01 1.5e-01]           [5.4e-03 1.1e-03 2.2e-03]          \n\t\t4          40726.6              [3.8e-02 1.2e-02 7.4e-03]           [1.7e-01 1.5e-01 1.8e-01]           [3.4e-04 1.1e-04 1.7e-04]          \n\n\tns[214] = [ 32  32 128]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          40727.5              [7.9e-03 3.4e-03 5.9e-03]           [1.8e-01 1.7e-01 1.9e-01]           [9.5e-05 3.5e-04 1.2e-03]          \n\t\t1          40728.5              [8.2e-03 3.1e-03 5.9e-03]           [1.9e-01 1.8e-01 1.7e-01]           [7.1e-05 8.2e-04 1.6e-03]          \n\t\t2          40729.4              [8.0e-03 3.3e-03 6.1e-03]           [2.0e-01 1.8e-01 1.9e-01]           [1.2e-03 9.0e-04 9.4e-04]          \n\t\t3          40730.8              [8.8e-03 3.7e-03 5.7e-03]           [2.2e-01 1.9e-01 1.9e-01]           [7.9e-04 6.4e-04 2.8e-03]          \n\t\t4          40731.8              [8.5e-03 3.5e-03 5.5e-03]           [2.3e-01 1.9e-01 1.6e-01]           [1.5e-03 1.7e-03 4.2e-03]          \n\n\tns[215] = [32 32 32]\n\t\ttrial      clock                times                               l2rerrors                           bcerrors                           \n\t\t0          40732.4              [3.4e-03 1.6e-03 5.0e-03]           [2.5e-01 2.5e-01 2.9e-01]           [1.7e-01 4.1e-03 2.4e-03]          \n\t\t1          40733.2              [3.3e-03 1.6e-03 5.2e-03]           [1.9e-01 1.9e-01 2.7e-01]           [3.8e-01 3.6e-04 3.1e-03]          \n\t\t2          40733.9              [3.4e-03 1.5e-03 5.2e-03]           [1.8e-01 1.8e-01 2.5e-01]           [5.7e-01 1.8e-03 3.3e-04]          \n\t\t3          40734.7              [3.3e-03 1.6e-03 5.1e-03]           [1.9e-01 2.0e-01 2.4e-01]           [3.0e-01 4.9e-04 7.8e-03]          \n\t\t4          40735.3              [3.4e-03 1.7e-03 5.0e-03]           [2.4e-01 2.3e-01 2.7e-01]           [3.3e-01 3.4e-03 2.6e-03]          \n\n</pre> In\u00a0[129]: Copied! <pre>names = [\n    \"Ackley\",\n    \"Borehole\",\n    \"Elliptic\",\n    \"Cookie\"\n    ]\nMSP = 15\nMSNP = 50\nALPHA = 0.25\nEXPAND = 0.2\nncols = len(names)\nnrows = 2\nfig,ax = pyplot.subplots(nrows=nrows,ncols=ncols,figsize=(1.2*PW,1.2*PW/ncols*nrows),sharex=\"col\",sharey=False)\nax = np.atleast_1d(ax).reshape((nrows,ncols))\nfor j in range(ncols):\n    name = names[j]\n    data_runs = torch.load(\"./runs.%s.pt\"%name)\n    gptypes = data_runs[\"gptypes\"]\n    mtimes_all = data_runs[\"times\"].nanmedian(-1).values\n    ml2rerrors_all = data_runs[\"l2rerrors\"].nanmedian(-1).values\n    mbcerrors_all = data_runs[\"bcerrors\"].nanmedian(-1).values\n    xmin,xmax = np.log10(mtimes_all[mtimes_all.isfinite()].min()),np.log10(mtimes_all[mtimes_all.isfinite()].max())\n    ymin_0,ymax_0 = np.log10(ml2rerrors_all[ml2rerrors_all.isfinite()].min()),np.log10(ml2rerrors_all[ml2rerrors_all.isfinite()].max())\n    ymin_1,ymax_1 = np.log10(mbcerrors_all[mbcerrors_all.isfinite()].min()),np.log10(mbcerrors_all[mbcerrors_all.isfinite()].max())\n    ax[0,j].set_xlim([10**(xmin-EXPAND*(xmax-xmin)),10**(xmax+EXPAND*(xmax-xmin))])\n    ax[0,j].set_ylim([10**(ymin_0-EXPAND*(ymax_0-ymin_0)),10**(ymax_0+EXPAND*(ymax_0-ymin_0))])\n    ax[1,j].set_ylim([10**(ymin_1-EXPAND*(ymax_1-ymin_1)),10**(ymax_1+EXPAND*(ymax_1-ymin_1))])\n    for i in range(len(gptypes)):\n        mtimes = mtimes_all[i]\n        ml2rerrors = ml2rerrors_all[i]\n        mbcerrors = mbcerrors_all[i]\n        ax[0,j].scatter(mtimes,ml2rerrors,marker=MARKERS[i],color=COLORS[i],s=MSNP)\n        ax[1,j].scatter(mtimes,mbcerrors,marker=MARKERS[i],color=COLORS[i],s=MSNP)\n        better_times = mtimes[:,None]&lt;mtimes[None,:]\n        better_l2rerrors = ml2rerrors[:,None]&lt;ml2rerrors[None,:]\n        better_bcerrors = mbcerrors[:,None]&lt;mbcerrors[None,:]\n        fpareto_l2rerrors = ~(better_times*better_l2rerrors).any(0)\n        mtimes_pareto_l2rerrors = mtimes[fpareto_l2rerrors*mtimes.isfinite()*ml2rerrors.isfinite()]\n        ml2rerrors_pareto = ml2rerrors[fpareto_l2rerrors*mtimes.isfinite()*ml2rerrors.isfinite()]\n        ord_pareto_l2errors = mtimes_pareto_l2rerrors.argsort()\n        mtimes_pareto_l2rerrors_sort = mtimes_pareto_l2rerrors[ord_pareto_l2errors]\n        ml2rerrors_pareto_sort = ml2rerrors_pareto[ord_pareto_l2errors]\n        ax[0,j].plot(mtimes_pareto_l2rerrors_sort,ml2rerrors_pareto_sort,marker=MARKERS[i],color=COLORS[i],markersize=MSP,label=gptypes[i] if j==0 else None)\n        ax[0,j].hlines(y=ml2rerrors_pareto_sort[-1].item(),xmin=mtimes_pareto_l2rerrors_sort[-1].item(),xmax=ax[0,j].get_xlim()[1],color=COLORS[i],alpha=ALPHA)\n        ax[0,j].vlines(x=mtimes_pareto_l2rerrors_sort[0].item(),ymin=ml2rerrors_pareto_sort[0].item(),ymax=ax[0,j].get_ylim()[1],color=COLORS[i],alpha=ALPHA)\n        fpareto_bcerrors = ~(better_times*better_bcerrors).any(0)\n        mtimes_pareto_bcerrors = mtimes[fpareto_bcerrors*mtimes.isfinite()*mbcerrors.isfinite()]\n        mbcerrors_pareto = mbcerrors[fpareto_bcerrors*mtimes.isfinite()*mbcerrors.isfinite()]\n        ord_pareto_bcerrors = mtimes_pareto_bcerrors.argsort()\n        mtimes_pareto_bcerrors_sort = mtimes_pareto_bcerrors[ord_pareto_bcerrors]\n        mbcerrors_pareto_sort = mbcerrors_pareto[ord_pareto_bcerrors]\n        ax[1,j].plot(mtimes_pareto_bcerrors_sort,mbcerrors_pareto_sort,marker=MARKERS[i],color=COLORS[i],markersize=MSP)\n        ax[1,j].hlines(y=mbcerrors_pareto_sort[-1].item(),xmin=mtimes_pareto_bcerrors_sort[-1].item(),xmax=ax[1,j].get_xlim()[1],color=COLORS[i],alpha=ALPHA)\n        ax[1,j].vlines(x=mtimes_pareto_bcerrors_sort[0].item(),ymin=mbcerrors_pareto_sort[0].item(),ymax=ax[1,j].get_ylim()[1],color=COLORS[i],alpha=ALPHA)\n    ax[0,j].set_title(name)\n    for i in range(2):\n        ax[i,j].set_xscale(\"log\",base=10)\n        ax[i,j].set_yscale(\"log\",base=10)\n        ax[-1,j].set_xlabel(\"time per opt step\")\n        ax[0,0].set_ylabel(r\"regression relative error\")\n        ax[1,0].set_ylabel(r\"Bayesian cubature error\")\n        ax[i,j].tick_params(labelbottom=True,axis='both',which='major',pad=15)\nfig.legend(loc=\"lower center\",ncols=len(gptypes),bbox_to_anchor=(0.52,-1/20))\nfig.tight_layout()\nfig.savefig(\"./errors.pdf\",bbox_inches=\"tight\")\n</pre> names = [     \"Ackley\",     \"Borehole\",     \"Elliptic\",     \"Cookie\"     ] MSP = 15 MSNP = 50 ALPHA = 0.25 EXPAND = 0.2 ncols = len(names) nrows = 2 fig,ax = pyplot.subplots(nrows=nrows,ncols=ncols,figsize=(1.2*PW,1.2*PW/ncols*nrows),sharex=\"col\",sharey=False) ax = np.atleast_1d(ax).reshape((nrows,ncols)) for j in range(ncols):     name = names[j]     data_runs = torch.load(\"./runs.%s.pt\"%name)     gptypes = data_runs[\"gptypes\"]     mtimes_all = data_runs[\"times\"].nanmedian(-1).values     ml2rerrors_all = data_runs[\"l2rerrors\"].nanmedian(-1).values     mbcerrors_all = data_runs[\"bcerrors\"].nanmedian(-1).values     xmin,xmax = np.log10(mtimes_all[mtimes_all.isfinite()].min()),np.log10(mtimes_all[mtimes_all.isfinite()].max())     ymin_0,ymax_0 = np.log10(ml2rerrors_all[ml2rerrors_all.isfinite()].min()),np.log10(ml2rerrors_all[ml2rerrors_all.isfinite()].max())     ymin_1,ymax_1 = np.log10(mbcerrors_all[mbcerrors_all.isfinite()].min()),np.log10(mbcerrors_all[mbcerrors_all.isfinite()].max())     ax[0,j].set_xlim([10**(xmin-EXPAND*(xmax-xmin)),10**(xmax+EXPAND*(xmax-xmin))])     ax[0,j].set_ylim([10**(ymin_0-EXPAND*(ymax_0-ymin_0)),10**(ymax_0+EXPAND*(ymax_0-ymin_0))])     ax[1,j].set_ylim([10**(ymin_1-EXPAND*(ymax_1-ymin_1)),10**(ymax_1+EXPAND*(ymax_1-ymin_1))])     for i in range(len(gptypes)):         mtimes = mtimes_all[i]         ml2rerrors = ml2rerrors_all[i]         mbcerrors = mbcerrors_all[i]         ax[0,j].scatter(mtimes,ml2rerrors,marker=MARKERS[i],color=COLORS[i],s=MSNP)         ax[1,j].scatter(mtimes,mbcerrors,marker=MARKERS[i],color=COLORS[i],s=MSNP)         better_times = mtimes[:,None] In\u00a0[125]: Copied! <pre>bh_data = torch.load(\"runs.Borehole.pt\")\nprint(bh_data.keys())\n# ms_bh = torch.tensor([16,15,14,13,12,11,10,9,8,7,6,5],dtype=int)\nms_bh = torch.tensor([6,7,8,9,10,11,12],dtype=int)\nprint(bh_data['gptypes'])\nncols = len(bh_data['gptypes'])\nl2rerrors_mesh_bh = torch.flip(bh_data['l2rerrors'].nanmedian(-1).values.reshape((ncols,12,12))[:,-8:-1,-8:-1],dims=(-2,-1))\nprint(l2rerrors_mesh_bh.shape)\nfig,ax = pyplot.subplots(nrows=1,ncols=3,figsize=(PW,PW/3),sharex=False,sharey=False)\nCMAPCOLORS = [\"Purples_r\",\"Greens_r\",\"Blues_r\"]\nfor k in range(ncols):\n    cmap = sns.color_palette(CMAPCOLORS[k], as_cmap=True) # https://seaborn.pydata.org/tutorial/color_palettes.html\n    # cmap = sns.cubehelix_palette(start=1/2, rot=-1/2, reverse=False, as_cmap=True)\n    # cmap = sns.color_palette(\"ch:start=.2,rot=-.3\", as_cmap=True)\n    heatmap = ax[k].pcolor(torch.log10(l2rerrors_mesh_bh[k]).T,cmap=cmap) \n    cax = fig.add_axes([ax[k].get_position().x0,ax[k].get_position().y0-.18,ax[k].get_position().x1-ax[k].get_position().x0,0.05])\n    fig.colorbar(heatmap,cax,location=\"bottom\",label=r\"$\\log_{10}(\\text{regression relative error})$\",extend=\"neither\")\n    for i in range(len(ms_bh)):\n        for j in range(len(ms_bh)):\n            v = np.log10(l2rerrors_mesh_bh[k,i,j].item())\n            if not np.isnan(v):\n                ax[k].text(i+1/2,j+1/2,\"%.2f\"%v,ha=\"center\",va=\"center\",color=\"black\")\n            else:\n                ax[k].text(i+1/2,j+1/2,r\"x\",ha=\"center\",va=\"center\",color=\"red\")\n    ax[k].set_title(bh_data['gptypes'][k])\n    for j in range(2):\n        ax[k].set_xticks(np.arange(len(ms_bh))+1/2)\n        ax[k].set_xticklabels([r\"$2^{%d}$\"%m for m in ms_bh])\n        ax[k].set_yticks(np.arange(len(ms_bh))+1/2)\n        ax[k].set_yticklabels([r\"$2^{%d}$\"%m for m in ms_bh])\n        ax[k].tick_params(axis='both',which='major',pad=15)\n        ax[k].set_xlabel(r\"$n_1$\")\n        ax[0].set_ylabel(r\"$n_2$\")\n        ax[k].grid(False)\nfig.savefig(\"./borehole_errors.pdf\",bbox_inches=\"tight\")\n</pre> bh_data = torch.load(\"runs.Borehole.pt\") print(bh_data.keys()) # ms_bh = torch.tensor([16,15,14,13,12,11,10,9,8,7,6,5],dtype=int) ms_bh = torch.tensor([6,7,8,9,10,11,12],dtype=int) print(bh_data['gptypes']) ncols = len(bh_data['gptypes']) l2rerrors_mesh_bh = torch.flip(bh_data['l2rerrors'].nanmedian(-1).values.reshape((ncols,12,12))[:,-8:-1,-8:-1],dims=(-2,-1)) print(l2rerrors_mesh_bh.shape) fig,ax = pyplot.subplots(nrows=1,ncols=3,figsize=(PW,PW/3),sharex=False,sharey=False) CMAPCOLORS = [\"Purples_r\",\"Greens_r\",\"Blues_r\"] for k in range(ncols):     cmap = sns.color_palette(CMAPCOLORS[k], as_cmap=True) # https://seaborn.pydata.org/tutorial/color_palettes.html     # cmap = sns.cubehelix_palette(start=1/2, rot=-1/2, reverse=False, as_cmap=True)     # cmap = sns.color_palette(\"ch:start=.2,rot=-.3\", as_cmap=True)     heatmap = ax[k].pcolor(torch.log10(l2rerrors_mesh_bh[k]).T,cmap=cmap)      cax = fig.add_axes([ax[k].get_position().x0,ax[k].get_position().y0-.18,ax[k].get_position().x1-ax[k].get_position().x0,0.05])     fig.colorbar(heatmap,cax,location=\"bottom\",label=r\"$\\log_{10}(\\text{regression relative error})$\",extend=\"neither\")     for i in range(len(ms_bh)):         for j in range(len(ms_bh)):             v = np.log10(l2rerrors_mesh_bh[k,i,j].item())             if not np.isnan(v):                 ax[k].text(i+1/2,j+1/2,\"%.2f\"%v,ha=\"center\",va=\"center\",color=\"black\")             else:                 ax[k].text(i+1/2,j+1/2,r\"x\",ha=\"center\",va=\"center\",color=\"red\")     ax[k].set_title(bh_data['gptypes'][k])     for j in range(2):         ax[k].set_xticks(np.arange(len(ms_bh))+1/2)         ax[k].set_xticklabels([r\"$2^{%d}$\"%m for m in ms_bh])         ax[k].set_yticks(np.arange(len(ms_bh))+1/2)         ax[k].set_yticklabels([r\"$2^{%d}$\"%m for m in ms_bh])         ax[k].tick_params(axis='both',which='major',pad=15)         ax[k].set_xlabel(r\"$n_1$\")         ax[0].set_ylabel(r\"$n_2$\")         ax[k].grid(False) fig.savefig(\"./borehole_errors.pdf\",bbox_inches=\"tight\") <pre>dict_keys(['ns', 'gptypes', 'times', 'l2rerrors', 'bcerrors'])\n['Cholesky SE', 'CG SE (GPyTorch)', 'Fast DSI (ours)']\ntorch.Size([3, 7, 7])\n</pre> In\u00a0[\u00a0]: Copied! <pre>class Advection:\n    def __init__(self, height, width, c, xm, xa, xb):\n        \"\"\"PBC\"\"\"\n        self.h = height  # height\n        self.w = width  # width\n        self.xm = xm  # center location\n        self.c = c  # velocity\n        self.xa = xa  # Left domain point\n        self.xb = xb  # right domain point\n\n    def solve(self, X):\n        x = X[:, 0:1]\n        t = X[:, 1:2]\n        n = np.shape(x)[0]\n        u = np.zeros_like(x)\n        h = self.h\n        w = self.w\n        c = self.c\n        xm = self.xm\n        xa = self.xa\n        xb = self.xb\n        for i in range(n):\n            na = (xm - w / 2.0 + c * t[i, 0] - xa) // (xb - xa)\n            nb = (xm + w / 2.0 + c * t[i, 0] - xa) // (xb - xa)\n            xl = xa + (xm - w / 2.0 + c * t[i, 0] - xa) - na * (xb - xa)\n            xr = xa + (xm + w / 2.0 + c * t[i, 0] - xa) - nb * (xb - xa)\n            if xl &lt; xr:\n                if x[i, 0] &gt;= xl and x[i, 0] &lt;= xr:\n                    u[i, 0] = h\n                else:\n                    u[i, 0] = 0.0\n            else:\n                if x[i, 0] &gt;= xl or x[i, 0] &lt;= xr:\n                    u[i, 0] = h\n                else:\n                    u[i, 0] = 0.0\n        # for i in range(n):\n        #    if (x[i,0]-c*t[i,0]&gt;=xm-w/2.0 and x[i,0]-c*t[i,0]&lt;=xm+w/2.0):\n        #        u[i,0] = h\n        #    else:\n        #        u[i,0] = 0.0\n        return u\n\n\nclass Advection_v2:\n    def __init__(self, x0, x1, c1, w, h1, c2, a, h2):\n        self.x0 = x0\n        self.x1 = x1\n        self.c1 = c1\n        self.w = w\n        self.h1 = h1\n        self.c2 = c2\n        self.a = a\n        self.h2 = h2\n\n    def solve(self, X):\n        x = X[:, 0:1] - X[:, 1:2]\n        dx = x - self.x0\n        n = dx // (self.x1 - self.x0)\n        x -= n * (self.x1 - self.x0)\n\n        xl = self.c1 - self.w / 2\n        xr = self.c1 + self.w / 2\n        u1 = self.h1 * np.heaviside(x - xl, 0.5) * np.heaviside(xr - x, 0.5)\n        u2 = np.sqrt(np.maximum(self.h2 ** 2 - self.a ** 2 * (x - self.c2) ** 2, 0))\n        return u1 + u2\n\nif False:\n    import deepxde as dde\n    nx = 40\n    nt = 40\n    geom = dde.geometry.Interval(0, 1)\n    timedomain = dde.geometry.TimeDomain(0, 1)\n    geomtime = dde.geometry.GeometryXTime(geom, timedomain)\n    x = geomtime.uniform_points(nx * nt, boundary=True)\n\n    z = qp.DigitalNetB2(dimension=6,randomize=\"DS\",seed=7)(2**13)\n    u1 = []\n    u2 = []\n    for i in range(z.shape[0]):\n        pde1 = Advection(\n            height = z[i,0] + 1,  # [1, 2] \n            width = 0.3 * z[i,1] + 0.3,  # [0.3, 0.6],\n            c = 1,\n            xm = 0.4 * z[i,2] + 0.3,  # x0, [0.3, 0.7]\n            xa = 0,\n            xb = 1)\n        pde2 = Advection_v2(\n            x0 = 0,\n            x1 = 1,\n            c1 = 0.1 * z[i,0] + 0.2,  # [0.2, 0.3]\n            w = 0.1 * z[i,1] + 0.1,  # [0.1, 0.2]\n            h1 = 1.5 * z[i,2] + 0.5,  # [0.5, 2]\n            c2 = 0.1 * z[i,3] + 0.7,  # [0.7, 0.8]\n            a = 5 * z[i,4] + 5,  # 1 / [0.1, 0.2] = [5, 10]\n            h2 = 1.5 * z[i,5] + 0.5)  # [0.5, 2]\n        u1.append(pde1.solve(x).reshape(nt, nx))\n        u2.append(pde2.solve(x).reshape(nt, nx))\n    u1 = np.array(u1)\n    u2 = np.array(u2)\n    data = {\"x1\":torch.from_numpy(z[:,:3]),\"y1\":torch.from_numpy(u1[:,nt//2,:]).T,\"x2\":torch.from_numpy(z),\"y2\":torch.from_numpy(u2[:,nt//2,:]).T}\n    torch.save(data,\"./advection.pt\")\n</pre> class Advection:     def __init__(self, height, width, c, xm, xa, xb):         \"\"\"PBC\"\"\"         self.h = height  # height         self.w = width  # width         self.xm = xm  # center location         self.c = c  # velocity         self.xa = xa  # Left domain point         self.xb = xb  # right domain point      def solve(self, X):         x = X[:, 0:1]         t = X[:, 1:2]         n = np.shape(x)[0]         u = np.zeros_like(x)         h = self.h         w = self.w         c = self.c         xm = self.xm         xa = self.xa         xb = self.xb         for i in range(n):             na = (xm - w / 2.0 + c * t[i, 0] - xa) // (xb - xa)             nb = (xm + w / 2.0 + c * t[i, 0] - xa) // (xb - xa)             xl = xa + (xm - w / 2.0 + c * t[i, 0] - xa) - na * (xb - xa)             xr = xa + (xm + w / 2.0 + c * t[i, 0] - xa) - nb * (xb - xa)             if xl &lt; xr:                 if x[i, 0] &gt;= xl and x[i, 0] &lt;= xr:                     u[i, 0] = h                 else:                     u[i, 0] = 0.0             else:                 if x[i, 0] &gt;= xl or x[i, 0] &lt;= xr:                     u[i, 0] = h                 else:                     u[i, 0] = 0.0         # for i in range(n):         #    if (x[i,0]-c*t[i,0]&gt;=xm-w/2.0 and x[i,0]-c*t[i,0]&lt;=xm+w/2.0):         #        u[i,0] = h         #    else:         #        u[i,0] = 0.0         return u   class Advection_v2:     def __init__(self, x0, x1, c1, w, h1, c2, a, h2):         self.x0 = x0         self.x1 = x1         self.c1 = c1         self.w = w         self.h1 = h1         self.c2 = c2         self.a = a         self.h2 = h2      def solve(self, X):         x = X[:, 0:1] - X[:, 1:2]         dx = x - self.x0         n = dx // (self.x1 - self.x0)         x -= n * (self.x1 - self.x0)          xl = self.c1 - self.w / 2         xr = self.c1 + self.w / 2         u1 = self.h1 * np.heaviside(x - xl, 0.5) * np.heaviside(xr - x, 0.5)         u2 = np.sqrt(np.maximum(self.h2 ** 2 - self.a ** 2 * (x - self.c2) ** 2, 0))         return u1 + u2  if False:     import deepxde as dde     nx = 40     nt = 40     geom = dde.geometry.Interval(0, 1)     timedomain = dde.geometry.TimeDomain(0, 1)     geomtime = dde.geometry.GeometryXTime(geom, timedomain)     x = geomtime.uniform_points(nx * nt, boundary=True)      z = qp.DigitalNetB2(dimension=6,randomize=\"DS\",seed=7)(2**13)     u1 = []     u2 = []     for i in range(z.shape[0]):         pde1 = Advection(             height = z[i,0] + 1,  # [1, 2]              width = 0.3 * z[i,1] + 0.3,  # [0.3, 0.6],             c = 1,             xm = 0.4 * z[i,2] + 0.3,  # x0, [0.3, 0.7]             xa = 0,             xb = 1)         pde2 = Advection_v2(             x0 = 0,             x1 = 1,             c1 = 0.1 * z[i,0] + 0.2,  # [0.2, 0.3]             w = 0.1 * z[i,1] + 0.1,  # [0.1, 0.2]             h1 = 1.5 * z[i,2] + 0.5,  # [0.5, 2]             c2 = 0.1 * z[i,3] + 0.7,  # [0.7, 0.8]             a = 5 * z[i,4] + 5,  # 1 / [0.1, 0.2] = [5, 10]             h2 = 1.5 * z[i,5] + 0.5)  # [0.5, 2]         u1.append(pde1.solve(x).reshape(nt, nx))         u2.append(pde2.solve(x).reshape(nt, nx))     u1 = np.array(u1)     u2 = np.array(u2)     data = {\"x1\":torch.from_numpy(z[:,:3]),\"y1\":torch.from_numpy(u1[:,nt//2,:]).T,\"x2\":torch.from_numpy(z),\"y2\":torch.from_numpy(u2[:,nt//2,:]).T}     torch.save(data,\"./advection.pt\")  In\u00a0[23]: Copied! <pre>adv_data = torch.load(\"./advection.pt\")\n# x_adv = adv_data[\"x1\"]\n# y_adv = adv_data[\"y1\"]\nx_adv = adv_data[\"x2\"]\ny_adv = adv_data[\"y2\"]\ndef adv_pregen(level, x):\n    assert 0&lt;=level&lt;40\n    x_data = x_adv[:x.size(0)]\n    y_data = y_adv[:,:x.size(0)]\n    assert (x_data==x).all()\n    return y_data[level]\nx_adv_test = x_adv[-2**8:]\ny_adv_test = y_adv[:,-2**8:]\ntimes,yhatts,muhats,sigmahats = fit_gps(\n    func = adv_pregen,\n    d = x_adv.shape[-1], \n    levelops = [i for i in range(40)],\n    n = 2**10*torch.ones(40,dtype=int),\n    iterations = 50,\n    xtest = x_adv_test,\n    fit_std_gp = True,\n    fit_gpytorch_gp = True, \n    fit_fgpdnet = True, \n    fit_fgplat = False,\n    requires_grad_noise = True, \n    seeds = [7 for i in range(40)],\n    rank = 5, \n    sgp_threshold = 2**14, \n    gpt_threshold = 2**14,\n    verbose = 5, \n    predlevels = [i for i in range(40)],\n    compute_muhats = True,\n    compute_sigmahats = False,\n    n_gp = 2**7*torch.ones(40,dtype=int),\n    n_gpt = 2**7*torch.ones(40,dtype=int),\n    n_fgpdnet = 2**7*torch.ones(40,dtype=int),\n    n_fgplat = None,)\nprint(\"times:\",times)\nprint(yhatts.shape)\nprint(muhats.shape)\n# print(sigmahats.shape)\n</pre> adv_data = torch.load(\"./advection.pt\") # x_adv = adv_data[\"x1\"] # y_adv = adv_data[\"y1\"] x_adv = adv_data[\"x2\"] y_adv = adv_data[\"y2\"] def adv_pregen(level, x):     assert 0&lt;=level&lt;40     x_data = x_adv[:x.size(0)]     y_data = y_adv[:,:x.size(0)]     assert (x_data==x).all()     return y_data[level] x_adv_test = x_adv[-2**8:] y_adv_test = y_adv[:,-2**8:] times,yhatts,muhats,sigmahats = fit_gps(     func = adv_pregen,     d = x_adv.shape[-1],      levelops = [i for i in range(40)],     n = 2**10*torch.ones(40,dtype=int),     iterations = 50,     xtest = x_adv_test,     fit_std_gp = True,     fit_gpytorch_gp = True,      fit_fgpdnet = True,      fit_fgplat = False,     requires_grad_noise = True,      seeds = [7 for i in range(40)],     rank = 5,      sgp_threshold = 2**14,      gpt_threshold = 2**14,     verbose = 5,      predlevels = [i for i in range(40)],     compute_muhats = True,     compute_sigmahats = False,     n_gp = 2**7*torch.ones(40,dtype=int),     n_gpt = 2**7*torch.ones(40,dtype=int),     n_fgpdnet = 2**7*torch.ones(40,dtype=int),     n_fgplat = None,) print(\"times:\",times) print(yhatts.shape) print(muhats.shape) # print(sigmahats.shape) <pre>     iter of 5.0e+01 | best loss  | loss      \n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n            0.00e+00 | 2.52e+04   | 2.52e+04  \n            5.00e+00 | 4.72e+02   | 4.72e+02  \n            1.00e+01 | -1.35e+03  | -1.35e+03 \n            1.50e+01 | -2.43e+03  | -2.43e+03 \n            2.00e+01 | -3.19e+03  | -3.19e+03 \n            2.50e+01 | -4.61e+03  | -4.61e+03 \n            3.00e+01 | -5.59e+03  | -5.59e+03 \n            3.50e+01 | -6.44e+03  | -6.44e+03 \n            4.00e+01 | -8.05e+03  | -8.05e+03 \n            4.50e+01 | -8.27e+03  | -8.27e+03 \n            5.00e+01 | -9.01e+03  | -9.01e+03 \niter: 0       GPT loss: 1.80e-01\niter: 5       GPT loss: 6.47e-02\niter: 10      GPT loss: -6.73e-02\niter: 15      GPT loss: -2.63e-01\niter: 20      GPT loss: 6.69e-01\niter: 25      GPT loss: -2.24e-01\niter: 30      GPT loss: 6.55e-01\niter: 35      GPT loss: 6.28e-01\niter: 40      GPT loss: 3.04e-01\niter: 45      GPT loss: 4.46e-01\niter: 49      GPT loss: -9.38e-02\n     iter of 5.0e+01 | best loss  | loss      \n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n            0.00e+00 | 7.05e+03   | 7.05e+03  \n            5.00e+00 | 9.70e+02   | 1.39e+03  \n            1.00e+01 | -7.81e+02  | -7.81e+02 \n            1.50e+01 | -2.30e+03  | -2.30e+03 \n            2.00e+01 | -2.87e+03  | -2.87e+03 \n            2.50e+01 | -4.03e+03  | -4.03e+03 \n            3.00e+01 | -5.23e+03  | -5.23e+03 \n            3.50e+01 | -5.60e+03  | -5.60e+03 \n            4.00e+01 | -6.44e+03  | -6.44e+03 \n            4.50e+01 | -7.26e+03  | -7.26e+03 \n            5.00e+01 | -8.86e+03  | -8.86e+03 \ntimes: tensor([4.9596, 0.4799, 0.7592])\ntorch.Size([3, 40, 256])\ntorch.Size([3, 40])\n</pre> In\u00a0[24]: Copied! <pre>with np.printoptions(formatter={\"float\":lambda x: \"%.1e\"%x}):\n    print(\"mean L2 relative errors: %s\"%str((torch.linalg.norm(y_adv_test-yhatts,dim=-2)/torch.linalg.norm(y_adv_test,dim=-2)).mean(-1)))\nALPHA = 0.25\nLW = .1\ngp_names = [\"Cholesky SE\",\"CG SE (GPyTorch)\",\"Fast DSI (ours)\"]\nxticks = torch.linspace(0,1,40)\nfig,ax = pyplot.subplots(nrows=2,ncols=3,figsize=(PW,PW*2/4),sharex=True,sharey=False)\nR = 7\nax[0,0].plot(xticks,y_adv_test[:,:2**0],color='k')#,linewidth=LW)\nfor i in range(3):\n    ax[0,1].plot(xticks,torch.abs(y_adv_test-yhatts[i]).median(-1).values,color=COLORS[i])\n    ax[0,2].plot(xticks,torch.abs(y_adv.mean(-1)-muhats[i]),label=gp_names[i],color=COLORS[i])\n    ax[1,i].plot(xticks,y_adv_test[:,R],color='k')\n    ax[1,i].plot(xticks,yhatts[i,:,R],color=COLORS[i])\n    # ax[1,i].fill_between(xticks,yhatts[i,:,R]-sigmahats[i,:,R],yhatts[i,:,R]+sigmahats[i,:,R],color=COLORS[i],alpha=ALPHA)\nax[0,1].set_yscale(\"log\",base=10)\nax[0,2].set_yscale(\"log\",base=10)\nfig.legend();\n# ax[1,0].set_yscale(\"log\",base=10)\n</pre> with np.printoptions(formatter={\"float\":lambda x: \"%.1e\"%x}):     print(\"mean L2 relative errors: %s\"%str((torch.linalg.norm(y_adv_test-yhatts,dim=-2)/torch.linalg.norm(y_adv_test,dim=-2)).mean(-1))) ALPHA = 0.25 LW = .1 gp_names = [\"Cholesky SE\",\"CG SE (GPyTorch)\",\"Fast DSI (ours)\"] xticks = torch.linspace(0,1,40) fig,ax = pyplot.subplots(nrows=2,ncols=3,figsize=(PW,PW*2/4),sharex=True,sharey=False) R = 7 ax[0,0].plot(xticks,y_adv_test[:,:2**0],color='k')#,linewidth=LW) for i in range(3):     ax[0,1].plot(xticks,torch.abs(y_adv_test-yhatts[i]).median(-1).values,color=COLORS[i])     ax[0,2].plot(xticks,torch.abs(y_adv.mean(-1)-muhats[i]),label=gp_names[i],color=COLORS[i])     ax[1,i].plot(xticks,y_adv_test[:,R],color='k')     ax[1,i].plot(xticks,yhatts[i,:,R],color=COLORS[i])     # ax[1,i].fill_between(xticks,yhatts[i,:,R]-sigmahats[i,:,R],yhatts[i,:,R]+sigmahats[i,:,R],color=COLORS[i],alpha=ALPHA) ax[0,1].set_yscale(\"log\",base=10) ax[0,2].set_yscale(\"log\",base=10) fig.legend(); # ax[1,0].set_yscale(\"log\",base=10) <pre>mean L2 relative errors: tensor([0.3217, 0.7101, 0.3234])\n</pre> In\u00a0[\u00a0]: Copied! <pre>muq_data = torch.load(\"./muq_data.pt\")\nx_muq = muq_data[\"x\"]\ny_muq = muq_data[\"y\"]\ndef muq_pregen(level, x):\n    assert 0&lt;=level&lt;31\n    x_data = x_muq[:x.size(0)]\n    y_data = y_muq[:,:x.size(0)]\n    assert (x_data==x).all()\n    return y_data[level]\nx_muq_test = x_muq[-2**8:]\ny_muq_test = y_muq[:,-2**8:]\ntimes,yhatts,muhats,sigmahats = fit_gps(\n    func = muq_pregen,\n    d = 3, \n    levelops = [i for i in range(31)],\n    n = 2**10*torch.ones(31,dtype=int),\n    iterations = 50,\n    xtest = x_muq_test,\n    fit_std_gp = True,\n    fit_gpytorch_gp = True, \n    fit_fgpdnet = True, \n    fit_fgplat = False,\n    requires_grad_noise = False, \n    seeds = [7 for i in range(31)],\n    rank = 31, \n    sgp_threshold = 2**12, \n    gpt_threshold = 2**12,\n    verbose = 5, \n    predlevels = [i for i in range(31)],\n    compute_muhats = True,\n    compute_sigmahats = False,\n    n_gp = 2**5*torch.ones(31,dtype=int),\n    n_gpt = 2**5*torch.ones(31,dtype=int),\n    n_fgpdnet = 2**8*torch.ones(31,dtype=int),\n    n_fgplat = None,)\nprint(\"times:\",times)\nprint(yhatts.shape) \nprint(muhats.shape) \n# print(sigmahats.shape) \n</pre> muq_data = torch.load(\"./muq_data.pt\") x_muq = muq_data[\"x\"] y_muq = muq_data[\"y\"] def muq_pregen(level, x):     assert 0&lt;=level&lt;31     x_data = x_muq[:x.size(0)]     y_data = y_muq[:,:x.size(0)]     assert (x_data==x).all()     return y_data[level] x_muq_test = x_muq[-2**8:] y_muq_test = y_muq[:,-2**8:] times,yhatts,muhats,sigmahats = fit_gps(     func = muq_pregen,     d = 3,      levelops = [i for i in range(31)],     n = 2**10*torch.ones(31,dtype=int),     iterations = 50,     xtest = x_muq_test,     fit_std_gp = True,     fit_gpytorch_gp = True,      fit_fgpdnet = True,      fit_fgplat = False,     requires_grad_noise = False,      seeds = [7 for i in range(31)],     rank = 31,      sgp_threshold = 2**12,      gpt_threshold = 2**12,     verbose = 5,      predlevels = [i for i in range(31)],     compute_muhats = True,     compute_sigmahats = False,     n_gp = 2**5*torch.ones(31,dtype=int),     n_gpt = 2**5*torch.ones(31,dtype=int),     n_fgpdnet = 2**8*torch.ones(31,dtype=int),     n_fgplat = None,) print(\"times:\",times) print(yhatts.shape)  print(muhats.shape)  # print(sigmahats.shape)  <pre>torch.Size([32768, 3]) torch.Size([31, 32768])\n</pre> <pre>\n---------------------------------------------------------------------------\nAssertionError                            Traceback (most recent call last)\nCell In[152], line 4\n      2 x_muq = muq_data[\"x\"]\n      3 y_muq = muq_data[\"y\"]\n----&gt; 4 print(x_muq.shape,y_muq.shape); assert False\n      5 def muq_pregen(level, x):\n      6     assert 0&lt;=level&lt;31\n\nAssertionError: </pre> In\u00a0[\u00a0]: Copied! <pre>ALPHA = 0.25\nLW = .1\ngp_names = [\"Cholesky SE\",\"CG SE (GPyTorch)\",\"Fast DSI (ours)\"]\nxticks = torch.arange(31)\nfig,ax = pyplot.subplots(nrows=1,ncols=4,figsize=(PW,PW/4),sharex=True,sharey=False)\nR = 0\nfor i in range(3):\n    ax[0].plot(xticks,y_muq_test[:,:2**7],color='k',linewidth=LW)\n    ax[2].plot(xticks[1:],torch.abs(y_muq_test-yhatts[i]).median(-1).values[1:])\n    ax[3].plot(xticks[1:],torch.abs(y_muq_test.mean(-1)-muhats[i])[1:],label=gp_names[i])\nax[2].set_yscale(\"log\",base=10)\nax[3].set_yscale(\"log\",base=10)\nfig.legend();\n# ax[1,0].set_yscale(\"log\",base=10)\n</pre> ALPHA = 0.25 LW = .1 gp_names = [\"Cholesky SE\",\"CG SE (GPyTorch)\",\"Fast DSI (ours)\"] xticks = torch.arange(31) fig,ax = pyplot.subplots(nrows=1,ncols=4,figsize=(PW,PW/4),sharex=True,sharey=False) R = 0 for i in range(3):     ax[0].plot(xticks,y_muq_test[:,:2**7],color='k',linewidth=LW)     ax[2].plot(xticks[1:],torch.abs(y_muq_test-yhatts[i]).median(-1).values[1:])     ax[3].plot(xticks[1:],torch.abs(y_muq_test.mean(-1)-muhats[i])[1:],label=gp_names[i]) ax[2].set_yscale(\"log\",base=10) ax[3].set_yscale(\"log\",base=10) fig.legend(); # ax[1,0].set_yscale(\"log\",base=10)"},{"location":"examples/papers/fmtgps/fmtgps/#draft-paper-on-fast-multitask-gps","title":"Draft Paper on Fast Multitask GPs\u00b6","text":""},{"location":"examples/papers/fmtgps/fmtgps/#setup","title":"Setup\u00b6","text":""},{"location":"examples/papers/fmtgps/fmtgps/#points","title":"Points\u00b6","text":""},{"location":"examples/papers/fmtgps/fmtgps/#inversion-viz","title":"Inversion Viz\u00b6","text":""},{"location":"examples/papers/fmtgps/fmtgps/#problems","title":"Problems\u00b6","text":""},{"location":"examples/papers/fmtgps/fmtgps/#benchmarks","title":"Benchmarks\u00b6","text":""},{"location":"examples/papers/fmtgps/fmtgps/#muq-beam","title":"MUQ Beam\u00b6","text":""},{"location":"examples/papers/fmtgps/fmtgps/#cookie","title":"Cookie\u00b6","text":""},{"location":"examples/papers/fmtgps/fmtgps/#solvers","title":"Solvers\u00b6","text":""},{"location":"examples/papers/fmtgps/fmtgps/#examples","title":"Examples\u00b6","text":""},{"location":"examples/papers/fmtgps/fmtgps/#plot-mtgp-1d","title":"Plot MTGP 1d\u00b6","text":""},{"location":"examples/papers/fmtgps/fmtgps/#plot-mtgp-2d","title":"Plot MTGP 2d\u00b6","text":""},{"location":"examples/papers/fmtgps/fmtgps/#simple","title":"Simple\u00b6","text":""},{"location":"examples/papers/fmtgps/fmtgps/#timing","title":"Timing\u00b6","text":""},{"location":"examples/papers/fmtgps/fmtgps/#errors-vs-times","title":"Errors vs Times\u00b6","text":""},{"location":"examples/papers/fmtgps/fmtgps/#borehole-analysis","title":"Borehole Analysis\u00b6","text":""},{"location":"examples/papers/fmtgps/fmtgps/#advection-equation","title":"Advection Equation\u00b6","text":"<p>https://github.com/MatthieuDarcy/KernelsOperatorLearning/blob/main/LowDataRegime/Advection%20equation%20I/Advection%20equation.ipynb</p> <p>https://github.com/lu-group/deeponet-fno/blob/main/data/advection/advection.py</p>"},{"location":"examples/papers/fmtgps/fmtgps/#muq-beam","title":"MUQ Beam\u00b6","text":""},{"location":"examples/papers/probnum25/probnum25/","title":"ProbNum25","text":"In\u00a0[4]: Copied! <pre>FULLRUN = True\n</pre> FULLRUN = True In\u00a0[5]: Copied! <pre>import fastgps\nimport qmcpy as qp \nimport torch \nimport numpy as np\nimport scipy.stats\nimport time\nimport pandas as pd\nimport warnings\nfrom matplotlib import pyplot,cm,colors\nimport warnings \nwarnings.simplefilter(\"ignore\")\n</pre> import fastgps import qmcpy as qp  import torch  import numpy as np import scipy.stats import time import pandas as pd import warnings from matplotlib import pyplot,cm,colors import warnings  warnings.simplefilter(\"ignore\") In\u00a0[6]: Copied! <pre>CISIZE = .99\nZSTAR = scipy.stats.norm.ppf(CISIZE+(1-CISIZE)/2)\nCOLORS = [\"xkcd:\"+color[:-1] for color in pd.read_csv(\"../../../../xkcd_colors.txt\",comment=\"#\").iloc[:,0].tolist()][::-1]\n#pyplot.style.use(\"seaborn-v0_8-whitegrid\")\nfrom tueplots.bundles import probnum2025\npyplot.rcParams.update(probnum2025())\nimport matplotlib \nmatplotlib.rcParams['figure.dpi'] = 256\n_golden = (1 + 5 ** 0.5) / 2\nMW1 = 240/72\nMW2 = 500/72\nif not FULLRUN:\n    MW1 *= 2\n    MW2 *= 2\nMH1 = MW1/_golden\nMH2 = MW2/_golden\n</pre> CISIZE = .99 ZSTAR = scipy.stats.norm.ppf(CISIZE+(1-CISIZE)/2) COLORS = [\"xkcd:\"+color[:-1] for color in pd.read_csv(\"../../../../xkcd_colors.txt\",comment=\"#\").iloc[:,0].tolist()][::-1] #pyplot.style.use(\"seaborn-v0_8-whitegrid\") from tueplots.bundles import probnum2025 pyplot.rcParams.update(probnum2025()) import matplotlib  matplotlib.rcParams['figure.dpi'] = 256 _golden = (1 + 5 ** 0.5) / 2 MW1 = 240/72 MW2 = 500/72 if not FULLRUN:     MW1 *= 2     MW2 *= 2 MH1 = MW1/_golden MH2 = MW2/_golden In\u00a0[4]: Copied! <pre>torch.manual_seed(11)\ntorch.set_default_dtype(torch.float64)\n</pre> torch.manual_seed(11) torch.set_default_dtype(torch.float64) In\u00a0[5]: Copied! <pre>def f_grad_f(f,x):\n    assert x.ndim==2\n    d = x.shape[1]\n    xs = [x[:,j] for j in range(d)]\n    for j in range(d): xs[j].requires_grad_()\n    x = torch.vstack(xs).T\n    y = f(x)\n    grad = [None]*d \n    for j in range(d):\n        grad[j] = torch.autograd.grad(y,xs[j],grad_outputs=torch.ones_like(y),create_graph=True)[0]\n    y_grad_y = torch.vstack([y]+grad).T \n    return y_grad_y.detach()\ndef f_curve_1d(x):\n    return np.pi*x[:,0]*torch.sin(4*np.pi*x[:,0])*torch.exp(-.01/(1/2-torch.abs(x[:,0]-1/2)))\ndef f_G(x): # GFunction from https://www.sfu.ca/~ssurjano/gfunc.html\n    assert x.ndim==2\n    d = x.shape[1] \n    a = (torch.arange(1,d+1)-2)/2\n    y = torch.prod((torch.abs(4*x-2)+a)/(1+a),dim=1)\n    return y\ndef f_peaks(x): # peaks function \n    x = 6*x-3\n    x1,x2 = x[:,0],x[:,1]\n    t1 = 3*(1-x1)**2*torch.exp(-x1**2-(x2+1)**2)\n    t2 = 10*(x1/5-x1**3-x2**5)*torch.exp(-x1**2-x2**2)\n    t3 = 1/3*torch.exp(-(x1-1)**2-x2**2)\n    return t1-t2-t3\ndef f_ackley(x, a=20, b=0.2, c=2*np.pi, scaling=32.768): # https://www.sfu.ca/~ssurjano/ackley.html\n    x = 2*scaling*x-scaling\n    t1 = a*torch.exp(-b*torch.sqrt(torch.mean(x**2,1)))\n    t2 = torch.exp(torch.mean(torch.cos(c*x),1))\n    t3 = a+np.exp(1)\n    y = -t1-t2+t3\n    return y\ndef f_brownian_motion(t, seed=23, numklterms=1000): # eq 6.8 https://artowen.su.domains/mc/Ch-processes.pdf\n    assert t.size(1)==1\n    j = torch.arange(numklterms)\n    z_j = torch.from_numpy(np.random.Generator(np.random.PCG64(seed)).uniform(0,1,numklterms))\n    bm = np.sqrt(2)/np.pi*(z_j*2/(2*j+1)*torch.sin((2*j+1)/2*np.pi*t)).sum(1)\n    return bm\ndef f_branin_2d(x, a=1, b=5.1/(4*np.pi**2), c=5/np.pi, r=6, s=10, t=1/(8*np.pi)): # https://www.sfu.ca/~ssurjano/branin.html\n    assert x.size(1)==2\n    x1,x2 = 15*x[:,0]-5,15*x[:,1]\n    y = a*(x2-b*x1**2+c*x1-r)**2+s*(1-t)*torch.cos(x1)+s\n    return y\ndef f_camel_six_hump_2d(x): # https://www.indusmic.com/post/six-hump-camel-function\n    assert x.size(1)==2\n    x1,x2 = 6*x[:,0]-3,4*x[:,1]-2\n    y = (4-2.1*x1**2+x1**4/3)*x1**2+x1*x2+(-4+4*x2**2)*x2**2\n    return y \ndef styblinski_tang(x): # https://www.sfu.ca/~ssurjano/stybtang.html\n    x = 10*x-5\n    y = 1/2*torch.sum(x**4-16*x**2+5*x,1)\n    return y\ndef hartmann_6d(x):\n    assert x.size(1)==6 \n    alpha = torch.tensor([1.0, 1.2, 3.0, 3.2])\n    A = torch.tensor([\n        [10,   3,   17,   3.5, 1.7, 8],\n        [0.05, 10,  17,   0.1, 8,   14],\n        [3,    3.5, 1.7,  10,  17,  8],\n        [17,   8,   0.05, 10,  0.1,  14]])\n    P = 10**(-4)*torch.tensor([\n        [1312, 1696, 5569, 124, 8283, 5886],\n        [2329, 4135, 8307, 3736, 1004, 9991],\n        [2348, 1451, 3522, 2883, 3047, 6650],\n        [4047, 8828, 8732, 5743, 1091, 381]])\n    y = -1/1.94*(2.58+torch.sum(alpha*torch.exp(-torch.einsum(\"ij,rij-&gt;ri\",A,(x[:,None,:]-P)**2)),1))\n    return y\ndef welch(x):\n    assert x.size(1)==20\n    x1,x2,x3,x4,x5,x6,x7,x8,x9,x10,x11,x12,x13,x14,x15,x16,x17,x18,x19,x20 = x[:,0],x[:,1],x[:,2],x[:,3],x[:,4],x[:,5],x[:,6],x[:,7],x[:,8],x[:,9],x[:,10],x[:,11],x[:,12],x[:,13],x[:,14],x[:,15],x[:,16],x[:,17],x[:,18],x[:,19]\n    y = 5*x12/(1+x1)+5*(x4-x20)**2+x5+40*x19**3-5*x19+0.05*x2+0.08*x3-0.03*x6+0.03*x7-0.09*x9-0.01*x10-0.07*x11+0.25*x13**2-0.04*x14+0.06*x15-0.01*x17-0.03*x18\n    return y\n</pre> def f_grad_f(f,x):     assert x.ndim==2     d = x.shape[1]     xs = [x[:,j] for j in range(d)]     for j in range(d): xs[j].requires_grad_()     x = torch.vstack(xs).T     y = f(x)     grad = [None]*d      for j in range(d):         grad[j] = torch.autograd.grad(y,xs[j],grad_outputs=torch.ones_like(y),create_graph=True)[0]     y_grad_y = torch.vstack([y]+grad).T      return y_grad_y.detach() def f_curve_1d(x):     return np.pi*x[:,0]*torch.sin(4*np.pi*x[:,0])*torch.exp(-.01/(1/2-torch.abs(x[:,0]-1/2))) def f_G(x): # GFunction from https://www.sfu.ca/~ssurjano/gfunc.html     assert x.ndim==2     d = x.shape[1]      a = (torch.arange(1,d+1)-2)/2     y = torch.prod((torch.abs(4*x-2)+a)/(1+a),dim=1)     return y def f_peaks(x): # peaks function      x = 6*x-3     x1,x2 = x[:,0],x[:,1]     t1 = 3*(1-x1)**2*torch.exp(-x1**2-(x2+1)**2)     t2 = 10*(x1/5-x1**3-x2**5)*torch.exp(-x1**2-x2**2)     t3 = 1/3*torch.exp(-(x1-1)**2-x2**2)     return t1-t2-t3 def f_ackley(x, a=20, b=0.2, c=2*np.pi, scaling=32.768): # https://www.sfu.ca/~ssurjano/ackley.html     x = 2*scaling*x-scaling     t1 = a*torch.exp(-b*torch.sqrt(torch.mean(x**2,1)))     t2 = torch.exp(torch.mean(torch.cos(c*x),1))     t3 = a+np.exp(1)     y = -t1-t2+t3     return y def f_brownian_motion(t, seed=23, numklterms=1000): # eq 6.8 https://artowen.su.domains/mc/Ch-processes.pdf     assert t.size(1)==1     j = torch.arange(numklterms)     z_j = torch.from_numpy(np.random.Generator(np.random.PCG64(seed)).uniform(0,1,numklterms))     bm = np.sqrt(2)/np.pi*(z_j*2/(2*j+1)*torch.sin((2*j+1)/2*np.pi*t)).sum(1)     return bm def f_branin_2d(x, a=1, b=5.1/(4*np.pi**2), c=5/np.pi, r=6, s=10, t=1/(8*np.pi)): # https://www.sfu.ca/~ssurjano/branin.html     assert x.size(1)==2     x1,x2 = 15*x[:,0]-5,15*x[:,1]     y = a*(x2-b*x1**2+c*x1-r)**2+s*(1-t)*torch.cos(x1)+s     return y def f_camel_six_hump_2d(x): # https://www.indusmic.com/post/six-hump-camel-function     assert x.size(1)==2     x1,x2 = 6*x[:,0]-3,4*x[:,1]-2     y = (4-2.1*x1**2+x1**4/3)*x1**2+x1*x2+(-4+4*x2**2)*x2**2     return y  def styblinski_tang(x): # https://www.sfu.ca/~ssurjano/stybtang.html     x = 10*x-5     y = 1/2*torch.sum(x**4-16*x**2+5*x,1)     return y def hartmann_6d(x):     assert x.size(1)==6      alpha = torch.tensor([1.0, 1.2, 3.0, 3.2])     A = torch.tensor([         [10,   3,   17,   3.5, 1.7, 8],         [0.05, 10,  17,   0.1, 8,   14],         [3,    3.5, 1.7,  10,  17,  8],         [17,   8,   0.05, 10,  0.1,  14]])     P = 10**(-4)*torch.tensor([         [1312, 1696, 5569, 124, 8283, 5886],         [2329, 4135, 8307, 3736, 1004, 9991],         [2348, 1451, 3522, 2883, 3047, 6650],         [4047, 8828, 8732, 5743, 1091, 381]])     y = -1/1.94*(2.58+torch.sum(alpha*torch.exp(-torch.einsum(\"ij,rij-&gt;ri\",A,(x[:,None,:]-P)**2)),1))     return y def welch(x):     assert x.size(1)==20     x1,x2,x3,x4,x5,x6,x7,x8,x9,x10,x11,x12,x13,x14,x15,x16,x17,x18,x19,x20 = x[:,0],x[:,1],x[:,2],x[:,3],x[:,4],x[:,5],x[:,6],x[:,7],x[:,8],x[:,9],x[:,10],x[:,11],x[:,12],x[:,13],x[:,14],x[:,15],x[:,16],x[:,17],x[:,18],x[:,19]     y = 5*x12/(1+x1)+5*(x4-x20)**2+x5+40*x19**3-5*x19+0.05*x2+0.08*x3-0.03*x6+0.03*x7-0.09*x9-0.01*x10-0.07*x11+0.25*x13**2-0.04*x14+0.06*x15-0.01*x17-0.03*x18     return y In\u00a0[7]: Copied! <pre>n = 2**6\nd = 3\nngrid = np.round(n**(1/d))\nassert ngrid**d==n\ngridmesh = torch.meshgrid(*(torch.linspace(0,1,int(ngrid)+2)[1:-1] for j in range(d)),indexing=\"ij\")\ngrid = torch.vstack([gridmesh[j].flatten() for j in range(d)]).T \nx_lattice = qp.Lattice(d,order=\"LINEAR\",seed=7)(n)\ndnb2 = qp.DigitalNetB2(d,seed=7,graycode=False,t=32)\nx_dnb2 = dnb2(n)\nfig,ax = pyplot.subplots(nrows=3,ncols=3,sharey=True,sharex=True,figsize=(240/72,240/72))\nS = 5\nfor j,(name,x) in enumerate(zip([\"grid\",\"lattice\",\"digital net\"],[grid,x_lattice,x_dnb2])):\n    ax[0,j].set_title(name)#,fontsize=\"xx-large\")\n    ax[0,j].scatter(x[:,0],x[:,1],s=S,color=COLORS[j])\n    ax[1,j].scatter(x[:,0],x[:,2],s=S,color=COLORS[j])\n    ax[2,j].scatter(x[:,1],x[:,2],s=S,color=COLORS[j])\n    ax[0,j].set_xlabel(r\"$x_1$\")#,fontsize=\"xx-large\")\n    ax[1,j].set_xlabel(r\"$x_1$\")#,fontsize=\"xx-large\")\n    ax[2,j].set_xlabel(r\"$x_2$\")#,fontsize=\"xx-large\")\n    for i in range(3):\n        ax[i,j].set_xlim([0,1]); ax[i,j].set_ylim([0,1])\n        ax[i,j].set_xticks([0,1]); ax[i,j].set_yticks([0,1])\n        # ax[i,j].set_xticks([0,1/4,1/2,3/4,1])\n        # ax[i,j].set_xticklabels([r\"$0$\",r\"$\\frac{1}{4}$\",r\"$\\frac{1}{2}$\",r\"$\\frac{3}{4}$\",r\"$1$\"])\n        # ax[i,j].set_yticks([0,1/4,1/2,3/4,1])\n        # ax[i,j].set_yticklabels([r\"$0$\",r\"$1/4$\",r\"$1/2$\",r\"$3/4$\",r\"$1$\"])\n        ax[i,j].set_aspect(1)\nax[0,0].set_ylabel(r\"$x_2$\")#,fontsize=\"xx-large\")\nax[1,0].set_ylabel(r\"$x_3$\")#,fontsize=\"xx-large\")\nax[2,0].set_ylabel(r\"$x_3$\")#,fontsize=\"xx-large\")\n#fig.tight_layout()\nif FULLRUN:\n    fig.savefig(\"points.pdf\")#,dpi=256,bbox_inches=\"tight\")\n</pre> n = 2**6 d = 3 ngrid = np.round(n**(1/d)) assert ngrid**d==n gridmesh = torch.meshgrid(*(torch.linspace(0,1,int(ngrid)+2)[1:-1] for j in range(d)),indexing=\"ij\") grid = torch.vstack([gridmesh[j].flatten() for j in range(d)]).T  x_lattice = qp.Lattice(d,order=\"LINEAR\",seed=7)(n) dnb2 = qp.DigitalNetB2(d,seed=7,graycode=False,t=32) x_dnb2 = dnb2(n) fig,ax = pyplot.subplots(nrows=3,ncols=3,sharey=True,sharex=True,figsize=(240/72,240/72)) S = 5 for j,(name,x) in enumerate(zip([\"grid\",\"lattice\",\"digital net\"],[grid,x_lattice,x_dnb2])):     ax[0,j].set_title(name)#,fontsize=\"xx-large\")     ax[0,j].scatter(x[:,0],x[:,1],s=S,color=COLORS[j])     ax[1,j].scatter(x[:,0],x[:,2],s=S,color=COLORS[j])     ax[2,j].scatter(x[:,1],x[:,2],s=S,color=COLORS[j])     ax[0,j].set_xlabel(r\"$x_1$\")#,fontsize=\"xx-large\")     ax[1,j].set_xlabel(r\"$x_1$\")#,fontsize=\"xx-large\")     ax[2,j].set_xlabel(r\"$x_2$\")#,fontsize=\"xx-large\")     for i in range(3):         ax[i,j].set_xlim([0,1]); ax[i,j].set_ylim([0,1])         ax[i,j].set_xticks([0,1]); ax[i,j].set_yticks([0,1])         # ax[i,j].set_xticks([0,1/4,1/2,3/4,1])         # ax[i,j].set_xticklabels([r\"$0$\",r\"$\\frac{1}{4}$\",r\"$\\frac{1}{2}$\",r\"$\\frac{3}{4}$\",r\"$1$\"])         # ax[i,j].set_yticks([0,1/4,1/2,3/4,1])         # ax[i,j].set_yticklabels([r\"$0$\",r\"$1/4$\",r\"$1/2$\",r\"$3/4$\",r\"$1$\"])         ax[i,j].set_aspect(1) ax[0,0].set_ylabel(r\"$x_2$\")#,fontsize=\"xx-large\") ax[1,0].set_ylabel(r\"$x_3$\")#,fontsize=\"xx-large\") ax[2,0].set_ylabel(r\"$x_3$\")#,fontsize=\"xx-large\") #fig.tight_layout() if FULLRUN:     fig.savefig(\"points.pdf\")#,dpi=256,bbox_inches=\"tight\") In\u00a0[\u00a0]: Copied! <pre>n = 2**4\nd = 2\nngrid = np.round(n**(1/d))\nassert ngrid**d==n\ngridmesh = torch.meshgrid(*(torch.linspace(0,1,int(ngrid)) for j in range(d)),indexing=\"ij\")\ngrid = torch.vstack([gridmesh[j].flatten() for j in range(d)]).T \nx_lattice = torch.from_numpy(qp.Lattice(d,order=\"LINEAR\",seed=7)(n))\nx_dnb2 = torch.from_numpy(qp.DigitalNetB2(d,seed=7,graycode=False,t=32)(n))\ni0mesh,i1mesh = torch.meshgrid(torch.arange(n),torch.arange(n),indexing=\"ij\")\nlbetas_grad = [torch.zeros((1,d),dtype=int)]+[ej for ej in torch.eye(d,dtype=int)]\nkernel_gaussian = qp.KernelSquaredExponential(d,torchify=True)\nkmat_gauss = kernel_gaussian(grid[:,None,:],grid[None,:,:]).detach()\nkmat_gauss_grad = torch.cat([torch.cat([kernel_gaussian(grid[:,None,:],grid[None,:,:],lbetas_grad[i0],lbetas_grad[i1]) for i1 in range(len(lbetas_grad))],-1) for i0 in range(len(lbetas_grad))],-2).detach()\nkernel_si = qp.KernelShiftInvar(d,alpha=4,lengthscales=2.5e-1,torchify=True)\nkmat_lat = kernel_si(x_lattice[:,None,:],x_lattice[None,:,:]).detach()\nkmat_lat_grad = torch.cat([torch.cat([kernel_si(x_lattice[:,None,:],x_lattice[None,:,:],lbetas_grad[i0],lbetas_grad[i1]) for i1 in range(len(lbetas_grad))],-1) for i0 in range(len(lbetas_grad))],-2).detach()\nkernel_dsi = qp.KernelDigShiftInvar(d,t=32,alpha=4,lengthscales=2.5e-1,torchify=True)\nkmat_net = kernel_dsi(x_dnb2[:,None,:],x_dnb2[None,:,:]).detach()\nkmat_net_grad = torch.cat([torch.cat([kernel_dsi(x_dnb2[:,None,:],x_dnb2[None,:,:],lbetas_grad[i0],lbetas_grad[i1]) for i1 in range(len(lbetas_grad))],-1) for i0 in range(len(lbetas_grad))],-2).detach()\nfig,ax = pyplot.subplots(nrows=3,ncols=2,figsize=(MW1,MW1/2*3))\nCMAP = \"gnuplot2\"\nfor i,(name,kmat) in enumerate(zip([\"SE grid\",\"SI lattice\", \"DSI digital net\"],[kmat_gauss,kmat_lat,kmat_net])):\n    #print(name) \n    #print(kmat[:4,:4])\n    #print()\n    ax[i,0].imshow(kmat,cmap=CMAP)\n    ax[i,0].set_ylabel(name)#,fontsize=\"xx-large\")\nfor i,(name,kmat) in enumerate(zip([\"SE grid\",\"SI lattice\", \"DSI digital net\"],[kmat_gauss_grad,kmat_lat_grad,kmat_net_grad])):\n    #print(name) \n    #print(kmat[:4,:4])\n    #print()\n    ax[i,1].imshow(kmat,cmap=CMAP)\nax[0,0].set_title(r\"Gram matrix with $f$\")\nax[0,1].set_title(r\"Gram matrix with $(f,\\nabla f)$\")\nfor i in range(3):\n    for j in range(2):\n        #ax[i,j].axis(\"off\")\n        ax[i,j].set_aspect(1)\n        for spine in [\"top\",\"right\",\"bottom\",\"left\"]:\n            ax[i,j].spines[spine].set_visible(False)\n        ax[i,j].set_yticks([]); ax[i,j].set_xticks([])\n        #ax[i,j].get_yaxis().set_visible(False)\nif FULLRUN:\n    fig.savefig(\"gram_matrix_structures.pdf\")#,dpi=256,bbox_inches=\"tight\")\n</pre> n = 2**4 d = 2 ngrid = np.round(n**(1/d)) assert ngrid**d==n gridmesh = torch.meshgrid(*(torch.linspace(0,1,int(ngrid)) for j in range(d)),indexing=\"ij\") grid = torch.vstack([gridmesh[j].flatten() for j in range(d)]).T  x_lattice = torch.from_numpy(qp.Lattice(d,order=\"LINEAR\",seed=7)(n)) x_dnb2 = torch.from_numpy(qp.DigitalNetB2(d,seed=7,graycode=False,t=32)(n)) i0mesh,i1mesh = torch.meshgrid(torch.arange(n),torch.arange(n),indexing=\"ij\") lbetas_grad = [torch.zeros((1,d),dtype=int)]+[ej for ej in torch.eye(d,dtype=int)] kernel_gaussian = qp.KernelSquaredExponential(d,torchify=True) kmat_gauss = kernel_gaussian(grid[:,None,:],grid[None,:,:]).detach() kmat_gauss_grad = torch.cat([torch.cat([kernel_gaussian(grid[:,None,:],grid[None,:,:],lbetas_grad[i0],lbetas_grad[i1]) for i1 in range(len(lbetas_grad))],-1) for i0 in range(len(lbetas_grad))],-2).detach() kernel_si = qp.KernelShiftInvar(d,alpha=4,lengthscales=2.5e-1,torchify=True) kmat_lat = kernel_si(x_lattice[:,None,:],x_lattice[None,:,:]).detach() kmat_lat_grad = torch.cat([torch.cat([kernel_si(x_lattice[:,None,:],x_lattice[None,:,:],lbetas_grad[i0],lbetas_grad[i1]) for i1 in range(len(lbetas_grad))],-1) for i0 in range(len(lbetas_grad))],-2).detach() kernel_dsi = qp.KernelDigShiftInvar(d,t=32,alpha=4,lengthscales=2.5e-1,torchify=True) kmat_net = kernel_dsi(x_dnb2[:,None,:],x_dnb2[None,:,:]).detach() kmat_net_grad = torch.cat([torch.cat([kernel_dsi(x_dnb2[:,None,:],x_dnb2[None,:,:],lbetas_grad[i0],lbetas_grad[i1]) for i1 in range(len(lbetas_grad))],-1) for i0 in range(len(lbetas_grad))],-2).detach() fig,ax = pyplot.subplots(nrows=3,ncols=2,figsize=(MW1,MW1/2*3)) CMAP = \"gnuplot2\" for i,(name,kmat) in enumerate(zip([\"SE grid\",\"SI lattice\", \"DSI digital net\"],[kmat_gauss,kmat_lat,kmat_net])):     #print(name)      #print(kmat[:4,:4])     #print()     ax[i,0].imshow(kmat,cmap=CMAP)     ax[i,0].set_ylabel(name)#,fontsize=\"xx-large\") for i,(name,kmat) in enumerate(zip([\"SE grid\",\"SI lattice\", \"DSI digital net\"],[kmat_gauss_grad,kmat_lat_grad,kmat_net_grad])):     #print(name)      #print(kmat[:4,:4])     #print()     ax[i,1].imshow(kmat,cmap=CMAP) ax[0,0].set_title(r\"Gram matrix with $f$\") ax[0,1].set_title(r\"Gram matrix with $(f,\\nabla f)$\") for i in range(3):     for j in range(2):         #ax[i,j].axis(\"off\")         ax[i,j].set_aspect(1)         for spine in [\"top\",\"right\",\"bottom\",\"left\"]:             ax[i,j].spines[spine].set_visible(False)         ax[i,j].set_yticks([]); ax[i,j].set_xticks([])         #ax[i,j].get_yaxis().set_visible(False) if FULLRUN:     fig.savefig(\"gram_matrix_structures.pdf\")#,dpi=256,bbox_inches=\"tight\") In\u00a0[\u00a0]: Copied! <pre>LW = None \nndraws = 3\nn = 127\nt = 32\nxticks = torch.linspace(0,1,n+2)[1:-1]\nxbticks = torch.floor(xticks*2**t).to(torch.int64)\nkmats = [\n    [\"SE\", qp.KernelSquaredExponential(1,lengthscales=1e-2,scale=2,torchify=True)(xticks[:,None,None],xticks[None,:,None]).detach()+1e-6*torch.eye(n)],\n    [r\"SI $\\alpha=2$\", qp.KernelShiftInvar(1,lengthscales=1e0,alpha=2,torchify=True)(xticks[:,None,None],xticks[None,:,None]).detach()+1e-6*torch.eye(n)],\n    [r\"DSI $\\alpha = 4$\", qp.KernelDigShiftInvar(1,lengthscales=1e0,t=t,alpha=4,torchify=True)(xticks[:,None,None],xticks[None,:,None]).detach()+1e-6*torch.eye(n)],\n]\nncols = len(kmats)\nfig,ax = pyplot.subplots(nrows=1,ncols=ncols,figsize=(MW1,MW1/3),sharex=True,sharey=True)\nfor i,(name,kmat) in enumerate(kmats):\n    L = torch.linalg.cholesky(kmat)\n    r = torch.randn(ndraws,n,generator=torch.Generator().manual_seed(5))\n    draws = torch.einsum(\"ik,rk-&gt;ri\",L,r)\n    for j in range(ndraws):\n        ax[i].plot(xticks,draws[j],color=COLORS[j],linewidth=LW)\n        ax[i].set_title(name)#,fontsize=\"xx-large\")\n        ax[i].set_xlim([0,1])\n        ax[i].set_xticks([0,1])\n        #ax[i].set_xticks([0,1/4,1/2,3/4,1])\n        #ax[i].set_xticklabels([r\"$0$\",r\"$1/4$\",r\"$1/2$\",r\"$3/4$\",r\"$1$\"])\nif FULLRUN:\n    fig.savefig(\"draws.pdf\")#,dpi=256,bbox_inches=\"tight\")\n</pre> LW = None  ndraws = 3 n = 127 t = 32 xticks = torch.linspace(0,1,n+2)[1:-1] xbticks = torch.floor(xticks*2**t).to(torch.int64) kmats = [     [\"SE\", qp.KernelSquaredExponential(1,lengthscales=1e-2,scale=2,torchify=True)(xticks[:,None,None],xticks[None,:,None]).detach()+1e-6*torch.eye(n)],     [r\"SI $\\alpha=2$\", qp.KernelShiftInvar(1,lengthscales=1e0,alpha=2,torchify=True)(xticks[:,None,None],xticks[None,:,None]).detach()+1e-6*torch.eye(n)],     [r\"DSI $\\alpha = 4$\", qp.KernelDigShiftInvar(1,lengthscales=1e0,t=t,alpha=4,torchify=True)(xticks[:,None,None],xticks[None,:,None]).detach()+1e-6*torch.eye(n)], ] ncols = len(kmats) fig,ax = pyplot.subplots(nrows=1,ncols=ncols,figsize=(MW1,MW1/3),sharex=True,sharey=True) for i,(name,kmat) in enumerate(kmats):     L = torch.linalg.cholesky(kmat)     r = torch.randn(ndraws,n,generator=torch.Generator().manual_seed(5))     draws = torch.einsum(\"ik,rk-&gt;ri\",L,r)     for j in range(ndraws):         ax[i].plot(xticks,draws[j],color=COLORS[j],linewidth=LW)         ax[i].set_title(name)#,fontsize=\"xx-large\")         ax[i].set_xlim([0,1])         ax[i].set_xticks([0,1])         #ax[i].set_xticks([0,1/4,1/2,3/4,1])         #ax[i].set_xticklabels([r\"$0$\",r\"$1/4$\",r\"$1/2$\",r\"$3/4$\",r\"$1$\"]) if FULLRUN:     fig.savefig(\"draws.pdf\")#,dpi=256,bbox_inches=\"tight\") In\u00a0[11]: Copied! <pre>funcs = [\n    (\"Ackley\",1,f_ackley,False,False),\n    (\"Branin\",2,f_branin_2d,False,True),\n    (\"Camel\",2,f_camel_six_hump_2d,False,False),\n    (\"Styblinski\",2,styblinski_tang,False,False),\n    (\"Hartmann\",6,hartmann_6d,False,True),\n    #(\"Welch\",20,welch,False,False),\n    ]\nn = 2**10 if FULLRUN else 2**5\ngps = [\n    [\"SE lattice\", lambda d,lbetas: fastgps.StandardGP(qp.KernelMultiTaskDerivs(qp.KernelGaussian(d,torchify=True),num_tasks=len(lbetas)),[qp.Lattice(d,seed=7) for i in range(len(lbetas))],derivatives=lbetas)],\n    [\"SI lattice\", lambda d,lbetas: fastgps.FastGPLattice(qp.KernelMultiTaskDerivs(qp.KernelShiftInvar(d,torchify=True,alpha=2),num_tasks=len(lbetas)),[qp.Lattice(d,seed=7) for i in range(len(lbetas))],derivatives=lbetas)],\n    [\"DSI digital net\", lambda d,lbetas: fastgps.FastGPDigitalNetB2(qp.KernelMultiTaskDerivs(qp.KernelDigShiftInvar(d,torchify=True,alpha=4,lengthscales=1e-2),num_tasks=len(lbetas)),[qp.DigitalNetB2(d,seed=7,randomize=\"DS\") for i in range(len(lbetas))],derivatives=lbetas)],\n]\nverbose_indent = 12\nopt_steps = 15\nverbose = 20#max(1,opt_steps//10)\nl2rerrors_no_grad = torch.nan*torch.ones((len(funcs),len(gps)))\nl2rerrors_grad = torch.nan*torch.ones((len(funcs),len(gps)))\nmll_no_grad = torch.nan*torch.ones((len(funcs),len(gps)))\nmll_grad = torch.nan*torch.ones((len(funcs),len(gps)))\ntimes_no_grad = torch.nan*torch.ones((len(funcs),len(gps)))\ntimes_grad = torch.nan*torch.ones((len(funcs),len(gps)))\ndatas = {}\ndatas_grad = {}\nfor i,(fname,d,f,bake,bake_grad) in enumerate(funcs):\n    print(fname)\n    xquery = torch.from_numpy(qp.Halton(d,seed=17).gen_samples(2**12))\n    yquery = f(xquery)\n    for j,(name,gp_constructor) in enumerate(gps):\n        print(\"    %s\"%name)\n        print(\"        nograd \")\n        lbetas = [torch.zeros((1,d),dtype=int)]\n        gp = gp_constructor(d,lbetas)\n        x_next = gp.get_x_next(n)\n        assert all((x_next_i==x_next[0]).all() for x_next_i in x_next)\n        f_tf = (lambda x: f(1-2*torch.abs(x-1/2))) if isinstance(gp,fastgps.FastGPLattice) and bake else f # possible Baker transform\n        gp.add_y_next([f(x_next[0])])\n        t0 = time.perf_counter()\n        data = gp.fit(\n            #iterations = opt_steps,\n            verbose_indent = verbose_indent,\n            verbose = verbose,\n            #stop_crit_improvement_threshold = 1e-3,\n            #stop_crit_wait_iterations = opt_steps,\n            store_hists = True,\n        )\n        times_no_grad[i,j] = (time.perf_counter()-t0)/(data[\"iteration\"][-1])\n        yhat = (gp.post_mean(xquery/2)+gp.post_mean(1-xquery/2))/2  if isinstance(gp,fastgps.FastGPLattice) and bake else gp.post_mean(xquery) # inverse Baker transform\n        l2rerrors_no_grad[i,j] = torch.linalg.norm(yhat-yquery)/torch.linalg.norm(yquery)\n        mll_no_grad[i,j] = data[\"loss_hist\"].max()\n        datas[fname+\"_\"+name] = data\n        print(\"            L2 relative error = %.0e\\tmll = %.0e\"%(l2rerrors_no_grad[i,j],mll_no_grad[i,j]))\n        print(\"        grad \")\n        lbetas = [torch.zeros((1,d),dtype=int)]+[ej for ej in torch.eye(d,dtype=int)]\n        gp = gp_constructor(d,lbetas)\n        x_next = gp.get_x_next(n*torch.ones(len(lbetas),dtype=int))\n        #assert all((x_next_i==x_next[0]).all() for x_next_i in x_next)\n        f_tf = (lambda x: f(1-2*torch.abs(x-1/2))) if isinstance(gp,fastgps.FastGPLattice) and bake_grad else f # possible Baker transform\n        gp.add_y_next([f_grad_f(f_tf,x_next[i])[:,i] for i in range(len(lbetas))])\n        t0 = time.perf_counter()\n        data = gp.fit(\n            #iterations = opt_steps,\n            verbose_indent = verbose_indent,\n            verbose = verbose,\n            #stop_crit_improvement_threshold = 1e-3,\n            #stop_crit_wait_iterations = opt_steps,\n            store_hists = True\n        )\n        times_grad[i,j] = (time.perf_counter()-t0)/(data[\"iteration\"][-1])\n        yhat = (gp.post_mean(xquery/2,task=0)+gp.post_mean(1-xquery/2,task=0))/2 if isinstance(gp,fastgps.FastGPLattice) and bake_grad else gp.post_mean(xquery,task=0) # inverse Baker transform\n        l2rerrors_grad[i,j] = torch.linalg.norm(yhat-yquery)/torch.linalg.norm(yquery)\n        mll_grad[i,j] = data[\"loss_hist\"].max()\n        datas_grad[fname+\"_\"+name] = data\n        print(\"            L2 relative error = %.0e\\t\\tmll = %.0e\"%(l2rerrors_grad[i,j],mll_grad[i,j]))\n    print()\n</pre> funcs = [     (\"Ackley\",1,f_ackley,False,False),     (\"Branin\",2,f_branin_2d,False,True),     (\"Camel\",2,f_camel_six_hump_2d,False,False),     (\"Styblinski\",2,styblinski_tang,False,False),     (\"Hartmann\",6,hartmann_6d,False,True),     #(\"Welch\",20,welch,False,False),     ] n = 2**10 if FULLRUN else 2**5 gps = [     [\"SE lattice\", lambda d,lbetas: fastgps.StandardGP(qp.KernelMultiTaskDerivs(qp.KernelGaussian(d,torchify=True),num_tasks=len(lbetas)),[qp.Lattice(d,seed=7) for i in range(len(lbetas))],derivatives=lbetas)],     [\"SI lattice\", lambda d,lbetas: fastgps.FastGPLattice(qp.KernelMultiTaskDerivs(qp.KernelShiftInvar(d,torchify=True,alpha=2),num_tasks=len(lbetas)),[qp.Lattice(d,seed=7) for i in range(len(lbetas))],derivatives=lbetas)],     [\"DSI digital net\", lambda d,lbetas: fastgps.FastGPDigitalNetB2(qp.KernelMultiTaskDerivs(qp.KernelDigShiftInvar(d,torchify=True,alpha=4,lengthscales=1e-2),num_tasks=len(lbetas)),[qp.DigitalNetB2(d,seed=7,randomize=\"DS\") for i in range(len(lbetas))],derivatives=lbetas)], ] verbose_indent = 12 opt_steps = 15 verbose = 20#max(1,opt_steps//10) l2rerrors_no_grad = torch.nan*torch.ones((len(funcs),len(gps))) l2rerrors_grad = torch.nan*torch.ones((len(funcs),len(gps))) mll_no_grad = torch.nan*torch.ones((len(funcs),len(gps))) mll_grad = torch.nan*torch.ones((len(funcs),len(gps))) times_no_grad = torch.nan*torch.ones((len(funcs),len(gps))) times_grad = torch.nan*torch.ones((len(funcs),len(gps))) datas = {} datas_grad = {} for i,(fname,d,f,bake,bake_grad) in enumerate(funcs):     print(fname)     xquery = torch.from_numpy(qp.Halton(d,seed=17).gen_samples(2**12))     yquery = f(xquery)     for j,(name,gp_constructor) in enumerate(gps):         print(\"    %s\"%name)         print(\"        nograd \")         lbetas = [torch.zeros((1,d),dtype=int)]         gp = gp_constructor(d,lbetas)         x_next = gp.get_x_next(n)         assert all((x_next_i==x_next[0]).all() for x_next_i in x_next)         f_tf = (lambda x: f(1-2*torch.abs(x-1/2))) if isinstance(gp,fastgps.FastGPLattice) and bake else f # possible Baker transform         gp.add_y_next([f(x_next[0])])         t0 = time.perf_counter()         data = gp.fit(             #iterations = opt_steps,             verbose_indent = verbose_indent,             verbose = verbose,             #stop_crit_improvement_threshold = 1e-3,             #stop_crit_wait_iterations = opt_steps,             store_hists = True,         )         times_no_grad[i,j] = (time.perf_counter()-t0)/(data[\"iteration\"][-1])         yhat = (gp.post_mean(xquery/2)+gp.post_mean(1-xquery/2))/2  if isinstance(gp,fastgps.FastGPLattice) and bake else gp.post_mean(xquery) # inverse Baker transform         l2rerrors_no_grad[i,j] = torch.linalg.norm(yhat-yquery)/torch.linalg.norm(yquery)         mll_no_grad[i,j] = data[\"loss_hist\"].max()         datas[fname+\"_\"+name] = data         print(\"            L2 relative error = %.0e\\tmll = %.0e\"%(l2rerrors_no_grad[i,j],mll_no_grad[i,j]))         print(\"        grad \")         lbetas = [torch.zeros((1,d),dtype=int)]+[ej for ej in torch.eye(d,dtype=int)]         gp = gp_constructor(d,lbetas)         x_next = gp.get_x_next(n*torch.ones(len(lbetas),dtype=int))         #assert all((x_next_i==x_next[0]).all() for x_next_i in x_next)         f_tf = (lambda x: f(1-2*torch.abs(x-1/2))) if isinstance(gp,fastgps.FastGPLattice) and bake_grad else f # possible Baker transform         gp.add_y_next([f_grad_f(f_tf,x_next[i])[:,i] for i in range(len(lbetas))])         t0 = time.perf_counter()         data = gp.fit(             #iterations = opt_steps,             verbose_indent = verbose_indent,             verbose = verbose,             #stop_crit_improvement_threshold = 1e-3,             #stop_crit_wait_iterations = opt_steps,             store_hists = True         )         times_grad[i,j] = (time.perf_counter()-t0)/(data[\"iteration\"][-1])         yhat = (gp.post_mean(xquery/2,task=0)+gp.post_mean(1-xquery/2,task=0))/2 if isinstance(gp,fastgps.FastGPLattice) and bake_grad else gp.post_mean(xquery,task=0) # inverse Baker transform         l2rerrors_grad[i,j] = torch.linalg.norm(yhat-yquery)/torch.linalg.norm(yquery)         mll_grad[i,j] = data[\"loss_hist\"].max()         datas_grad[fname+\"_\"+name] = data         print(\"            L2 relative error = %.0e\\t\\tmll = %.0e\"%(l2rerrors_grad[i,j],mll_grad[i,j]))     print() <pre>Ackley\n    SE lattice\n        nograd \n             iter of 5.0e+03 | best loss  | loss       | term1      | term2     \n            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n                    0.00e+00 | 4.21e+07   | 4.21e+07   | 8.42e+07   | -9.38e+03 \n                    2.00e+01 | -8.37e+02  | -8.37e+02  | 5.56e+02   | -4.11e+03 \n                    4.00e+01 | -8.76e+02  | -8.76e+02  | 7.80e+02   | -4.41e+03 \n                    5.10e+01 | -8.76e+02  | -8.76e+02  | 7.75e+02   | -4.41e+03 \n            L2 relative error = 3e-04\tmll = 4e+07\n        grad \n             iter of 5.0e+03 | best loss  | loss       | term1      | term2     \n            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n                    0.00e+00 | 7.18e+11   | 7.18e+11   | 1.44e+12   | -1.88e+04 \n                    2.00e+01 | 4.31e+03   | 4.32e+03   | 1.86e+03   | 3.01e+03  \n                    4.00e+01 | 4.05e+03   | 4.05e+03   | 3.18e+03   | 1.16e+03  \n                    4.80e+01 | 4.05e+03   | 4.05e+03   | 3.21e+03   | 1.13e+03  \n            L2 relative error = 3e-04\t\tmll = 7e+11\n    SI lattice\n        nograd \n             iter of 5.0e+03 | best loss  | loss       | term1      | term2     \n            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n                    0.00e+00 | 1.26e+07   | 1.26e+07   | 2.52e+07   | -1.43e+04 \n                    2.00e+01 | -5.01e+02  | -5.01e+02  | 1.01e+03   | -3.89e+03 \n                    3.80e+01 | -5.01e+02  | -5.01e+02  | 1.03e+03   | -3.92e+03 \n            L2 relative error = 1e-04\tmll = 1e+07\n        grad \n             iter of 5.0e+03 | best loss  | loss       | term1      | term2     \n            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n                    0.00e+00 | 1.32e+07   | 1.32e+07   | 2.64e+07   | -1.59e+04 \n                    2.00e+01 | 4.65e+03   | 4.65e+03   | 1.94e+03   | 3.61e+03  \n                    3.60e+01 | 4.65e+03   | 4.65e+03   | 2.06e+03   | 3.48e+03  \n            L2 relative error = 9e-05\t\tmll = 1e+07\n    DSI digital net\n        nograd \n             iter of 5.0e+03 | best loss  | loss       | term1      | term2     \n            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n                    0.00e+00 | 2.28e+09   | 2.28e+09   | 4.56e+09   | -1.49e+04 \n                    2.00e+01 | 1.92e+03   | 2.42e+03   | 3.43e+03   | -4.71e+02 \n                    4.00e+01 | 1.84e+03   | 1.84e+03   | 1.02e+03   | 7.72e+02  \n                    5.00e+01 | 1.84e+03   | 1.84e+03   | 1.02e+03   | 7.69e+02  \n            L2 relative error = 2e-02\tmll = 2e+09\n        grad \n             iter of 5.0e+03 | best loss  | loss       | term1      | term2     \n            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n                    0.00e+00 | 4.09e+22   | 4.09e+22   | 8.18e+22   | -4.90e+04 \n                    1.60e+01 | -1.86e+23  | nan        | nan        | -inf      \n            L2 relative error = 1e+04\t\tmll = nan\n\nBranin\n    SE lattice\n        nograd \n             iter of 5.0e+03 | best loss  | loss       | term1      | term2     \n            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n                    0.00e+00 | 2.08e+08   | 2.08e+08   | 4.16e+08   | -9.31e+03 \n                    2.00e+01 | -3.13e+03  | -3.13e+03  | 3.68e+02   | -8.51e+03 \n                    4.00e+01 | -3.42e+03  | -3.42e+03  | 4.80e+01   | -8.77e+03 \n                    6.00e+01 | -3.43e+03  | -3.43e+03  | 5.67e+01   | -8.80e+03 \n                    6.30e+01 | -3.43e+03  | -3.43e+03  | 5.42e+01   | -8.79e+03 \n            L2 relative error = 1e-03\tmll = 2e+08\n        grad \n             iter of 5.0e+03 | best loss  | loss       | term1      | term2     \n            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n                    0.00e+00 | 3.80e+10   | 3.80e+10   | 7.61e+10   | -2.81e+04 \n                    2.00e+01 | -8.94e+03  | -8.94e+03  | 4.95e+02   | -2.40e+04 \n                    4.00e+01 | -1.23e+04  | -1.23e+04  | 2.94e+02   | -3.05e+04 \n                    5.20e+01 | -1.23e+04  | -1.23e+04  | 5.57e+02   | -3.08e+04 \n            L2 relative error = 3e-02\t\tmll = 4e+10\n    SI lattice\n        nograd \n             iter of 5.0e+03 | best loss  | loss       | term1      | term2     \n            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n                    0.00e+00 | 1.23e+08   | 1.23e+08   | 2.47e+08   | -6.04e+03 \n                    2.00e+01 | 5.07e+03   | 5.07e+03   | 1.28e+03   | 6.98e+03  \n                    4.00e+01 | 4.91e+03   | 4.91e+03   | 1.01e+03   | 6.94e+03  \n                    6.00e+01 | 4.90e+03   | 4.90e+03   | 1.03e+03   | 6.89e+03  \n                    8.00e+01 | 4.84e+03   | 4.84e+03   | 1.03e+03   | 6.77e+03  \n                    1.00e+02 | 4.78e+03   | 4.78e+03   | 9.96e+02   | 6.69e+03  \n                    1.20e+02 | 4.78e+03   | 4.78e+03   | 1.03e+03   | 6.64e+03  \n                    1.40e+02 | 4.78e+03   | 4.78e+03   | 1.02e+03   | 6.65e+03  \n                    1.60e+02 | 4.78e+03   | 4.78e+03   | 1.02e+03   | 6.65e+03  \n            L2 relative error = 2e-01\tmll = 1e+08\n        grad \n             iter of 5.0e+03 | best loss  | loss       | term1      | term2     \n            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n                    0.00e+00 | 2.65e+06   | 2.65e+06   | 5.30e+06   | -2.52e+03 \n                    2.00e+01 | 1.52e+04   | 1.52e+04   | 4.45e+03   | 2.03e+04  \n                    4.00e+01 | 1.37e+04   | 1.37e+04   | 3.17e+03   | 1.85e+04  \n                    6.00e+01 | 1.36e+04   | 1.36e+04   | 3.06e+03   | 1.85e+04  \n                    8.00e+01 | 1.36e+04   | 1.36e+04   | 3.07e+03   | 1.85e+04  \n                    9.60e+01 | 1.36e+04   | 1.36e+04   | 3.07e+03   | 1.85e+04  \n            L2 relative error = 2e-03\t\tmll = 3e+06\n    DSI digital net\n        nograd \n             iter of 5.0e+03 | best loss  | loss       | term1      | term2     \n            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n                    0.00e+00 | 1.26e+08   | 1.26e+08   | 2.51e+08   | -9.42e+03 \n                    2.00e+01 | 2.84e+03   | 2.84e+03   | 1.33e+03   | 2.47e+03  \n                    4.00e+01 | 2.78e+03   | 2.78e+03   | 9.62e+02   | 2.71e+03  \n                    6.00e+01 | 2.78e+03   | 2.78e+03   | 1.03e+03   | 2.65e+03  \n                    7.80e+01 | 2.78e+03   | 2.78e+03   | 1.02e+03   | 2.65e+03  \n            L2 relative error = 2e-02\tmll = 1e+08\n        grad \n             iter of 5.0e+03 | best loss  | loss       | term1      | term2     \n            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n                    0.00e+00 | 7.35e+15   | 7.35e+15   | 1.47e+16   | -3.77e+04 \n                    1.90e+01 | -2.71e+19  | -1.25e+19  | -2.50e+19  | -2.71e+04 \n            L2 relative error = 6e+04\t\tmll = 5e+20\n\nCamel\n    SE lattice\n        nograd \n             iter of 5.0e+03 | best loss  | loss       | term1      | term2     \n            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n                    0.00e+00 | 2.41e+08   | 2.41e+08   | 4.82e+08   | -9.31e+03 \n                    2.00e+01 | -2.63e+03  | -2.63e+03  | 6.85e+02   | -7.83e+03 \n                    4.00e+01 | -3.26e+03  | -3.25e+03  | 1.12e+02   | -8.50e+03 \n                    6.00e+01 | -3.27e+03  | -3.27e+03  | 1.30e+02   | -8.56e+03 \n                    7.60e+01 | -3.28e+03  | -3.28e+03  | 1.21e+02   | -8.55e+03 \n            L2 relative error = 7e-03\tmll = 2e+08\n        grad \n             iter of 5.0e+03 | best loss  | loss       | term1      | term2     \n            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n                    0.00e+00 | 5.06e+10   | 5.06e+10   | 1.01e+11   | -2.81e+04 \n                    2.00e+01 | -6.54e+03  | -6.54e+03  | 1.53e+03   | -2.03e+04 \n                    4.00e+01 | -1.01e+04  | 6.62e+04   | 4.75e-18   | 1.27e+05  \n            L2 relative error = 8e-01\t\tmll = 5e+10\n    SI lattice\n        nograd \n             iter of 5.0e+03 | best loss  | loss       | term1      | term2     \n            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n                    0.00e+00 | 5.83e+05   | 5.83e+05   | 1.17e+06   | -6.04e+03 \n                    2.00e+01 | 2.26e+03   | 2.26e+03   | 1.28e+03   | 1.34e+03  \n                    4.00e+01 | 1.92e+03   | 1.92e+03   | 8.70e+02   | 1.09e+03  \n                    6.00e+01 | 1.87e+03   | 1.87e+03   | 1.04e+03   | 8.14e+02  \n                    8.00e+01 | 1.85e+03   | 1.85e+03   | 1.03e+03   | 7.85e+02  \n                    1.00e+02 | 1.85e+03   | 1.85e+03   | 1.03e+03   | 7.84e+02  \n                    1.04e+02 | 1.85e+03   | 1.85e+03   | 1.03e+03   | 7.84e+02  \n            L2 relative error = 2e-02\tmll = 6e+05\n        grad \n             iter of 5.0e+03 | best loss  | loss       | term1      | term2     \n            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n                    0.00e+00 | 2.14e+06   | 2.14e+06   | 4.28e+06   | -2.52e+03 \n                    2.00e+01 | 1.46e+04   | 1.46e+04   | 4.00e+03   | 1.96e+04  \n                    4.00e+01 | 1.40e+04   | 1.40e+04   | 3.43e+03   | 1.90e+04  \n                    6.00e+01 | 1.36e+04   | 1.36e+04   | 3.19e+03   | 1.83e+04  \n                    8.00e+01 | 1.35e+04   | 1.35e+04   | 2.96e+03   | 1.85e+04  \n                    1.00e+02 | 1.35e+04   | 1.35e+04   | 3.12e+03   | 1.83e+04  \n                    1.20e+02 | 1.35e+04   | 1.35e+04   | 3.09e+03   | 1.83e+04  \n                    1.40e+02 | 1.35e+04   | 1.35e+04   | 3.07e+03   | 1.83e+04  \n                    1.58e+02 | 1.35e+04   | 1.35e+04   | 3.07e+03   | 1.83e+04  \n            L2 relative error = 1e-01\t\tmll = 2e+06\n    DSI digital net\n        nograd \n             iter of 5.0e+03 | best loss  | loss       | term1      | term2     \n            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n                    0.00e+00 | 1.08e+07   | 1.08e+07   | 2.15e+07   | -9.42e+03 \n                    2.00e+01 | 2.59e+03   | 2.59e+03   | 1.01e+03   | 2.29e+03  \n                    4.00e+01 | 1.18e+03   | 1.19e+03   | 9.49e+02   | -4.46e+02 \n                    6.00e+01 | 1.05e+03   | 1.05e+03   | 1.05e+03   | -8.44e+02 \n                    8.00e+01 | 1.02e+03   | 1.02e+03   | 1.00e+03   | -8.50e+02 \n                    9.40e+01 | 1.02e+03   | 1.02e+03   | 1.03e+03   | -8.73e+02 \n            L2 relative error = 4e-02\tmll = 1e+07\n        grad \n             iter of 5.0e+03 | best loss  | loss       | term1      | term2     \n            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n                    0.00e+00 | 1.20e+16   | 1.20e+16   | 2.40e+16   | -3.77e+04 \n                    2.00e+01 | -2.41e+17  | 2.94e+15   | 5.88e+15   | -3.26e+04 \n                    2.20e+01 | -2.41e+17  | 2.50e+15   | 5.00e+15   | -3.20e+04 \n            L2 relative error = 2e+04\t\tmll = 6e+17\n\nStyblinski\n    SE lattice\n        nograd \n             iter of 5.0e+03 | best loss  | loss       | term1      | term2     \n            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n                    0.00e+00 | 6.98e+08   | 6.98e+08   | 1.40e+09   | -9.31e+03 \n                    2.00e+01 | -2.67e+03  | -2.67e+03  | 3.68e+02   | -7.59e+03 \n                    4.00e+01 | -3.20e+03  | -3.20e+03  | 8.16e+01   | -8.36e+03 \n                    6.00e+01 | -3.32e+03  | -3.32e+03  | 4.68e+01   | -8.56e+03 \n                    7.20e+01 | -3.32e+03  | -3.32e+03  | 6.68e+01   | -8.59e+03 \n            L2 relative error = 1e+00\tmll = 7e+08\n        grad \n             iter of 5.0e+03 | best loss  | loss       | term1      | term2     \n            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n                    0.00e+00 | 3.23e+09   | 3.23e+09   | 6.46e+09   | -2.81e+04 \n                    2.00e+01 | -7.74e+03  | -7.74e+03  | 1.98e+03   | -2.31e+04 \n                    3.40e+01 | -1.02e+04  | -5.30e+03  | 5.54e-01   | -1.62e+04 \n            L2 relative error = 3e-01\t\tmll = 3e+09\n    SI lattice\n        nograd \n             iter of 5.0e+03 | best loss  | loss       | term1      | term2     \n            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n                    0.00e+00 | 5.48e+06   | 5.48e+06   | 1.10e+07   | -6.04e+03 \n                    2.00e+01 | 3.60e+03   | 3.60e+03   | 1.16e+03   | 4.17e+03  \n                    4.00e+01 | 3.06e+03   | 3.06e+03   | 1.11e+03   | 3.13e+03  \n                    6.00e+01 | 2.92e+03   | 2.92e+03   | 1.12e+03   | 2.85e+03  \n                    8.00e+01 | 2.87e+03   | 2.87e+03   | 1.05e+03   | 2.81e+03  \n                    1.00e+02 | 2.82e+03   | 2.82e+03   | 1.02e+03   | 2.75e+03  \n                    1.20e+02 | 2.77e+03   | 2.77e+03   | 1.05e+03   | 2.61e+03  \n                    1.40e+02 | 2.60e+03   | 2.73e+03   | 2.05e+03   | 1.54e+03  \n                    1.60e+02 | 2.55e+03   | 2.55e+03   | 1.11e+03   | 2.10e+03  \n                    1.80e+02 | 2.49e+03   | 2.49e+03   | 1.03e+03   | 2.06e+03  \n                    2.00e+02 | 2.45e+03   | 2.45e+03   | 1.01e+03   | 2.02e+03  \n                    2.20e+02 | 2.43e+03   | 2.43e+03   | 1.02e+03   | 1.96e+03  \n                    2.40e+02 | 2.41e+03   | 2.41e+03   | 1.05e+03   | 1.90e+03  \n                    2.60e+02 | 2.39e+03   | 2.39e+03   | 1.02e+03   | 1.89e+03  \n                    2.80e+02 | 2.38e+03   | 2.38e+03   | 1.03e+03   | 1.86e+03  \n                    3.00e+02 | 2.37e+03   | 2.37e+03   | 1.07e+03   | 1.80e+03  \n                    3.20e+02 | 2.36e+03   | 2.36e+03   | 1.05e+03   | 1.80e+03  \n                    3.40e+02 | 2.36e+03   | 2.36e+03   | 1.04e+03   | 1.80e+03  \n                    3.60e+02 | 2.36e+03   | 2.36e+03   | 1.05e+03   | 1.78e+03  \n                    3.80e+02 | 2.36e+03   | 2.36e+03   | 1.03e+03   | 1.80e+03  \n                    4.00e+02 | 2.35e+03   | 2.35e+03   | 1.03e+03   | 1.80e+03  \n                    4.20e+02 | 2.35e+03   | 2.35e+03   | 1.05e+03   | 1.78e+03  \n                    4.30e+02 | 2.35e+03   | 2.35e+03   | 1.04e+03   | 1.78e+03  \n            L2 relative error = 3e-02\tmll = 5e+06\n        grad \n             iter of 5.0e+03 | best loss  | loss       | term1      | term2     \n            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n                    0.00e+00 | 3.30e+07   | 3.30e+07   | 6.59e+07   | -2.52e+03 \n                    2.00e+01 | 1.88e+04   | 1.88e+04   | 3.73e+03   | 2.82e+04  \n                    4.00e+01 | 1.86e+04   | 1.86e+04   | 3.19e+03   | 2.84e+04  \n                    6.00e+01 | 1.84e+04   | 1.84e+04   | 2.70e+03   | 2.85e+04  \n                    8.00e+01 | 1.84e+04   | 1.84e+04   | 3.22e+03   | 2.78e+04  \n                    1.00e+02 | 1.83e+04   | 1.83e+04   | 3.08e+03   | 2.79e+04  \n                    1.20e+02 | 1.83e+04   | 1.83e+04   | 3.07e+03   | 2.79e+04  \n                    1.40e+02 | 1.83e+04   | 1.83e+04   | 3.08e+03   | 2.79e+04  \n                    1.56e+02 | 1.83e+04   | 1.83e+04   | 3.07e+03   | 2.79e+04  \n            L2 relative error = 3e-01\t\tmll = 3e+07\n    DSI digital net\n        nograd \n             iter of 5.0e+03 | best loss  | loss       | term1      | term2     \n            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n                    0.00e+00 | 2.08e+07   | 2.08e+07   | 4.16e+07   | -9.42e+03 \n                    2.00e+01 | 3.97e+03   | 3.97e+03   | 8.55e+02   | 5.20e+03  \n                    4.00e+01 | 2.56e+03   | 2.56e+03   | 1.29e+03   | 1.95e+03  \n                    6.00e+01 | 1.04e+03   | 1.04e+03   | 1.18e+03   | -9.87e+02 \n                    8.00e+01 | 8.77e+02   | 8.77e+02   | 1.03e+03   | -1.15e+03 \n                    1.00e+02 | 8.46e+02   | 8.46e+02   | 1.02e+03   | -1.21e+03 \n                    1.20e+02 | 8.45e+02   | 8.45e+02   | 1.03e+03   | -1.22e+03 \n                    1.40e+02 | 8.39e+02   | 8.40e+02   | 1.03e+03   | -1.23e+03 \n                    1.48e+02 | 8.39e+02   | 8.41e+02   | 1.03e+03   | -1.23e+03 \n            L2 relative error = 5e-02\tmll = 2e+07\n        grad \n             iter of 5.0e+03 | best loss  | loss       | term1      | term2     \n            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n                    0.00e+00 | 2.34e+16   | 2.34e+16   | 4.68e+16   | -3.77e+04 \n                    2.00e+01 | 4.98e+13   | nan        | nan        | -inf      \n                    2.10e+01 | 4.98e+13   | nan        | nan        | -inf      \n            L2 relative error = 1e+01\t\tmll = nan\n\nHartmann\n    SE lattice\n        nograd \n             iter of 5.0e+03 | best loss  | loss       | term1      | term2     \n            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n                    0.00e+00 | 1.25e+04   | 1.25e+04   | 3.11e+04   | -7.89e+03 \n                    2.00e+01 | -1.10e+03  | -1.10e+03  | 1.02e+03   | -5.11e+03 \n                    4.00e+01 | -1.11e+03  | -1.11e+03  | 1.13e+03   | -5.23e+03 \n                    4.60e+01 | -1.11e+03  | -1.11e+03  | 1.13e+03   | -5.23e+03 \n            L2 relative error = 3e-02\tmll = 1e+04\n        grad \n             iter of 5.0e+03 | best loss  | loss       | term1      | term2     \n            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n                    0.00e+00 | 1.22e+06   | 1.22e+06   | 2.48e+06   | -6.08e+04 \n                    2.00e+01 | -5.59e+03  | -5.59e+03  | 6.47e+03   | -3.08e+04 \n                    4.00e+01 | -6.70e+03  | -6.70e+03  | 6.29e+03   | -3.29e+04 \n                    6.00e+01 | -6.71e+03  | -6.71e+03  | 6.45e+03   | -3.30e+04 \n                    6.20e+01 | -6.71e+03  | -6.71e+03  | 6.46e+03   | -3.31e+04 \n            L2 relative error = 9e-03\t\tmll = 1e+06\n    SI lattice\n        nograd \n             iter of 5.0e+03 | best loss  | loss       | term1      | term2     \n            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n                    0.00e+00 | 4.30e+03   | 4.30e+03   | 1.80e+00   | 6.72e+03  \n                    2.00e+01 | -5.63e+02  | -5.63e+02  | 1.11e+03   | -4.12e+03 \n                    4.00e+01 | -6.46e+02  | -6.46e+02  | 1.07e+03   | -4.24e+03 \n                    6.00e+01 | -6.50e+02  | -6.50e+02  | 1.02e+03   | -4.21e+03 \n                    8.00e+01 | -6.51e+02  | -6.51e+02  | 1.02e+03   | -4.21e+03 \n                    8.20e+01 | -6.51e+02  | -6.51e+02  | 1.02e+03   | -4.21e+03 \n            L2 relative error = 6e-02\tmll = 4e+03\n        grad \n             iter of 5.0e+03 | best loss  | loss       | term1      | term2     \n            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n                    0.00e+00 | 4.06e+04   | 4.06e+04   | 2.13e+00   | 6.80e+04  \n                    2.00e+01 | 5.91e+03   | 5.91e+03   | 5.63e+03   | -6.99e+03 \n                    4.00e+01 | 5.50e+03   | 5.50e+03   | 7.12e+03   | -9.29e+03 \n                    6.00e+01 | 5.48e+03   | 5.48e+03   | 7.06e+03   | -9.28e+03 \n                    8.00e+01 | 5.46e+03   | 5.46e+03   | 7.09e+03   | -9.34e+03 \n                    1.00e+02 | 5.46e+03   | 5.46e+03   | 7.16e+03   | -9.41e+03 \n                    1.20e+02 | 5.46e+03   | 5.46e+03   | 7.18e+03   | -9.43e+03 \n                    1.40e+02 | 5.46e+03   | 5.46e+03   | 7.16e+03   | -9.42e+03 \n                    1.49e+02 | 5.46e+03   | 5.46e+03   | 7.16e+03   | -9.42e+03 \n            L2 relative error = 4e-02\t\tmll = 4e+04\n    DSI digital net\n        nograd \n             iter of 5.0e+03 | best loss  | loss       | term1      | term2     \n            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n                    0.00e+00 | 1.02e+03   | 1.02e+03   | 5.44e+03   | -5.29e+03 \n                    2.00e+01 | -4.92e+02  | -4.92e+02  | 9.54e+02   | -3.82e+03 \n                    4.00e+01 | -6.10e+02  | -6.10e+02  | 1.09e+03   | -4.19e+03 \n                    6.00e+01 | -6.16e+02  | -6.16e+02  | 1.01e+03   | -4.12e+03 \n                    8.00e+01 | -6.17e+02  | -6.17e+02  | 1.02e+03   | -4.14e+03 \n                    1.00e+02 | -6.17e+02  | -6.17e+02  | 1.02e+03   | -4.14e+03 \n                    1.03e+02 | -6.17e+02  | -6.17e+02  | 1.02e+03   | -4.14e+03 \n            L2 relative error = 8e-02\tmll = 1e+03\n        grad \n             iter of 5.0e+03 | best loss  | loss       | term1      | term2     \n            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n                    0.00e+00 | 1.49e+06   | 1.49e+06   | 3.02e+06   | -4.67e+04 \n                    2.00e+01 | 7.31e+03   | 7.31e+03   | 7.03e+03   | -5.59e+03 \n                    4.00e+01 | 6.30e+03   | 6.32e+03   | 7.42e+03   | -7.96e+03 \n                    6.00e+01 | 6.24e+03   | 6.24e+03   | 7.16e+03   | -7.84e+03 \n                    8.00e+01 | 6.21e+03   | 6.21e+03   | 7.17e+03   | -7.92e+03 \n                    1.00e+02 | 6.13e+03   | 6.13e+03   | 7.01e+03   | -7.93e+03 \n                    1.20e+02 | 6.12e+03   | 6.12e+03   | 7.15e+03   | -8.08e+03 \n                    1.40e+02 | 6.11e+03   | 6.11e+03   | 7.17e+03   | -8.11e+03 \n                    1.60e+02 | 6.11e+03   | 6.11e+03   | 7.16e+03   | -8.11e+03 \n                    1.80e+02 | 6.11e+03   | 6.11e+03   | 7.16e+03   | -8.11e+03 \n                    2.00e+02 | 6.11e+03   | 6.11e+03   | 7.16e+03   | -8.11e+03 \n                    2.20e+02 | 6.11e+03   | 6.11e+03   | 7.15e+03   | -8.10e+03 \n                    2.27e+02 | 6.11e+03   | 6.11e+03   | 7.17e+03   | -8.12e+03 \n            L2 relative error = 4e-01\t\tmll = 1e+06\n\n</pre> In\u00a0[12]: Copied! <pre>with np.printoptions(formatter={\"float\":lambda x: \"%.0e\"%x}):\n    print(\"l2rerrors_no_grad\\n\")\n    print(l2rerrors_no_grad.numpy())\n    print(\"\\n\\nl2rerrors_grad\")\n    print(l2rerrors_grad.numpy())\n    print(\"\\n\\nmll_no_grad\")\n    print(mll_no_grad.numpy())\n    print(\"\\n\\nmll_grad\")\n    print(mll_grad.numpy())\n</pre> with np.printoptions(formatter={\"float\":lambda x: \"%.0e\"%x}):     print(\"l2rerrors_no_grad\\n\")     print(l2rerrors_no_grad.numpy())     print(\"\\n\\nl2rerrors_grad\")     print(l2rerrors_grad.numpy())     print(\"\\n\\nmll_no_grad\")     print(mll_no_grad.numpy())     print(\"\\n\\nmll_grad\")     print(mll_grad.numpy()) <pre>l2rerrors_no_grad\n\n[[3e-04 1e-04 2e-02]\n [1e-03 2e-01 2e-02]\n [7e-03 2e-02 4e-02]\n [1e+00 3e-02 5e-02]\n [3e-02 6e-02 8e-02]]\n\n\nl2rerrors_grad\n[[3e-04 9e-05 1e+04]\n [3e-02 2e-03 6e+04]\n [8e-01 1e-01 2e+04]\n [3e-01 3e-01 1e+01]\n [9e-03 4e-02 4e-01]]\n\n\nmll_no_grad\n[[4e+07 1e+07 2e+09]\n [2e+08 1e+08 1e+08]\n [2e+08 6e+05 1e+07]\n [7e+08 5e+06 2e+07]\n [1e+04 4e+03 1e+03]]\n\n\nmll_grad\n[[7e+11 1e+07 nan]\n [4e+10 3e+06 5e+20]\n [5e+10 2e+06 6e+17]\n [3e+09 3e+07 nan]\n [1e+06 4e+04 1e+06]]\n</pre> In\u00a0[13]: Copied! <pre>fnames = [item[0] for item in funcs]\n_s = r\"\"\"  \\begin{tabular}{r|%s|%s}\n\"\"\"%(\"\".join(\"cc\" for name,gp in gps),\"\".join(\"cc\" for name,gp in gps))\n_s += \"    \" + \"&amp; \" + r\"\\multicolumn{%d}{c|}{$L_2$ relative error} &amp; \\multicolumn{%d}{c}{time per optimization step} \\\\\"%(2*len(gps),2*len(gps))+ \"\\n\"\n_s += \"   \"+\" &amp; \"+\" &amp; \".join(r\"\\multicolumn{2}{c}{%s}\"%name for name,gp in gps) +\" &amp; \" + \" &amp; \".join(r\"\\multicolumn{2}{c}{%s}\"%name for name,gp in gps) + r\" \\\\\" + \"\\n\"\n_s += \"    \"+\"benchmark &amp; \" + \" &amp; \".join(r\"$f$ &amp; $(f, \\nabla f)$\" for name,gp in gps) + \" &amp; \" + \" &amp; \".join(r\"$f$ &amp; $(f, \\nabla f)$\" for name,gp in gps) + r\" \\\\\" + \"\\n\"\n_s += \"    \"+r\"\\hline\" + \"\\n\"\nfor i in range(len(l2rerrors_no_grad)):\n    _t = (\"        \"+ \"%s &amp; \"%fnames[i] \n    + \" &amp; \".join(r\"%.1e &amp; %.1e\"%(l2rerrors_no_grad[i,j],l2rerrors_grad[i,j]) for j in range(l2rerrors_no_grad.size(1)))\n    + \" &amp; \"  \n    + \" &amp; \".join(r\"%.1e &amp; %.1e\"%(times_no_grad[i,j],times_grad[i,j]) for j in range(times_no_grad.size(1))) \n    + r\" \\\\\" + \"\\n\")\n    _t = _t.replace(\"e-0\",\"e-\").replace(\"e+0\",\"e\").replace(\"Styblinski\",\"StyTang\")\n    _s += _t\n_s += r\"  \\end{tabular}\"\nif FULLRUN:\n    file = open(\"./benchmarks_accuracy_time.tex\",\"w\")\n    file.write(_s)\n    file.close()\n</pre> fnames = [item[0] for item in funcs] _s = r\"\"\"  \\begin{tabular}{r|%s|%s} \"\"\"%(\"\".join(\"cc\" for name,gp in gps),\"\".join(\"cc\" for name,gp in gps)) _s += \"    \" + \"&amp; \" + r\"\\multicolumn{%d}{c|}{$L_2$ relative error} &amp; \\multicolumn{%d}{c}{time per optimization step} \\\\\"%(2*len(gps),2*len(gps))+ \"\\n\" _s += \"   \"+\" &amp; \"+\" &amp; \".join(r\"\\multicolumn{2}{c}{%s}\"%name for name,gp in gps) +\" &amp; \" + \" &amp; \".join(r\"\\multicolumn{2}{c}{%s}\"%name for name,gp in gps) + r\" \\\\\" + \"\\n\" _s += \"    \"+\"benchmark &amp; \" + \" &amp; \".join(r\"$f$ &amp; $(f, \\nabla f)$\" for name,gp in gps) + \" &amp; \" + \" &amp; \".join(r\"$f$ &amp; $(f, \\nabla f)$\" for name,gp in gps) + r\" \\\\\" + \"\\n\" _s += \"    \"+r\"\\hline\" + \"\\n\" for i in range(len(l2rerrors_no_grad)):     _t = (\"        \"+ \"%s &amp; \"%fnames[i]      + \" &amp; \".join(r\"%.1e &amp; %.1e\"%(l2rerrors_no_grad[i,j],l2rerrors_grad[i,j]) for j in range(l2rerrors_no_grad.size(1)))     + \" &amp; \"       + \" &amp; \".join(r\"%.1e &amp; %.1e\"%(times_no_grad[i,j],times_grad[i,j]) for j in range(times_no_grad.size(1)))      + r\" \\\\\" + \"\\n\")     _t = _t.replace(\"e-0\",\"e-\").replace(\"e+0\",\"e\").replace(\"Styblinski\",\"StyTang\")     _s += _t _s += r\"  \\end{tabular}\" if FULLRUN:     file = open(\"./benchmarks_accuracy_time.tex\",\"w\")     file.write(_s)     file.close() In\u00a0[14]: Copied! <pre>fnames = [item[0] for item in funcs]\n_s = r\"\"\"  \\begin{tabular}{r|%s}\n\"\"\"%(\"\".join(\"cc\" for name,gp in gps))\n_s += \"    \" + \"&amp; \" + r\"\\multicolumn{%d}{c}{MLL (Marginal Log Likelihood)} \\\\\"%(2*len(gps))+ \"\\n\"\n_s += \"   \"+\" &amp; \"+\" &amp; \".join(r\"\\multicolumn{2}{c}{%s}\"%name for name,gp in gps) + r\" \\\\\" + \"\\n\"\n_s += \"    \"+\"benchmark &amp; \" + \" &amp; \".join(r\"$f$ &amp; $(f, \\nabla f)$\" for name,gp in gps) + r\" \\\\\" + \"\\n\"\n_s += \"    \"+r\"\\hline\" + \"\\n\"\nfor i in range(len(mll_no_grad)):\n    _t = (\"        \"+ \"%s &amp; \"%fnames[i] \n    + \" &amp; \".join(r\"%.1e &amp; %.1e\"%(mll_no_grad[i,j],mll_grad[i,j]) for j in range(mll_no_grad.size(1)))\n    + r\" \\\\\" + \"\\n\")\n    _t = _t.replace(\"e-0\",\"e-\").replace(\"e+0\",\"e\").replace(\"Styblinski\",\"StyTang\")\n    _s += _t\n_s += r\"  \\end{tabular}\"\nif FULLRUN:\n    file = open(\"./benchmarks_mll.tex\",\"w\")\n    file.write(_s)\n    file.close()\n</pre> fnames = [item[0] for item in funcs] _s = r\"\"\"  \\begin{tabular}{r|%s} \"\"\"%(\"\".join(\"cc\" for name,gp in gps)) _s += \"    \" + \"&amp; \" + r\"\\multicolumn{%d}{c}{MLL (Marginal Log Likelihood)} \\\\\"%(2*len(gps))+ \"\\n\" _s += \"   \"+\" &amp; \"+\" &amp; \".join(r\"\\multicolumn{2}{c}{%s}\"%name for name,gp in gps) + r\" \\\\\" + \"\\n\" _s += \"    \"+\"benchmark &amp; \" + \" &amp; \".join(r\"$f$ &amp; $(f, \\nabla f)$\" for name,gp in gps) + r\" \\\\\" + \"\\n\" _s += \"    \"+r\"\\hline\" + \"\\n\" for i in range(len(mll_no_grad)):     _t = (\"        \"+ \"%s &amp; \"%fnames[i]      + \" &amp; \".join(r\"%.1e &amp; %.1e\"%(mll_no_grad[i,j],mll_grad[i,j]) for j in range(mll_no_grad.size(1)))     + r\" \\\\\" + \"\\n\")     _t = _t.replace(\"e-0\",\"e-\").replace(\"e+0\",\"e\").replace(\"Styblinski\",\"StyTang\")     _s += _t _s += r\"  \\end{tabular}\" if FULLRUN:     file = open(\"./benchmarks_mll.tex\",\"w\")     file.write(_s)     file.close() In\u00a0[7]: Copied! <pre>ACKLEY_ONLY = False \nALPHA = 0.25 \nLW = None\nd = 1\nn = 2**3\nnticks = 500\nxticks = torch.linspace(0,1,nticks+2)[1:-1]\ngrid = torch.linspace(0,1,n+2)[1:-1,None]\nlattice = qp.Lattice(d,seed=17) \ndnb2 = qp.DigitalNetB2(d,seed=7,randomize=\"DS\")\nlbetas_no_grad = [torch.zeros((1,d),dtype=int)]\nlbetas_grad = [torch.zeros((1,d),dtype=int)]+[ej for ej in torch.eye(d,dtype=int)]\ngps = [\n    [\"SE lattice\", lambda: fastgps.StandardGP(qp.KernelMultiTaskDerivs(qp.KernelSquaredExponential(d,torchify=True),num_tasks=len(lbetas_no_grad)),[lattice]*len(lbetas_no_grad),derivatives=lbetas_no_grad),False],\n    [r\"SI lattice\", lambda: fastgps.FastGPLattice(qp.KernelMultiTaskDerivs(qp.KernelShiftInvar(d,torchify=True,alpha=2),num_tasks=len(lbetas_no_grad)),[lattice]*len(lbetas_no_grad),derivatives=lbetas_no_grad),False],\n    [r\"DSI digital net\", lambda: fastgps.FastGPDigitalNetB2(qp.KernelMultiTaskDerivs(qp.KernelDigShiftInvar(d,torchify=True,alpha=4),num_tasks=len(lbetas_no_grad)),[dnb2]*len(lbetas_no_grad),derivatives=lbetas_no_grad),False],\n]\nif ACKLEY_ONLY:\n    funcs = [f_ackley]\n    fnames = [\"Ackley\"]\nelse:\n    funcs = [f_ackley,f_curve_1d,styblinski_tang,f_G,f_brownian_motion]\n    fnames = [\"Ackley\",\"smooth curve\",\"Styblinski-Tang\",\"G function\",\"Brownian motion\"]\nncols = len(gps)\nnrows = len(funcs)\nfig,ax = pyplot.subplots(nrows=nrows,ncols=ncols,figsize=(MW2,MH2/3*nrows*1.2),sharex=True,sharey=\"row\")\nax = np.atleast_1d(ax).reshape((nrows,ncols))\nfor j,(f,fname) in enumerate(zip(funcs,fnames)):\n    yticks = f(xticks[:,None])\n    print(fname)\n    for i,(name,gp_construct,grad) in enumerate(gps):\n        print(\"\\t\"+name)\n        gp = gp_construct()\n        x_next = gp.get_x_next(n=n*torch.ones(gp.num_tasks,dtype=int))\n        if not grad:\n            yf = [f(x_next[0])]\n        else:\n            yf = [f_grad_f(f,x_next[i])[:,i] for i in range(len(x_next))]\n        gp.add_y_next(yf)\n        ax[j,i].plot(xticks.cpu(),yticks.cpu(),color=\"k\",linewidth=LW)\n        data = gp.fit(verbose=0,store_hists=True)\n        pmean = gp.post_mean(xticks[:,None])[0]\n        l2rerror = torch.linalg.norm(pmean-yticks)/torch.linalg.norm(yticks)\n        ax[j,i].plot(xticks.cpu(),pmean.cpu(),color=COLORS[i],linewidth=LW)\n        pvar = gp.post_var(xticks[:,None])[0]\n        ci_low = pmean-ZSTAR*torch.sqrt(pvar)\n        ci_high = pmean+ZSTAR*torch.sqrt(pvar)\n        ciwidth = torch.mean(ci_high-ci_low)\n        capture = torch.mean(((yticks&lt;ci_high)*(yticks&gt;ci_low)).to(torch.float))\n        ax[j,i].fill_between(xticks.cpu(),ci_high.cpu(),ci_low.cpu(),color=COLORS[i],alpha=ALPHA)\n        ax[j,i].scatter(gp.x[0][:,0].cpu(),yf[0].cpu(),color=\"k\",s=None)\n        ax[j,i].set_xlim([0,1])\n        mll_str = (\"%.1e\"%data[\"loss\"].max()).replace(\"e+0\",\"e\").replace(\"e+\",\"e\").replace(\"e-0\",\"e-\")\n        spacer = \"\\ \\ \\ \" if FULLRUN else \"     \"\n        ax[j,i].set_xlabel(r\"%.1e\"%l2rerror+spacer+\"%.1e\"%ciwidth+spacer+\"%.1f\"%(100*capture)+(\"\\%\" if FULLRUN else \"%\")+spacer+mll_str)\nfor i,(name,gp,grad) in enumerate(gps):\n    ax[0,i].set_title(\"%s\\n%s, %s, %s, %s\"%(name,r\"$L_2$ r-error\",\" CI width\",\"CI capture\",\"MLL\"))#,fontsize=\"xx-large\")\n    #ax[-1,i].set_xlabel(r\"$x$\")#,fontsize=\"xx-large\")\n    ax[-1,i].set_xlim([0,1])\n    ax[-1,i].set_xticks([0,1])\n    # ax[-1,i].set_xticks([0,1/4,1/2,3/4,1])\n    # ax[-1,i].set_xticklabels([r\"$0$\",r\"$1/4$\",r\"$1/2$\",r\"$3/4$\",r\"$1$\"])\nfor j,(f,fname) in enumerate(zip(funcs,fnames)):\n    ax[j,0].set_ylabel(fname)#,fontsize=\"xx-large\")\n#fig.tight_layout()\nif FULLRUN:\n    if ACKLEY_ONLY:\n        fig.savefig(\"1d_gps_ackley.pdf\")#,dpi=256,bbox_inches=\"tight\")\n    else:\n        fig.savefig(\"1d_gps.pdf\")#,dpi=256,bbox_inches=\"tight\")\n</pre> ACKLEY_ONLY = False  ALPHA = 0.25  LW = None d = 1 n = 2**3 nticks = 500 xticks = torch.linspace(0,1,nticks+2)[1:-1] grid = torch.linspace(0,1,n+2)[1:-1,None] lattice = qp.Lattice(d,seed=17)  dnb2 = qp.DigitalNetB2(d,seed=7,randomize=\"DS\") lbetas_no_grad = [torch.zeros((1,d),dtype=int)] lbetas_grad = [torch.zeros((1,d),dtype=int)]+[ej for ej in torch.eye(d,dtype=int)] gps = [     [\"SE lattice\", lambda: fastgps.StandardGP(qp.KernelMultiTaskDerivs(qp.KernelSquaredExponential(d,torchify=True),num_tasks=len(lbetas_no_grad)),[lattice]*len(lbetas_no_grad),derivatives=lbetas_no_grad),False],     [r\"SI lattice\", lambda: fastgps.FastGPLattice(qp.KernelMultiTaskDerivs(qp.KernelShiftInvar(d,torchify=True,alpha=2),num_tasks=len(lbetas_no_grad)),[lattice]*len(lbetas_no_grad),derivatives=lbetas_no_grad),False],     [r\"DSI digital net\", lambda: fastgps.FastGPDigitalNetB2(qp.KernelMultiTaskDerivs(qp.KernelDigShiftInvar(d,torchify=True,alpha=4),num_tasks=len(lbetas_no_grad)),[dnb2]*len(lbetas_no_grad),derivatives=lbetas_no_grad),False], ] if ACKLEY_ONLY:     funcs = [f_ackley]     fnames = [\"Ackley\"] else:     funcs = [f_ackley,f_curve_1d,styblinski_tang,f_G,f_brownian_motion]     fnames = [\"Ackley\",\"smooth curve\",\"Styblinski-Tang\",\"G function\",\"Brownian motion\"] ncols = len(gps) nrows = len(funcs) fig,ax = pyplot.subplots(nrows=nrows,ncols=ncols,figsize=(MW2,MH2/3*nrows*1.2),sharex=True,sharey=\"row\") ax = np.atleast_1d(ax).reshape((nrows,ncols)) for j,(f,fname) in enumerate(zip(funcs,fnames)):     yticks = f(xticks[:,None])     print(fname)     for i,(name,gp_construct,grad) in enumerate(gps):         print(\"\\t\"+name)         gp = gp_construct()         x_next = gp.get_x_next(n=n*torch.ones(gp.num_tasks,dtype=int))         if not grad:             yf = [f(x_next[0])]         else:             yf = [f_grad_f(f,x_next[i])[:,i] for i in range(len(x_next))]         gp.add_y_next(yf)         ax[j,i].plot(xticks.cpu(),yticks.cpu(),color=\"k\",linewidth=LW)         data = gp.fit(verbose=0,store_hists=True)         pmean = gp.post_mean(xticks[:,None])[0]         l2rerror = torch.linalg.norm(pmean-yticks)/torch.linalg.norm(yticks)         ax[j,i].plot(xticks.cpu(),pmean.cpu(),color=COLORS[i],linewidth=LW)         pvar = gp.post_var(xticks[:,None])[0]         ci_low = pmean-ZSTAR*torch.sqrt(pvar)         ci_high = pmean+ZSTAR*torch.sqrt(pvar)         ciwidth = torch.mean(ci_high-ci_low)         capture = torch.mean(((yticksci_low)).to(torch.float))         ax[j,i].fill_between(xticks.cpu(),ci_high.cpu(),ci_low.cpu(),color=COLORS[i],alpha=ALPHA)         ax[j,i].scatter(gp.x[0][:,0].cpu(),yf[0].cpu(),color=\"k\",s=None)         ax[j,i].set_xlim([0,1])         mll_str = (\"%.1e\"%data[\"loss\"].max()).replace(\"e+0\",\"e\").replace(\"e+\",\"e\").replace(\"e-0\",\"e-\")         spacer = \"\\ \\ \\ \" if FULLRUN else \"     \"         ax[j,i].set_xlabel(r\"%.1e\"%l2rerror+spacer+\"%.1e\"%ciwidth+spacer+\"%.1f\"%(100*capture)+(\"\\%\" if FULLRUN else \"%\")+spacer+mll_str) for i,(name,gp,grad) in enumerate(gps):     ax[0,i].set_title(\"%s\\n%s, %s, %s, %s\"%(name,r\"$L_2$ r-error\",\" CI width\",\"CI capture\",\"MLL\"))#,fontsize=\"xx-large\")     #ax[-1,i].set_xlabel(r\"$x$\")#,fontsize=\"xx-large\")     ax[-1,i].set_xlim([0,1])     ax[-1,i].set_xticks([0,1])     # ax[-1,i].set_xticks([0,1/4,1/2,3/4,1])     # ax[-1,i].set_xticklabels([r\"$0$\",r\"$1/4$\",r\"$1/2$\",r\"$3/4$\",r\"$1$\"]) for j,(f,fname) in enumerate(zip(funcs,fnames)):     ax[j,0].set_ylabel(fname)#,fontsize=\"xx-large\") #fig.tight_layout() if FULLRUN:     if ACKLEY_ONLY:         fig.savefig(\"1d_gps_ackley.pdf\")#,dpi=256,bbox_inches=\"tight\")     else:         fig.savefig(\"1d_gps.pdf\")#,dpi=256,bbox_inches=\"tight\") <pre>Ackley\n\tSE lattice\n\tSI lattice\n\tDSI digital net\nsmooth curve\n\tSE lattice\n\tSI lattice\n\tDSI digital net\nStyblinski-Tang\n\tSE lattice\n\tSI lattice\n\tDSI digital net\nG function\n\tSE lattice\n\tSI lattice\n\tDSI digital net\nBrownian motion\n\tSE lattice\n\tSI lattice\n\tDSI digital net\n</pre> In\u00a0[16]: Copied! <pre>d = 2\nn = 2**12 if FULLRUN else 2**8\nf = f_ackley\nnticks = 129 if FULLRUN else 65\nx1ticks = x2ticks = torch.linspace(0,1,nticks+2)[1:-1]\nx1mesh,x2mesh = torch.meshgrid(x1ticks,x2ticks,indexing=\"ij\")\nxticks = torch.vstack([x1mesh.flatten(),x2mesh.flatten()]).T \nyticks = f(xticks)\nymesh = yticks.reshape(x1mesh.shape)\n</pre> d = 2 n = 2**12 if FULLRUN else 2**8 f = f_ackley nticks = 129 if FULLRUN else 65 x1ticks = x2ticks = torch.linspace(0,1,nticks+2)[1:-1] x1mesh,x2mesh = torch.meshgrid(x1ticks,x2ticks,indexing=\"ij\") xticks = torch.vstack([x1mesh.flatten(),x2mesh.flatten()]).T  yticks = f(xticks) ymesh = yticks.reshape(x1mesh.shape) In\u00a0[\u00a0]: Copied! <pre>class GridGen2d(qp.DiscreteDistribution):\n    def __init__(self):\n        self.mimics = 'StdUniform'\n        self.low_discrepancy = False\n        super().__init__(dimension=2,replications=None,seed=None,d_limit=np.inf,n_limit=np.inf)\n    def _gen_samples(self, n=None, n_min=None, n_max=None, return_binary=False, warn=True):\n        assert n is None and n_min==0 and (not return_binary)\n        n = n_max \n        grid1ticks = grid2ticks = torch.linspace(0,1,int(np.sqrt(n)))\n        grid1mesh,grid2mesh = torch.meshgrid(grid1ticks,grid2ticks,indexing=\"ij\")\n        grid = torch.vstack([grid1mesh.flatten(),grid2mesh.flatten()]).T\n        return grid.numpy()[None]\nlbetas = [torch.zeros((1,d),dtype=int)]\ngps_grad = [\n    [\"SE grid\", fastgps.StandardGP(qp.KernelSquaredExponential(d,torchify=True),GridGen2d())],\n    [\"SI lattice\", fastgps.FastGPLattice(qp.KernelShiftInvar(d,torchify=True,alpha=2),qp.Lattice(d,seed=7))],\n    [\"DSI digital net\", fastgps.FastGPDigitalNetB2(qp.KernelDigShiftInvar(d,torchify=True,alpha=4),qp.DigitalNetB2(d,seed=7))],\n]\nngps = len(gps_grad)\nnames = [name for name,gp in gps_grad]\ntimes = torch.zeros(len(gps_grad))\nmlls = torch.zeros(len(gps_grad))\npmeanmeshes_list = [None]*ngps \npvarmeshses_list = [None]*ngps\nfor i,(name,gp) in enumerate(gps_grad):\n    print(name) \n    if len(lbetas)==1:\n        yf = [f(gp.get_x_next(n))]\n    else:\n        assert len(lbetas)==(1+d)\n        yf = [f_grad_f(f,x_next_i)[:,i] for x_next_i in gp.get_x_next(n=n*torch.ones(gp.num_tasks,dtype=int))]\n    gp.add_y_next(yf)\n    t0 = time.perf_counter()\n    data = gp.fit(\n        #xticks,yticks,\n        #iterations=500,\n        #verbose=50,\n        #stop_crit_wait_iterations=500\n        store_hists = True\n    )\n    times[i] = (time.perf_counter()-t0)/(data[\"iteration\"][-1])\n    mlls[i] = data[\"loss_hist\"].max()\n    pmeanmeshes_list[i] = gp.post_mean(xticks,task=0).reshape(x1mesh.shape)[None,:,:].cpu()\n    #pvarmeshses_list[i] = gp.post_var(xticks,task=0).reshape(x1mesh.shape)[None,:,:].cpu()\n</pre> class GridGen2d(qp.DiscreteDistribution):     def __init__(self):         self.mimics = 'StdUniform'         self.low_discrepancy = False         super().__init__(dimension=2,replications=None,seed=None,d_limit=np.inf,n_limit=np.inf)     def _gen_samples(self, n=None, n_min=None, n_max=None, return_binary=False, warn=True):         assert n is None and n_min==0 and (not return_binary)         n = n_max          grid1ticks = grid2ticks = torch.linspace(0,1,int(np.sqrt(n)))         grid1mesh,grid2mesh = torch.meshgrid(grid1ticks,grid2ticks,indexing=\"ij\")         grid = torch.vstack([grid1mesh.flatten(),grid2mesh.flatten()]).T         return grid.numpy()[None] lbetas = [torch.zeros((1,d),dtype=int)] gps_grad = [     [\"SE grid\", fastgps.StandardGP(qp.KernelSquaredExponential(d,torchify=True),GridGen2d())],     [\"SI lattice\", fastgps.FastGPLattice(qp.KernelShiftInvar(d,torchify=True,alpha=2),qp.Lattice(d,seed=7))],     [\"DSI digital net\", fastgps.FastGPDigitalNetB2(qp.KernelDigShiftInvar(d,torchify=True,alpha=4),qp.DigitalNetB2(d,seed=7))], ] ngps = len(gps_grad) names = [name for name,gp in gps_grad] times = torch.zeros(len(gps_grad)) mlls = torch.zeros(len(gps_grad)) pmeanmeshes_list = [None]*ngps  pvarmeshses_list = [None]*ngps for i,(name,gp) in enumerate(gps_grad):     print(name)      if len(lbetas)==1:         yf = [f(gp.get_x_next(n))]     else:         assert len(lbetas)==(1+d)         yf = [f_grad_f(f,x_next_i)[:,i] for x_next_i in gp.get_x_next(n=n*torch.ones(gp.num_tasks,dtype=int))]     gp.add_y_next(yf)     t0 = time.perf_counter()     data = gp.fit(         #xticks,yticks,         #iterations=500,         #verbose=50,         #stop_crit_wait_iterations=500         store_hists = True     )     times[i] = (time.perf_counter()-t0)/(data[\"iteration\"][-1])     mlls[i] = data[\"loss_hist\"].max()     pmeanmeshes_list[i] = gp.post_mean(xticks,task=0).reshape(x1mesh.shape)[None,:,:].cpu()     #pvarmeshses_list[i] = gp.post_var(xticks,task=0).reshape(x1mesh.shape)[None,:,:].cpu() <pre>SE grid\n     iter of 5.0e+03 | best loss  | loss       | term1      | term2     \n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n            0.00e+00 | 2.57e+07   | 2.57e+07   | 5.14e+07   | -3.76e+04 \n            5.00e+00 | 1.05e+07   | 1.05e+07   | 2.09e+07   | -3.74e+04 \n            1.00e+01 | 1.37e+04   | 1.37e+04   | 5.27e+04   | -3.28e+04 \n            1.50e+01 | -6.16e+03  | -6.16e+03  | 4.42e+03   | -2.43e+04 \n            2.00e+01 | -6.16e+03  | -6.15e+03  | 6.00e+03   | -2.58e+04 \n            2.50e+01 | -6.25e+03  | -6.25e+03  | 4.77e+03   | -2.48e+04 \n            3.00e+01 | -6.26e+03  | -6.26e+03  | 4.60e+03   | -2.46e+04 \n            3.50e+01 | -6.26e+03  | -6.26e+03  | 4.66e+03   | -2.47e+04 \n            3.80e+01 | -6.26e+03  | -6.26e+03  | 4.64e+03   | -2.47e+04 \nSI lattice\n     iter of 5.0e+03 | best loss  | loss       | term1      | term2     \n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n            0.00e+00 | 2.90e+06   | 2.90e+06   | 5.83e+06   | -3.83e+04 \n            5.00e+00 | 4.68e+05   | 4.68e+05   | 9.57e+05   | -2.96e+04 \n            1.00e+01 | 5.04e+03   | 5.04e+03   | 1.03e+04   | -7.74e+03 \n            1.50e+01 | 3.42e+03   | 3.42e+03   | 5.27e+03   | -5.95e+03 \n            2.00e+01 | 1.01e+03   | 1.01e+03   | 5.95e+03   | -1.14e+04 \n            2.50e+01 | 8.43e+02   | 8.43e+02   | 6.01e+03   | -1.19e+04 \n            3.00e+01 | 6.78e+02   | 6.78e+02   | 4.60e+03   | -1.08e+04 \n            3.50e+01 | 5.96e+02   | 5.96e+02   | 4.38e+03   | -1.07e+04 \n            4.00e+01 | 5.11e+02   | 5.11e+02   | 4.14e+03   | -1.06e+04 \n            4.50e+01 | 4.02e+02   | 4.02e+02   | 4.40e+03   | -1.11e+04 \n            5.00e+01 | 3.90e+02   | 3.93e+02   | 4.43e+03   | -1.12e+04 \n            5.50e+01 | 3.89e+02   | 3.89e+02   | 4.31e+03   | -1.11e+04 \n            6.00e+01 | 3.86e+02   | 3.86e+02   | 4.12e+03   | -1.09e+04 \n            6.50e+01 | 3.82e+02   | 3.82e+02   | 4.23e+03   | -1.10e+04 \n            7.00e+01 | 3.81e+02   | 3.81e+02   | 4.14e+03   | -1.09e+04 \n            7.50e+01 | 3.80e+02   | 3.80e+02   | 4.11e+03   | -1.09e+04 \n            8.00e+01 | 3.80e+02   | 3.80e+02   | 4.13e+03   | -1.09e+04 \n            8.50e+01 | 3.80e+02   | 3.80e+02   | 4.13e+03   | -1.09e+04 \n            9.00e+01 | 3.79e+02   | 3.79e+02   | 4.11e+03   | -1.09e+04 \nDSI digital net\n     iter of 5.0e+03 | best loss  | loss       | term1      | term2     \n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n            0.00e+00 | 2.82e+04   | 2.82e+04   | 5.96e+04   | -1.07e+04 \n            5.00e+00 | 6.84e+03   | 6.84e+03   | 8.06e+03   | -1.92e+03 \n            1.00e+01 | 6.30e+03   | 6.35e+03   | 5.64e+03   | -4.80e+02 \n            1.50e+01 | 6.25e+03   | 6.25e+03   | 4.74e+03   | 2.38e+02  \n            2.00e+01 | 6.19e+03   | 6.19e+03   | 4.39e+03   | 4.71e+02  \n            2.50e+01 | 6.09e+03   | 6.09e+03   | 4.35e+03   | 3.01e+02  \n            3.00e+01 | 5.91e+03   | 5.91e+03   | 4.18e+03   | 1.05e+02  \n            3.50e+01 | 5.80e+03   | 5.82e+03   | 3.27e+03   | 8.43e+02  \n            4.00e+01 | 5.75e+03   | 5.75e+03   | 3.90e+03   | 6.91e+01  \n            4.50e+01 | 5.73e+03   | 5.73e+03   | 4.29e+03   | -3.47e+02 \n            5.00e+01 | 5.71e+03   | 5.71e+03   | 4.24e+03   | -3.51e+02 \n            5.50e+01 | 5.66e+03   | 5.66e+03   | 4.12e+03   | -3.41e+02 \n            6.00e+01 | 5.64e+03   | 5.64e+03   | 3.74e+03   | 1.65e+01  \n            6.50e+01 | 5.62e+03   | 5.62e+03   | 4.05e+03   | -3.28e+02 \n            7.00e+01 | 5.62e+03   | 5.62e+03   | 4.23e+03   | -5.13e+02 \n            7.50e+01 | 5.61e+03   | 5.61e+03   | 4.21e+03   | -5.19e+02 \n            8.00e+01 | 5.58e+03   | 5.58e+03   | 4.14e+03   | -5.06e+02 \n            8.50e+01 | 5.54e+03   | 5.54e+03   | 4.29e+03   | -7.39e+02 \n            9.00e+01 | 5.48e+03   | 5.48e+03   | 4.61e+03   | -1.19e+03 \n            9.50e+01 | 5.40e+03   | 5.40e+03   | 4.08e+03   | -8.18e+02 \n            1.00e+02 | 5.33e+03   | 5.33e+03   | 4.15e+03   | -1.03e+03 \n            1.05e+02 | 5.26e+03   | 5.26e+03   | 4.21e+03   | -1.22e+03 \n            1.10e+02 | 5.21e+03   | 5.21e+03   | 4.27e+03   | -1.39e+03 \n            1.15e+02 | 5.17e+03   | 5.17e+03   | 4.35e+03   | -1.54e+03 \n            1.20e+02 | 5.17e+03   | 5.17e+03   | 4.42e+03   | -1.60e+03 \n            1.25e+02 | 5.17e+03   | 5.17e+03   | 4.17e+03   | -1.36e+03 \n            1.30e+02 | 5.17e+03   | 5.17e+03   | 4.15e+03   | -1.35e+03 \n            1.35e+02 | 5.16e+03   | 5.16e+03   | 4.23e+03   | -1.43e+03 \n            1.40e+02 | 5.16e+03   | 5.16e+03   | 4.08e+03   | -1.29e+03 \n            1.45e+02 | 5.16e+03   | 5.16e+03   | 4.06e+03   | -1.28e+03 \n            1.50e+02 | 5.16e+03   | 5.16e+03   | 4.08e+03   | -1.29e+03 \n            1.55e+02 | 5.16e+03   | 5.16e+03   | 4.10e+03   | -1.32e+03 \n            1.60e+02 | 5.16e+03   | 5.16e+03   | 4.09e+03   | -1.31e+03 \n            1.65e+02 | 5.16e+03   | 5.16e+03   | 4.10e+03   | -1.31e+03 \n            1.70e+02 | 5.16e+03   | 5.16e+03   | 4.03e+03   | -1.24e+03 \n            1.75e+02 | 5.16e+03   | 5.16e+03   | 4.01e+03   | -1.23e+03 \n            1.78e+02 | 5.16e+03   | 5.16e+03   | 4.09e+03   | -1.31e+03 \n</pre> In\u00a0[18]: Copied! <pre>CMAP = cm.gnuplot2\nnrows = 2\nncols = len(gps_grad)\nfig = pyplot.figure(figsize=(1.5*MW1,1.75*MW1/ncols*nrows))\nax = np.array([[fig.add_subplot(nrows,ncols,ncols*i+j+1,projection=\"3d\") for j in range(ncols)] for i in range(nrows)],dtype=object)\npmeanmeshes = torch.vstack(pmeanmeshes_list)\n# pvarmeshes = torch.vstack(pvarmeshses_list)\n# pvarmeshes[pvarmeshes&lt;0] = 0.\n# ci_low = pmeanmeshes-ZSTAR*torch.sqrt(pvarmeshes)\n# ci_high = pmeanmeshes+ZSTAR*torch.sqrt(pvarmeshes)\n# ciwidth = ci_high-ci_low\n# capture = (ci_low&lt;ymesh)*(ymesh&lt;ci_high)\nerrors = torch.abs(pmeanmeshes-ymesh.cpu())\nerrormeshes = torch.log10(errors)\nsurfs = [None]*len(gps_grad)\nfor i,name in enumerate(names):\n    pmean_max,pmean_min = pmeanmeshes.max(),pmeanmeshes.min()\n    error_max,error_min = torch.quantile(errormeshes,1),torch.quantile(errormeshes,.05)\n    norm_pmean = colors.Normalize(vmin=pmean_min,vmax=pmean_max)\n    norm_err = colors.Normalize(vmin=error_min,vmax=error_max)\n    surfs[i] = ax[0,i].plot_surface(x1mesh.cpu(),x2mesh.cpu(),pmeanmeshes[i],\n        cmap=CMAP,\n        vmin=pmean_min,vmax=pmean_max,\n        antialiased=False,rstride=1,cstride=1,edgecolor='none')\n    surfs[i] = ax[1,i].plot_surface(x1mesh.cpu(),x2mesh.cpu(),errormeshes[i],\n        cmap=CMAP,\n        vmin=error_min,vmax=error_max,\n        #facecolors=CMAP(norm_err(errormeshes[i])),\n        antialiased=True,rstride=1,cstride=1,edgecolor='none')\n    l2rerror = torch.linalg.norm(errors[i])/torch.linalg.norm(ymesh)\n    ax[0,i].set_title(\"%s\\n\"%name\n        +(r\"error = %.1e\"%l2rerror).replace(\"e-0\",\"e-\")\n        +\"\\n\"\n        +(r\"time = %.1e\"%times[i]).replace(\"e-0\",\"e-\").replace(\"e+0\",\"e\")\n        +\"\\n\"\n        # +(r\"average CI width = %.2e\"%ciwidth[i].mean()).replace(\"e-0\",\"e-\").replace(\"e+0\",\"e\")\n        # +\"\\n\"\n        # +(r\"CI capture = %.1f\"%(100*capture[i].to(torch.float).mean())).replace(\"e-0\",\"e-\").replace(\"e+0\",\"e\")+\"\\%\"\n        # +\"\\n\"\n        +(r\"MLL = %.1e\"%(mlls[i])).replace(\"e-0\",\"e-\").replace(\"e+0\",\"e\"))\n    ax[0,i].set_zlim([pmean_min,pmean_max])\n    #ax[1,i].set_zlim([errormeshes.min(),errormeshes.max()])\ncax_pmean = fig.add_axes([\n    ax[0,0].get_position().x0-0.2,\n    ax[0,0].get_position().y0,\n    .025,\n    ax[0,0].get_position().y1-ax[0,0].get_position().y0,\n])\ncax_err = fig.add_axes([\n    ax[1,0].get_position().x0-0.2,\n    ax[1,0].get_position().y0,\n    0.025,\n    ax[1,0].get_position().y1-ax[1,0].get_position().y0,\n])\ncbar_pmean = fig.colorbar(cm.ScalarMappable(norm=norm_pmean,cmap=CMAP),cax_pmean,orientation=\"vertical\",location=\"left\",label=r\"GP prediction\")\ncbar_err = fig.colorbar(cm.ScalarMappable(norm=norm_err,cmap=CMAP),cax_err,orientation=\"vertical\",extend=\"min\",location=\"left\",label=\"error\")\nfor i in range(ncols):\n    for j in range(nrows):\n        ax[j,i].axis(\"off\")\n        # ax[j,i].set_xlim([0,1]); ax[j,i].set_xticks([0,1])\n        # ax[j,i].set_ylim([0,1]); ax[j,i].set_yticks([0,1])\n        # ax[j,i].xaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))\n        # ax[j,i].yaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))\n        # ax[j,i].zaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))\n        # ax[j,i].xaxis._axinfo[\"grid\"]['color'] =  (1,1,1,0)\n        # ax[j,i].yaxis._axinfo[\"grid\"]['color'] =  (1,1,1,0)\n        # ax[j,i].zaxis._axinfo[\"grid\"]['color'] =  (1,1,1,0)\n        #ax[j,i].set_xlabel(r\"$x_1$\",fontsize=\"xx-large\")\n        #ax[j,i].set_ylabel(r\"$x_2$\",fontsize=\"xx-large\")\n    #ax[0,i].set_xlabel(\"test\")\n    #ax[0,i].set_zlabel(r\"$f(\\mathbf{x})$\")#,fontsize=\"xx-large\")\n    #ax[1,i].set_zlabel(r\"$|f(\\mathbf{x})-f_{\\mathrm{true}}(\\mathbf{x})|$\")#,fontsize=\"xx-large\")\n    #ax[0,i].set_zlim([pmeanmeshes.min(),pmeanmeshes.max()]); ax[i,j].set_yticks([0,1])\n    #ax[1,i].zaxis._set_scale('log')\n    #ax[1,i].set_zscale(\"log\",base=10)\ncbar_err.ax.set_yticklabels([r\"$10^{%.1f}$\"%tick for tick in cbar_err.ax.get_yticks()])\n#fig.tight_layout()\nif FULLRUN:\n    fig.savefig(\"2d_gp.pdf\",bbox_inches=\"tight\")#,dpi=256)\n</pre> CMAP = cm.gnuplot2 nrows = 2 ncols = len(gps_grad) fig = pyplot.figure(figsize=(1.5*MW1,1.75*MW1/ncols*nrows)) ax = np.array([[fig.add_subplot(nrows,ncols,ncols*i+j+1,projection=\"3d\") for j in range(ncols)] for i in range(nrows)],dtype=object) pmeanmeshes = torch.vstack(pmeanmeshes_list) # pvarmeshes = torch.vstack(pvarmeshses_list) # pvarmeshes[pvarmeshes&lt;0] = 0. # ci_low = pmeanmeshes-ZSTAR*torch.sqrt(pvarmeshes) # ci_high = pmeanmeshes+ZSTAR*torch.sqrt(pvarmeshes) # ciwidth = ci_high-ci_low # capture = (ci_low In\u00a0[\u00a0]: Copied! <pre>if FULLRUN:\n    dvec = 2**torch.arange(0,5,dtype=int)\n    nvec = 2**torch.arange(0,10,dtype=int)\nelse:\n    dvec = torch.tensor([1,2])\n    nvec = 2**torch.arange(0,7,dtype=int)\ngps = [\n    [\"SE lattice\", lambda n,lbetas: fastgps.StandardGP(qp.KernelMultiTaskDerivs(qp.KernelGaussian(d,lengthscales=1.07e-01,scale=1.01e+01,torchify=True),num_tasks=len(lbetas)),[qp.Lattice(d,seed=11) for i in range(len(lbetas))],derivatives=lbetas)],\n    [\"SI lattice\", lambda n,lbetas: fastgps.FastGPLattice(qp.KernelMultiTaskDerivs(qp.KernelShiftInvar(d,lengthscales=1e0,scale=5e8,alpha=2,torchify=True),num_tasks=len(lbetas)),[qp.Lattice(d,seed=11) for i in range(len(lbetas))],derivatives=lbetas)],\n    [\"DSI digital net\", lambda n,lbetas: fastgps.FastGPDigitalNetB2(qp.KernelMultiTaskDerivs(qp.KernelDigShiftInvar(d,lengthscales=2.74e-07,scale=1.53e+09,alpha=4,torchify=True),num_tasks=len(lbetas)),[qp.DigitalNetB2(d,seed=11,randomize='DS') for i in range(len(lbetas))],derivatives=lbetas)],\n]\nf = f_ackley\n</pre> if FULLRUN:     dvec = 2**torch.arange(0,5,dtype=int)     nvec = 2**torch.arange(0,10,dtype=int) else:     dvec = torch.tensor([1,2])     nvec = 2**torch.arange(0,7,dtype=int) gps = [     [\"SE lattice\", lambda n,lbetas: fastgps.StandardGP(qp.KernelMultiTaskDerivs(qp.KernelGaussian(d,lengthscales=1.07e-01,scale=1.01e+01,torchify=True),num_tasks=len(lbetas)),[qp.Lattice(d,seed=11) for i in range(len(lbetas))],derivatives=lbetas)],     [\"SI lattice\", lambda n,lbetas: fastgps.FastGPLattice(qp.KernelMultiTaskDerivs(qp.KernelShiftInvar(d,lengthscales=1e0,scale=5e8,alpha=2,torchify=True),num_tasks=len(lbetas)),[qp.Lattice(d,seed=11) for i in range(len(lbetas))],derivatives=lbetas)],     [\"DSI digital net\", lambda n,lbetas: fastgps.FastGPDigitalNetB2(qp.KernelMultiTaskDerivs(qp.KernelDigShiftInvar(d,lengthscales=2.74e-07,scale=1.53e+09,alpha=4,torchify=True),num_tasks=len(lbetas)),[qp.DigitalNetB2(d,seed=11,randomize='DS') for i in range(len(lbetas))],derivatives=lbetas)], ] f = f_ackley In\u00a0[\u00a0]: Copied! <pre>print(\"dvec = %s\"%str(dvec.numpy()))\nprint(\"nvec = %s\"%str(nvec.numpy()))\nprint()\ntimes = torch.nan*torch.zeros((len(gps),len(dvec),len(nvec)))\ntimes_grad = torch.nan*torch.zeros((len(gps),len(dvec),len(nvec)))\nl2rerrors = torch.nan*torch.zeros((len(gps),len(dvec),len(nvec)))\nl2rerrors_grad = torch.nan*torch.zeros((len(gps),len(dvec),len(nvec)))\nfor j,d in enumerate(dvec):\n    x_test = torch.from_numpy(qp.Halton(d,seed=7)(2**10))\n    y_test = f(x_test)\n    print(\"d = %d\"%d)\n    for i,n in enumerate(nvec):\n        print(\"    n = %d\"%n)\n        for k,(name,gp_constructor) in enumerate(gps):\n            # no grad\n            lbetas = [torch.zeros(d,dtype=int)]\n            gp = gp_constructor(d,lbetas)\n            #if isinstance(gp,fastgps.StandardGP) and n&gt;=128: continue \n            x_next = gp.get_x_next(n=torch.tensor([n]))\n            gp.add_y_next(f(x_next[0]))\n            t0 = time.perf_counter()\n            data = gp.fit(\n                #iterations=250,\n                verbose=25 if FULLRUN else 0,\n                verbose_indent=8,\n                #stop_crit_improvement_threshold = 1e-2,\n                #stop_crit_wait_iterations=250,\n                store_hists = True,\n            )\n            times[k,j,i] = (time.perf_counter()-t0)/(data[\"iteration\"][-1])\n            yhat = gp.post_mean(x_test)\n            l2rerrors[k,j,i] = torch.linalg.norm(y_test-yhat)/torch.linalg.norm(y_test)\n            if FULLRUN: print(\"        %35s: \\tl2 relative error = %-15.0e\"%(name,l2rerrors[k,j,i]))\n            if d&gt;=8 and isinstance(gp,fastgps.StandardGP) and n&gt;=256: continue \n            # grad\n            lbetas_grad = [torch.zeros(d,dtype=int)]+[ej for ej in torch.eye(d,dtype=int)]\n            gp = gp_constructor(d,lbetas_grad)\n            x_next = gp.get_x_next(n*torch.ones(gp.num_tasks,dtype=int))\n            gp.add_y_next([f_grad_f(f,x_next[i])[:,i] for i in range(len(x_next))])\n            t0 = time.perf_counter()\n            data = gp.fit(\n                #iterations=250,\n                verbose=25 if FULLRUN else 0,\n                verbose_indent=8,\n                #stop_crit_improvement_threshold = 1e-2,\n                #stop_crit_wait_iterations=250,\n                store_hists = True,\n            )\n            times_grad[k,j,i] = (time.perf_counter()-t0)/(data[\"iteration\"][-1])\n            yhat = gp.post_mean(x_test,task=0)\n            l2rerrors_grad[k,j,i] = torch.linalg.norm(y_test-yhat)/torch.linalg.norm(y_test)\n            if FULLRUN: print(\"        %35s: \\tl2 relative error = %-15.0e\"%(name+\" + grad\",l2rerrors_grad[k,j,i]))\nif FULLRUN:\n    torch.save(nvec,\"speed_accuracy_comp_data/nvec.pth\")\n    torch.save(l2rerrors,\"speed_accuracy_comp_data/l2rerrors.pth\")\n    torch.save(l2rerrors_grad,\"speed_accuracy_comp_data/l2rerrors_grad.pth\")\n    torch.save(times,\"speed_accuracy_comp_data/times.pth\")\n    torch.save(times_grad,\"speed_accuracy_comp_data/times_grad.pth\")\n</pre> print(\"dvec = %s\"%str(dvec.numpy())) print(\"nvec = %s\"%str(nvec.numpy())) print() times = torch.nan*torch.zeros((len(gps),len(dvec),len(nvec))) times_grad = torch.nan*torch.zeros((len(gps),len(dvec),len(nvec))) l2rerrors = torch.nan*torch.zeros((len(gps),len(dvec),len(nvec))) l2rerrors_grad = torch.nan*torch.zeros((len(gps),len(dvec),len(nvec))) for j,d in enumerate(dvec):     x_test = torch.from_numpy(qp.Halton(d,seed=7)(2**10))     y_test = f(x_test)     print(\"d = %d\"%d)     for i,n in enumerate(nvec):         print(\"    n = %d\"%n)         for k,(name,gp_constructor) in enumerate(gps):             # no grad             lbetas = [torch.zeros(d,dtype=int)]             gp = gp_constructor(d,lbetas)             #if isinstance(gp,fastgps.StandardGP) and n&gt;=128: continue              x_next = gp.get_x_next(n=torch.tensor([n]))             gp.add_y_next(f(x_next[0]))             t0 = time.perf_counter()             data = gp.fit(                 #iterations=250,                 verbose=25 if FULLRUN else 0,                 verbose_indent=8,                 #stop_crit_improvement_threshold = 1e-2,                 #stop_crit_wait_iterations=250,                 store_hists = True,             )             times[k,j,i] = (time.perf_counter()-t0)/(data[\"iteration\"][-1])             yhat = gp.post_mean(x_test)             l2rerrors[k,j,i] = torch.linalg.norm(y_test-yhat)/torch.linalg.norm(y_test)             if FULLRUN: print(\"        %35s: \\tl2 relative error = %-15.0e\"%(name,l2rerrors[k,j,i]))             if d&gt;=8 and isinstance(gp,fastgps.StandardGP) and n&gt;=256: continue              # grad             lbetas_grad = [torch.zeros(d,dtype=int)]+[ej for ej in torch.eye(d,dtype=int)]             gp = gp_constructor(d,lbetas_grad)             x_next = gp.get_x_next(n*torch.ones(gp.num_tasks,dtype=int))             gp.add_y_next([f_grad_f(f,x_next[i])[:,i] for i in range(len(x_next))])             t0 = time.perf_counter()             data = gp.fit(                 #iterations=250,                 verbose=25 if FULLRUN else 0,                 verbose_indent=8,                 #stop_crit_improvement_threshold = 1e-2,                 #stop_crit_wait_iterations=250,                 store_hists = True,             )             times_grad[k,j,i] = (time.perf_counter()-t0)/(data[\"iteration\"][-1])             yhat = gp.post_mean(x_test,task=0)             l2rerrors_grad[k,j,i] = torch.linalg.norm(y_test-yhat)/torch.linalg.norm(y_test)             if FULLRUN: print(\"        %35s: \\tl2 relative error = %-15.0e\"%(name+\" + grad\",l2rerrors_grad[k,j,i])) if FULLRUN:     torch.save(nvec,\"speed_accuracy_comp_data/nvec.pth\")     torch.save(l2rerrors,\"speed_accuracy_comp_data/l2rerrors.pth\")     torch.save(l2rerrors_grad,\"speed_accuracy_comp_data/l2rerrors_grad.pth\")     torch.save(times,\"speed_accuracy_comp_data/times.pth\")     torch.save(times_grad,\"speed_accuracy_comp_data/times_grad.pth\") In\u00a0[9]: Copied! <pre>TRIMFIG = False \nif FULLRUN:\n    nvec = torch.load(\"speed_accuracy_comp_data/nvec.pth\")\n    l2rerrors = torch.load(\"speed_accuracy_comp_data/l2rerrors.pth\")\n    l2rerrors_grad = torch.load(\"speed_accuracy_comp_data/l2rerrors_grad.pth\")\n    times = torch.load(\"speed_accuracy_comp_data/times.pth\")\n    times_grad = torch.load(\"speed_accuracy_comp_data/times_grad.pth\")\nncols = 3\nif TRIMFIG:\n    js = [3]\nelse:\n    js = [i for i in range(len(dvec))]\nnrows = len(js)\nfig,ax = pyplot.subplots(nrows=nrows,ncols=ncols,figsize=(MW2,MH2/3*nrows))\nax = ax.reshape((nrows,ncols))\nS = None\nLW = None\nMARKER = \".\"\nLINESTYLE = \"dotted\"\nfor j,jplt in enumerate(js):\n    for k in range(times.size(0)):\n        label = gps[k][0]+r\" $f$\" if j==0 else None\n        ax[j,0].plot(nvec,l2rerrors[k,jplt],marker=MARKER,color=COLORS[k],label=label,linewidth=LW,markersize=S)\n        ax[j,1].plot(nvec,times[k,jplt],marker=MARKER,color=COLORS[k],label=None,linewidth=LW,markersize=S)\n        ax[j,2].plot(times[k,jplt],l2rerrors[k,jplt],marker=MARKER,color=COLORS[k],label=None,linewidth=LW,markersize=S)\n        if gps[k][0]==\"DSI digital net\": continue\n        label_grad = (gps[k][0]+r\" $(f,\\nabla f)$\") if j==0 else None\n        ax[j,1].plot(nvec,times_grad[k,jplt],marker=MARKER,color=COLORS[k],label=None,linewidth=LW,markersize=S,linestyle=LINESTYLE)\n        ax[j,0].plot(nvec,l2rerrors_grad[k,jplt],marker=MARKER,color=COLORS[k],label=label_grad,linewidth=LW,markersize=S,linestyle=LINESTYLE)\n        ax[j,2].plot(times_grad[k,jplt],l2rerrors_grad[k,jplt],marker=MARKER,color=COLORS[k],label=None,linewidth=LW,markersize=S,linestyle=LINESTYLE)\n        # ax[j,2].scatter(times[k,j],l2rerrors[k,j],marker=MARKER,color=COLORS[k],label=None,s=S*10)\n        # ax[j,2].scatter(times_grad[k,j],l2rerrors_grad[k,j],marker=MARKER,color=COLORS[k],label=None,s=S*10)\nfor j,jplt in enumerate(js):\n    for i in range(ncols):\n        ax[j,i].set_yscale(\"log\",base=10)\n    #ax[j-j_offset,0].set_xscale(\"log\",base=10)\n    #ax[j-j_offset,1].set_xscale(\"log\",base=10)\n    ax[j,0].set_ylabel(r\"$d=%d$\"%dvec[jplt])#,fontsize=\"xx-large\")\n    ax[j,2].set_xscale(\"log\",base=10)\nax[0,0].set_title(r\"$L_2$ relative error vs $n$\")#,fontsize=\"xx-large\")\nax[0,1].set_title(r\"time vs $n$\")#,fontsize=\"xx-large\")\nax[0,2].set_title(r\"$L_2$ relative error vs time\")#,fontsize=\"xx-large\")\nif FULLRUN:\n    if TRIMFIG:\n        fig.legend(frameon=False,ncol=6,bbox_to_anchor=(.5,1.1),loc=\"upper center\") # ,fontsize=\"xx-large\"\n        fig.tight_layout()\n        fig.savefig(\"speed_accuracy_comp_trim.pdf\",dpi=256,bbox_inches=\"tight\")\n    else:\n        fig.legend(frameon=False,ncol=6,bbox_to_anchor=(.5,1.03),loc=\"upper center\") # ,fontsize=\"xx-large\"\n        fig.tight_layout()\n        fig.savefig(\"speed_accuracy_comp.pdf\",dpi=256,bbox_inches=\"tight\")\nelse:\n    fig.tight_layout()\n</pre> TRIMFIG = False  if FULLRUN:     nvec = torch.load(\"speed_accuracy_comp_data/nvec.pth\")     l2rerrors = torch.load(\"speed_accuracy_comp_data/l2rerrors.pth\")     l2rerrors_grad = torch.load(\"speed_accuracy_comp_data/l2rerrors_grad.pth\")     times = torch.load(\"speed_accuracy_comp_data/times.pth\")     times_grad = torch.load(\"speed_accuracy_comp_data/times_grad.pth\") ncols = 3 if TRIMFIG:     js = [3] else:     js = [i for i in range(len(dvec))] nrows = len(js) fig,ax = pyplot.subplots(nrows=nrows,ncols=ncols,figsize=(MW2,MH2/3*nrows)) ax = ax.reshape((nrows,ncols)) S = None LW = None MARKER = \".\" LINESTYLE = \"dotted\" for j,jplt in enumerate(js):     for k in range(times.size(0)):         label = gps[k][0]+r\" $f$\" if j==0 else None         ax[j,0].plot(nvec,l2rerrors[k,jplt],marker=MARKER,color=COLORS[k],label=label,linewidth=LW,markersize=S)         ax[j,1].plot(nvec,times[k,jplt],marker=MARKER,color=COLORS[k],label=None,linewidth=LW,markersize=S)         ax[j,2].plot(times[k,jplt],l2rerrors[k,jplt],marker=MARKER,color=COLORS[k],label=None,linewidth=LW,markersize=S)         if gps[k][0]==\"DSI digital net\": continue         label_grad = (gps[k][0]+r\" $(f,\\nabla f)$\") if j==0 else None         ax[j,1].plot(nvec,times_grad[k,jplt],marker=MARKER,color=COLORS[k],label=None,linewidth=LW,markersize=S,linestyle=LINESTYLE)         ax[j,0].plot(nvec,l2rerrors_grad[k,jplt],marker=MARKER,color=COLORS[k],label=label_grad,linewidth=LW,markersize=S,linestyle=LINESTYLE)         ax[j,2].plot(times_grad[k,jplt],l2rerrors_grad[k,jplt],marker=MARKER,color=COLORS[k],label=None,linewidth=LW,markersize=S,linestyle=LINESTYLE)         # ax[j,2].scatter(times[k,j],l2rerrors[k,j],marker=MARKER,color=COLORS[k],label=None,s=S*10)         # ax[j,2].scatter(times_grad[k,j],l2rerrors_grad[k,j],marker=MARKER,color=COLORS[k],label=None,s=S*10) for j,jplt in enumerate(js):     for i in range(ncols):         ax[j,i].set_yscale(\"log\",base=10)     #ax[j-j_offset,0].set_xscale(\"log\",base=10)     #ax[j-j_offset,1].set_xscale(\"log\",base=10)     ax[j,0].set_ylabel(r\"$d=%d$\"%dvec[jplt])#,fontsize=\"xx-large\")     ax[j,2].set_xscale(\"log\",base=10) ax[0,0].set_title(r\"$L_2$ relative error vs $n$\")#,fontsize=\"xx-large\") ax[0,1].set_title(r\"time vs $n$\")#,fontsize=\"xx-large\") ax[0,2].set_title(r\"$L_2$ relative error vs time\")#,fontsize=\"xx-large\") if FULLRUN:     if TRIMFIG:         fig.legend(frameon=False,ncol=6,bbox_to_anchor=(.5,1.1),loc=\"upper center\") # ,fontsize=\"xx-large\"         fig.tight_layout()         fig.savefig(\"speed_accuracy_comp_trim.pdf\",dpi=256,bbox_inches=\"tight\")     else:         fig.legend(frameon=False,ncol=6,bbox_to_anchor=(.5,1.03),loc=\"upper center\") # ,fontsize=\"xx-large\"         fig.tight_layout()         fig.savefig(\"speed_accuracy_comp.pdf\",dpi=256,bbox_inches=\"tight\") else:     fig.tight_layout() In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/papers/probnum25/probnum25/#probnum-2025-paper-on-fast-gaussian-processes","title":"ProbNum 2025 Paper on Fast Gaussian Processes\u00b6","text":"<p>Aleksei G. Sorokin, Pieterjan Robbe, Fred J. Hickernell. (2025). Fast Gaussian process regression for high dimensional functions with derivative information. Proceedings of the First International Conference on Probabilistic Numerics, in Proceedings of Machine Learning Research 271:35-49 Available from https://proceedings.mlr.press/v271/sorokin25a.html.</p>"},{"location":"examples/papers/probnum25/probnum25/#setup","title":"Setup\u00b6","text":""},{"location":"examples/papers/probnum25/probnum25/#test-functions","title":"Test functions\u00b6","text":""},{"location":"examples/papers/probnum25/probnum25/#pointsets","title":"Pointsets\u00b6","text":""},{"location":"examples/papers/probnum25/probnum25/#kernel-matrix-structures","title":"Kernel Matrix Structures\u00b6","text":""},{"location":"examples/papers/probnum25/probnum25/#prior-draws","title":"Prior Draws\u00b6","text":""},{"location":"examples/papers/probnum25/probnum25/#benchmarks","title":"Benchmarks\u00b6","text":""},{"location":"examples/papers/probnum25/probnum25/#1d-example","title":"1d example\u00b6","text":""},{"location":"examples/papers/probnum25/probnum25/#2d-example","title":"2d example\u00b6","text":""},{"location":"examples/papers/probnum25/probnum25/#speed-comparison","title":"Speed Comparison\u00b6","text":""},{"location":"examples/simple/compare_gps_plot/","title":"Compare GPs","text":"In\u00a0[65]: Copied! <pre>import fastgps\nimport qmcpy as qp\nimport numpy as np\nimport torch\nimport pandas as pd\nimport itertools\nfrom matplotlib import pyplot\nimport tueplots.figsizes\n</pre> import fastgps import qmcpy as qp import numpy as np import torch import pandas as pd import itertools from matplotlib import pyplot import tueplots.figsizes In\u00a0[66]: Copied! <pre>device = \"cpu\"\nif device!=\"mps\":\n    torch.set_default_dtype(torch.float64)\n</pre> device = \"cpu\" if device!=\"mps\":     torch.set_default_dtype(torch.float64) In\u00a0[67]: Copied! <pre>colors = [\"xkcd:\"+color[:-1] for color in pd.read_csv(\"../../../xkcd_colors.txt\",comment=\"#\").iloc[:,0].tolist()][::-1]\n</pre> colors = [\"xkcd:\"+color[:-1] for color in pd.read_csv(\"../../../xkcd_colors.txt\",comment=\"#\").iloc[:,0].tolist()][::-1] In\u00a0[68]: Copied! <pre>d = 1\ndef f_ackley(x, a=20, b=0.2, c=2*np.pi, scaling=32.768):\n    return x[:,0]*torch.exp(-x[:,0])\n    # https://www.sfu.ca/~ssurjano/ackley.html\n    assert x.ndim==2\n    x = 2*scaling*x-scaling\n    t1 = a*torch.exp(-b*torch.sqrt(torch.mean(x**2,1)))\n    t2 = torch.exp(torch.mean(torch.cos(c*x),1))\n    t3 = a+np.exp(1)\n    y = -t1-t2+t3\n    return y\n</pre> d = 1 def f_ackley(x, a=20, b=0.2, c=2*np.pi, scaling=32.768):     return x[:,0]*torch.exp(-x[:,0])     # https://www.sfu.ca/~ssurjano/ackley.html     assert x.ndim==2     x = 2*scaling*x-scaling     t1 = a*torch.exp(-b*torch.sqrt(torch.mean(x**2,1)))     t2 = torch.exp(torch.mean(torch.cos(c*x),1))     t3 = a+np.exp(1)     y = -t1-t2+t3     return y In\u00a0[69]: Copied! <pre>n = 2**2\nxticks = torch.linspace(0,1,501,device=device)\nyticks = f_ackley(xticks[:,None])\n</pre> n = 2**2 xticks = torch.linspace(0,1,501,device=device) yticks = f_ackley(xticks[:,None]) In\u00a0[70]: Copied! <pre>print(\"  n = %d\"%n)\nsgp = fastgps.StandardGP(\n    qp.KernelSquaredExponential(d,torchify=True,device=device),\n    qp.DigitalNetB2(d,seed=11),\n    )\nx_next = sgp.get_x_next(n)\ny_next = f_ackley(x_next)\nsgp.add_y_next(y_next)\nsgp.fit()\npmean_std_n,pstd_std_n,q,ci_low_std_n,ci_high_std_n = sgp.post_ci(xticks[:,None])\nx_std_n,y_std_n = sgp.x.clone(),sgp.y.clone()\nnprojs = [n,2*n,4*n,8*n,16*n]\nvprojs = [sgp.post_cubature_var(n=nproj) for nproj in nprojs]\nprint(\"    posterior cubature var: \"+((\"n = %d %-10.2e\")*len(nprojs))%tuple(itertools.chain(*zip(nprojs,vprojs))))\nfig,ax = pyplot.subplots(nrows=1,ncols=1,figsize=(6,4))\nax.set_xscale(\"log\",base=10)\nax.set_yscale(\"log\",base=10)\nax.plot(nprojs,vprojs,'-o');\n</pre> print(\"  n = %d\"%n) sgp = fastgps.StandardGP(     qp.KernelSquaredExponential(d,torchify=True,device=device),     qp.DigitalNetB2(d,seed=11),     ) x_next = sgp.get_x_next(n) y_next = f_ackley(x_next) sgp.add_y_next(y_next) sgp.fit() pmean_std_n,pstd_std_n,q,ci_low_std_n,ci_high_std_n = sgp.post_ci(xticks[:,None]) x_std_n,y_std_n = sgp.x.clone(),sgp.y.clone() nprojs = [n,2*n,4*n,8*n,16*n] vprojs = [sgp.post_cubature_var(n=nproj) for nproj in nprojs] print(\"    posterior cubature var: \"+((\"n = %d %-10.2e\")*len(nprojs))%tuple(itertools.chain(*zip(nprojs,vprojs)))) fig,ax = pyplot.subplots(nrows=1,ncols=1,figsize=(6,4)) ax.set_xscale(\"log\",base=10) ax.set_yscale(\"log\",base=10) ax.plot(nprojs,vprojs,'-o'); <pre>  n = 4\n     iter of 5.0e+03 | best loss  | loss      \n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n            0.00e+00 | -3.04e+00  | -3.04e+00 \n            5.00e+00 | -3.98e+00  | -3.98e+00 \n            1.00e+01 | -4.92e+00  | -4.92e+00 \n            1.50e+01 | -5.00e+00  | -5.00e+00 \n            2.00e+01 | -5.02e+00  | -5.00e+00 \n            2.20e+01 | -5.02e+00  | -5.01e+00 \n    posterior cubature var: n = 4 2.92e-05  n = 8 1.27e-05  n = 16 6.27e-06  n = 32 3.12e-06  n = 64 1.56e-06  \n</pre> In\u00a0[71]: Copied! <pre>print(\"  n = %d\"%(2*n))\nx_next = sgp.get_x_next(2*n)\ny_next = f_ackley(x_next)\nsgp.add_y_next(y_next)\nsgp.fit()\npmean_std_2n,pstd_std_2n,q,ci_low_std_2n,ci_high_std_2n = sgp.post_ci(xticks[:,None])\nx_std_2n,y_std_2n = sgp.x.clone(),sgp.y.clone()\nnprojs = [2*n,4*n,8*n,16*n,32*n]\nvprojs = [sgp.post_cubature_var(n=nproj) for nproj in nprojs]\nprint(\"    posterior cubature var: \"+((\"n = %d %-10.2e\")*len(nprojs))%tuple(itertools.chain(*zip(nprojs,vprojs))))\nax.plot(nprojs,vprojs,'-o')\nfig\n</pre> print(\"  n = %d\"%(2*n)) x_next = sgp.get_x_next(2*n) y_next = f_ackley(x_next) sgp.add_y_next(y_next) sgp.fit() pmean_std_2n,pstd_std_2n,q,ci_low_std_2n,ci_high_std_2n = sgp.post_ci(xticks[:,None]) x_std_2n,y_std_2n = sgp.x.clone(),sgp.y.clone() nprojs = [2*n,4*n,8*n,16*n,32*n] vprojs = [sgp.post_cubature_var(n=nproj) for nproj in nprojs] print(\"    posterior cubature var: \"+((\"n = %d %-10.2e\")*len(nprojs))%tuple(itertools.chain(*zip(nprojs,vprojs)))) ax.plot(nprojs,vprojs,'-o') fig <pre>  n = 8\n     iter of 5.0e+03 | best loss  | loss      \n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n            0.00e+00 | -1.76e+01  | -1.76e+01 \n            5.00e+00 | -1.76e+01  | -1.76e+01 \n            1.00e+01 | -1.76e+01  | -1.76e+01 \n    posterior cubature var: n = 8 1.28e-05  n = 16 6.27e-06  n = 32 3.12e-06  n = 64 1.56e-06  n = 128 7.81e-07  \n</pre> Out[71]: In\u00a0[72]: Copied! <pre>print(\"  n = %d\"%(4*n))\nx_next = sgp.get_x_next(4*n)\nassert x_next.shape==(2*n,1)\ny_next = f_ackley(x_next)\nsgp.add_y_next(y_next)\nsgp.fit()\npmean_std_4n,pstd_std_4n,q,ci_low_std_4n,ci_high_std_4n = sgp.post_ci(xticks[:,None])\nx_std_4n,y_std_4n = sgp.x.clone(),sgp.y.clone()\nnprojs = [4*n,8*n,16*n,32*n,64*n]\nvprojs = [sgp.post_cubature_var(n=nproj) for nproj in nprojs]\nprint(\"    posterior cubature var: \"+((\"n = %d %-10.2e\")*len(nprojs))%tuple(itertools.chain(*zip(nprojs,vprojs))))\nax.plot(nprojs,vprojs,'-o')\nfig\n</pre> print(\"  n = %d\"%(4*n)) x_next = sgp.get_x_next(4*n) assert x_next.shape==(2*n,1) y_next = f_ackley(x_next) sgp.add_y_next(y_next) sgp.fit() pmean_std_4n,pstd_std_4n,q,ci_low_std_4n,ci_high_std_4n = sgp.post_ci(xticks[:,None]) x_std_4n,y_std_4n = sgp.x.clone(),sgp.y.clone() nprojs = [4*n,8*n,16*n,32*n,64*n] vprojs = [sgp.post_cubature_var(n=nproj) for nproj in nprojs] print(\"    posterior cubature var: \"+((\"n = %d %-10.2e\")*len(nprojs))%tuple(itertools.chain(*zip(nprojs,vprojs)))) ax.plot(nprojs,vprojs,'-o') fig <pre>  n = 16\n     iter of 5.0e+03 | best loss  | loss      \n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n            0.00e+00 | -4.46e+01  | -4.46e+01 \n            5.00e+00 | -4.47e+01  | -4.47e+01 \n            1.00e+01 | -4.48e+01  | -4.48e+01 \n            1.50e+01 | -4.48e+01  | -4.48e+01 \n            1.60e+01 | -4.48e+01  | -4.48e+01 \n    posterior cubature var: n = 16 6.27e-06  n = 32 3.12e-06  n = 64 1.56e-06  n = 128 7.81e-07  n = 256 3.91e-07  \n</pre> Out[72]: In\u00a0[73]: Copied! <pre>print(\"  n = %d\"%n)\nfgp = fastgps.FastGPLattice(\n    qp.KernelShiftInvar(d,torchify=True,device=device),\n    qp.Lattice(d,seed=7),\n    )\nx_next = fgp.get_x_next(n)\ny_next = f_ackley(x_next)\nfgp.add_y_next(y_next)\nfgp.fit()\npmean_lattice_n,pstd_lattice_n,q,ci_low_lattice_n,ci_high_lattice_n = fgp.post_ci(xticks[:,None])\nx_lattice_n,y_lattice_n = fgp.x.clone(),fgp.y.clone()\nnprojs = [n,2*n,4*n,8*n,16*n]\nvprojs = [fgp.post_cubature_var(n=nproj) for nproj in nprojs]\nprint(\"    posterior cubature var: \"+((\"n = %d %-10.2e\")*len(nprojs))%tuple(itertools.chain(*zip(nprojs,vprojs))))\nfig,ax = pyplot.subplots(nrows=1,ncols=1,figsize=(6,4))\nax.set_xscale(\"log\",base=10)\nax.set_yscale(\"log\",base=10)\nax.plot(nprojs,vprojs,'-o');\n</pre> print(\"  n = %d\"%n) fgp = fastgps.FastGPLattice(     qp.KernelShiftInvar(d,torchify=True,device=device),     qp.Lattice(d,seed=7),     ) x_next = fgp.get_x_next(n) y_next = f_ackley(x_next) fgp.add_y_next(y_next) fgp.fit() pmean_lattice_n,pstd_lattice_n,q,ci_low_lattice_n,ci_high_lattice_n = fgp.post_ci(xticks[:,None]) x_lattice_n,y_lattice_n = fgp.x.clone(),fgp.y.clone() nprojs = [n,2*n,4*n,8*n,16*n] vprojs = [fgp.post_cubature_var(n=nproj) for nproj in nprojs] print(\"    posterior cubature var: \"+((\"n = %d %-10.2e\")*len(nprojs))%tuple(itertools.chain(*zip(nprojs,vprojs)))) fig,ax = pyplot.subplots(nrows=1,ncols=1,figsize=(6,4)) ax.set_xscale(\"log\",base=10) ax.set_yscale(\"log\",base=10) ax.plot(nprojs,vprojs,'-o'); <pre>  n = 4\n     iter of 5.0e+03 | best loss  | loss      \n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n            0.00e+00 | 5.45e+00   | 5.45e+00  \n            5.00e+00 | 2.88e+00   | 2.88e+00  \n            1.00e+01 | -1.68e+00  | -1.68e+00 \n            1.50e+01 | -1.95e+00  | -1.95e+00 \n            2.00e+01 | -2.12e+00  | -2.12e+00 \n            2.50e+01 | -2.23e+00  | -2.23e+00 \n            3.00e+01 | -2.33e+00  | -2.33e+00 \n            3.50e+01 | -2.49e+00  | -2.49e+00 \n            4.00e+01 | -2.87e+00  | -2.87e+00 \n            4.50e+01 | -3.81e+00  | -3.81e+00 \n            5.00e+01 | -4.56e+00  | -4.49e+00 \n            5.50e+01 | -4.82e+00  | -4.82e+00 \n            6.00e+01 | -5.14e+00  | -5.14e+00 \n            6.50e+01 | -5.35e+00  | -5.35e+00 \n            7.00e+01 | -5.36e+00  | -5.36e+00 \n            7.40e+01 | -5.36e+00  | -5.36e+00 \n    posterior cubature var: n = 4 6.01e-07  n = 8 5.08e-07  n = 16 1.45e-07  n = 32 1.17e-08  n = 64 7.44e-10  \n</pre> In\u00a0[74]: Copied! <pre>print(\"  n = %d\"%(2*n))\nx_next = fgp.get_x_next(2*n)\ny_next = f_ackley(x_next)\nfgp.add_y_next(y_next)\nfgp.fit()\npmean_lattice_2n,pstd_lattice_2n,q,ci_low_lattice_2n,ci_high_lattice_2n = fgp.post_ci(xticks[:,None])\nx_lattice_2n,y_lattice_2n = fgp.x.clone(),fgp.y.clone()\nnprojs = [2*n,4*n,8*n,16*n,32*n]\nvprojs = [fgp.post_cubature_var(n=nproj) for nproj in nprojs]\nprint(\"    posterior cubature var: \"+((\"n = %d %-10.2e\")*len(nprojs))%tuple(itertools.chain(*zip(nprojs,vprojs))))\nax.plot(nprojs,vprojs,'-o');\nfig\n</pre> print(\"  n = %d\"%(2*n)) x_next = fgp.get_x_next(2*n) y_next = f_ackley(x_next) fgp.add_y_next(y_next) fgp.fit() pmean_lattice_2n,pstd_lattice_2n,q,ci_low_lattice_2n,ci_high_lattice_2n = fgp.post_ci(xticks[:,None]) x_lattice_2n,y_lattice_2n = fgp.x.clone(),fgp.y.clone() nprojs = [2*n,4*n,8*n,16*n,32*n] vprojs = [fgp.post_cubature_var(n=nproj) for nproj in nprojs] print(\"    posterior cubature var: \"+((\"n = %d %-10.2e\")*len(nprojs))%tuple(itertools.chain(*zip(nprojs,vprojs)))) ax.plot(nprojs,vprojs,'-o'); fig <pre>  n = 8\n     iter of 5.0e+03 | best loss  | loss      \n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n            0.00e+00 | -1.50e+01  | -1.50e+01 \n            5.00e+00 | -1.51e+01  | -1.51e+01 \n            1.00e+01 | -1.51e+01  | -1.51e+01 \n            1.10e+01 | -1.51e+01  | -1.51e+01 \n    posterior cubature var: n = 8 4.32e-07  n = 16 1.18e-07  n = 32 9.30e-09  n = 64 5.91e-10  n = 128 3.70e-11  \n</pre> Out[74]: In\u00a0[75]: Copied! <pre>print(\"  n = %d\"%(4*n))\nx_next = fgp.get_x_next(4*n)\nassert x_next.shape==(2*n,1)\ny_next = f_ackley(x_next)\nfgp.add_y_next(y_next)\nfgp.fit()\npmean_lattice_4n,pstd_lattice_4n,q,ci_low_lattice_4n,ci_high_lattice_4n = fgp.post_ci(xticks[:,None])\nx_lattice_4n,y_lattice_4n = fgp.x.clone(),fgp.y.clone()\nprint(\"    posterior cubature var: %-10.2e n=%d: %-10.2e n=%d: %-10.2e\"%\\\n    (fgp.post_cubature_var(),8*n,fgp.post_cubature_var(n=8*n),16*n,fgp.post_cubature_var(n=16*n)))\nnprojs = [4*n,8*n,16*n,32*n,64*n]\nvprojs = [fgp.post_cubature_var(n=nproj) for nproj in nprojs]\nprint(\"    posterior cubature var: \"+((\"n = %d %-10.2e\")*len(nprojs))%tuple(itertools.chain(*zip(nprojs,vprojs))))\nax.plot(nprojs,vprojs,'-o')\nfig\n</pre> print(\"  n = %d\"%(4*n)) x_next = fgp.get_x_next(4*n) assert x_next.shape==(2*n,1) y_next = f_ackley(x_next) fgp.add_y_next(y_next) fgp.fit() pmean_lattice_4n,pstd_lattice_4n,q,ci_low_lattice_4n,ci_high_lattice_4n = fgp.post_ci(xticks[:,None]) x_lattice_4n,y_lattice_4n = fgp.x.clone(),fgp.y.clone() print(\"    posterior cubature var: %-10.2e n=%d: %-10.2e n=%d: %-10.2e\"%\\     (fgp.post_cubature_var(),8*n,fgp.post_cubature_var(n=8*n),16*n,fgp.post_cubature_var(n=16*n))) nprojs = [4*n,8*n,16*n,32*n,64*n] vprojs = [fgp.post_cubature_var(n=nproj) for nproj in nprojs] print(\"    posterior cubature var: \"+((\"n = %d %-10.2e\")*len(nprojs))%tuple(itertools.chain(*zip(nprojs,vprojs)))) ax.plot(nprojs,vprojs,'-o') fig <pre>  n = 16\n     iter of 5.0e+03 | best loss  | loss      \n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n            0.00e+00 | -3.63e+01  | -3.63e+01 \n            5.00e+00 | -3.70e+01  | -3.70e+01 \n            1.00e+01 | -3.71e+01  | -3.71e+01 \n            1.20e+01 | -3.71e+01  | -3.71e+01 \n    posterior cubature var: 1.59e-07   n=32: 1.34e-08   n=64: 8.54e-10  \n    posterior cubature var: n = 16 1.59e-07  n = 32 1.34e-08  n = 64 8.54e-10  n = 128 5.34e-11  n = 256 3.34e-12  \n</pre> Out[75]: In\u00a0[76]: Copied! <pre>print(\"  n = %d\"%n)\nfgp = fastgps.FastGPDigitalNetB2(\n    qp.KernelDigShiftInvarCombined(d,torchify=True,device=device),\n    qp.DigitalNetB2(d,seed=7),\n    )\nx_next = fgp.get_x_next(n)\ny_next = f_ackley(x_next)\nfgp.add_y_next(y_next)\nfgp.fit()\npmean_dnb2_n,pstd_dnb2_n,q,ci_low_dnb2_n,ci_high_dnb2_n = fgp.post_ci(xticks[:,None])\nx_dnb2_n,y_dnb2_n = fgp.x.clone(),fgp.y.clone()\nprint(\"    posterior cubature var: %-10.2e n=%d: %-10.2e n=%d: %-10.2e\"%\\\n    (fgp.post_cubature_var(),2*n,fgp.post_cubature_var(n=2*n),4*n,fgp.post_cubature_var(n=4*n)))\nnprojs = [n,2*n,4*n,8*n,16*n]\nvprojs = [fgp.post_cubature_var(n=nproj) for nproj in nprojs]\nprint(\"    posterior cubature var: \"+((\"n = %d %-10.2e\")*len(nprojs))%tuple(itertools.chain(*zip(nprojs,vprojs))))\nfig,ax = pyplot.subplots(nrows=1,ncols=1,figsize=(6,4))\nax.set_xscale(\"log\",base=10)\nax.set_yscale(\"log\",base=10)\nax.plot(nprojs,vprojs,'-o');\n</pre> print(\"  n = %d\"%n) fgp = fastgps.FastGPDigitalNetB2(     qp.KernelDigShiftInvarCombined(d,torchify=True,device=device),     qp.DigitalNetB2(d,seed=7),     ) x_next = fgp.get_x_next(n) y_next = f_ackley(x_next) fgp.add_y_next(y_next) fgp.fit() pmean_dnb2_n,pstd_dnb2_n,q,ci_low_dnb2_n,ci_high_dnb2_n = fgp.post_ci(xticks[:,None]) x_dnb2_n,y_dnb2_n = fgp.x.clone(),fgp.y.clone() print(\"    posterior cubature var: %-10.2e n=%d: %-10.2e n=%d: %-10.2e\"%\\     (fgp.post_cubature_var(),2*n,fgp.post_cubature_var(n=2*n),4*n,fgp.post_cubature_var(n=4*n))) nprojs = [n,2*n,4*n,8*n,16*n] vprojs = [fgp.post_cubature_var(n=nproj) for nproj in nprojs] print(\"    posterior cubature var: \"+((\"n = %d %-10.2e\")*len(nprojs))%tuple(itertools.chain(*zip(nprojs,vprojs)))) fig,ax = pyplot.subplots(nrows=1,ncols=1,figsize=(6,4)) ax.set_xscale(\"log\",base=10) ax.set_yscale(\"log\",base=10) ax.plot(nprojs,vprojs,'-o'); <pre>  n = 4\n     iter of 5.0e+03 | best loss  | loss      \n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n            0.00e+00 | 7.15e+00   | 7.15e+00  \n            5.00e+00 | 3.26e+00   | 3.26e+00  \n            1.00e+01 | -1.97e+00  | -1.87e+00 \n            1.50e+01 | -2.68e+00  | -2.68e+00 \n            2.00e+01 | -3.14e+00  | -3.14e+00 \n            2.50e+01 | -3.43e+00  | -3.43e+00 \n            3.00e+01 | -3.70e+00  | -3.70e+00 \n            3.50e+01 | -3.91e+00  | -3.91e+00 \n            4.00e+01 | -4.03e+00  | -4.03e+00 \n            4.50e+01 | -4.13e+00  | -4.13e+00 \n            5.00e+01 | -4.15e+00  | -4.15e+00 \n            5.50e+01 | -4.18e+00  | -4.18e+00 \n    posterior cubature var: 2.85e-04   n=8: 1.37e-04   n=16: 3.97e-05  \n    posterior cubature var: n = 4 2.85e-04  n = 8 1.37e-04  n = 16 3.97e-05  n = 32 2.59e-05  n = 64 1.39e-06  \n</pre> In\u00a0[77]: Copied! <pre>print(\"  n = %d\"%(2*n))\nx_next = fgp.get_x_next(2*n)\ny_next = f_ackley(x_next)\nfgp.add_y_next(y_next)\nfgp.fit()\npmean_dnb2_2n,pstd_dnb2_2n,q,ci_low_dnb2_2n,ci_high_dnb2_2n = fgp.post_ci(xticks[:,None])\nx_dnb2_2n,y_dnb2_2n = fgp.x.clone(),fgp.y.clone()\nnprojs = [2*n,4*n,8*n,16*n,32*n]\nvprojs = [fgp.post_cubature_var(n=nproj) for nproj in nprojs]\nprint(\"    posterior cubature var: \"+((\"n = %d %-10.2e\")*len(nprojs))%tuple(itertools.chain(*zip(nprojs,vprojs))))\nax.plot(nprojs,vprojs,'-o');\nfig\n</pre> print(\"  n = %d\"%(2*n)) x_next = fgp.get_x_next(2*n) y_next = f_ackley(x_next) fgp.add_y_next(y_next) fgp.fit() pmean_dnb2_2n,pstd_dnb2_2n,q,ci_low_dnb2_2n,ci_high_dnb2_2n = fgp.post_ci(xticks[:,None]) x_dnb2_2n,y_dnb2_2n = fgp.x.clone(),fgp.y.clone() nprojs = [2*n,4*n,8*n,16*n,32*n] vprojs = [fgp.post_cubature_var(n=nproj) for nproj in nprojs] print(\"    posterior cubature var: \"+((\"n = %d %-10.2e\")*len(nprojs))%tuple(itertools.chain(*zip(nprojs,vprojs)))) ax.plot(nprojs,vprojs,'-o'); fig <pre>  n = 8\n     iter of 5.0e+03 | best loss  | loss      \n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n            0.00e+00 | -1.09e+01  | -1.09e+01 \n            5.00e+00 | -1.13e+01  | -1.13e+01 \n            1.00e+01 | -1.14e+01  | -1.14e+01 \n            1.50e+01 | -1.16e+01  | -1.16e+01 \n            2.00e+01 | -1.17e+01  | -1.17e+01 \n            2.50e+01 | -1.20e+01  | -1.20e+01 \n            3.00e+01 | -1.20e+01  | -1.20e+01 \n            3.50e+01 | -1.20e+01  | -1.20e+01 \n    posterior cubature var: n = 8 2.49e-05  n = 16 1.40e-05  n = 32 1.30e-05  n = 64 2.00e-07  n = 128 5.94e-08  \n</pre> Out[77]: In\u00a0[78]: Copied! <pre>print(\"  n = %d\"%(4*n))\nx_next = fgp.get_x_next(4*n)\ny_next = f_ackley(x_next)\nfgp.add_y_next(y_next)\nfgp.fit()\npmean_dnb2_4n,pstd_dnb2_4n,q,ci_low_dnb2_4n,ci_high_dnb2_4n = fgp.post_ci(xticks[:,None])\nx_dnb2_4n,y_dnb2_4n = fgp.x.clone(),fgp.y.clone()\nnprojs = [4*n,8*n,16*n,32*n,64*n]\nvprojs = [fgp.post_cubature_var(n=nproj) for nproj in nprojs]\nprint(\"    posterior cubature var: \"+((\"n = %d %-10.2e\")*len(nprojs))%tuple(itertools.chain(*zip(nprojs,vprojs))))\nax.plot(nprojs,vprojs,'-o')\nfig\n</pre> print(\"  n = %d\"%(4*n)) x_next = fgp.get_x_next(4*n) y_next = f_ackley(x_next) fgp.add_y_next(y_next) fgp.fit() pmean_dnb2_4n,pstd_dnb2_4n,q,ci_low_dnb2_4n,ci_high_dnb2_4n = fgp.post_ci(xticks[:,None]) x_dnb2_4n,y_dnb2_4n = fgp.x.clone(),fgp.y.clone() nprojs = [4*n,8*n,16*n,32*n,64*n] vprojs = [fgp.post_cubature_var(n=nproj) for nproj in nprojs] print(\"    posterior cubature var: \"+((\"n = %d %-10.2e\")*len(nprojs))%tuple(itertools.chain(*zip(nprojs,vprojs)))) ax.plot(nprojs,vprojs,'-o') fig <pre>  n = 16\n     iter of 5.0e+03 | best loss  | loss      \n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n            0.00e+00 | -2.71e+01  | -2.71e+01 \n            5.00e+00 | -2.71e+01  | -2.71e+01 \n            1.00e+01 | -2.71e+01  | -2.71e+01 \n            1.50e+01 | -2.72e+01  | -2.72e+01 \n            1.60e+01 | -2.72e+01  | -2.72e+01 \n    posterior cubature var: n = 16 1.27e-05  n = 32 1.18e-05  n = 64 1.67e-07  n = 128 5.02e-08  n = 256 1.00e-08  \n</pre> Out[78]: In\u00a0[79]: Copied! <pre>data = [\n    [\n        (x_std_n,y_std_n,pmean_std_n,ci_low_std_n,ci_high_std_n),\n        (x_lattice_n,y_lattice_n,pmean_lattice_n,ci_low_lattice_n,ci_high_lattice_n),\n        (x_dnb2_n,y_dnb2_n,pmean_dnb2_n,ci_low_dnb2_n,ci_high_dnb2_n)\n    ],\n    [   \n        (x_std_2n,y_std_2n,pmean_std_2n,ci_low_std_2n,ci_high_std_2n),\n        (x_lattice_2n,y_lattice_2n,pmean_lattice_2n,ci_low_lattice_2n,ci_high_lattice_2n),\n        (x_dnb2_2n,y_dnb2_2n,pmean_dnb2_2n,ci_low_dnb2_2n,ci_high_dnb2_2n)\n    ],\n    [   \n        (x_std_4n,y_std_4n,pmean_std_4n,ci_low_std_4n,ci_high_std_4n),\n        (x_lattice_4n,y_lattice_4n,pmean_lattice_4n,ci_low_lattice_4n,ci_high_lattice_4n),\n        (x_dnb2_4n,y_dnb2_4n,pmean_dnb2_4n,ci_low_dnb2_4n,ci_high_dnb2_4n)\n    ],\n]\n</pre> data = [     [         (x_std_n,y_std_n,pmean_std_n,ci_low_std_n,ci_high_std_n),         (x_lattice_n,y_lattice_n,pmean_lattice_n,ci_low_lattice_n,ci_high_lattice_n),         (x_dnb2_n,y_dnb2_n,pmean_dnb2_n,ci_low_dnb2_n,ci_high_dnb2_n)     ],     [            (x_std_2n,y_std_2n,pmean_std_2n,ci_low_std_2n,ci_high_std_2n),         (x_lattice_2n,y_lattice_2n,pmean_lattice_2n,ci_low_lattice_2n,ci_high_lattice_2n),         (x_dnb2_2n,y_dnb2_2n,pmean_dnb2_2n,ci_low_dnb2_2n,ci_high_dnb2_2n)     ],     [            (x_std_4n,y_std_4n,pmean_std_4n,ci_low_std_4n,ci_high_std_4n),         (x_lattice_4n,y_lattice_4n,pmean_lattice_4n,ci_low_lattice_4n,ci_high_lattice_4n),         (x_dnb2_4n,y_dnb2_4n,pmean_dnb2_4n,ci_low_dnb2_4n,ci_high_dnb2_4n)     ], ] In\u00a0[80]: Copied! <pre>nrows = 3\nncols = len(data[0])\n_alpha = 0.25\npyplot.rcParams.update(tueplots.figsizes.icml2024_full(nrows=nrows,ncols=ncols))\nfig,ax = pyplot.subplots(nrows=nrows,ncols=ncols)\nax = ax.reshape((nrows,ncols))\nfor i in range(3):\n    for j in range(ncols):\n        x,y,pmean,ci_low,ci_high = data[i][j]\n        ax[i,j].plot(xticks.cpu(),yticks.cpu(),color=\"k\")\n        ax[i,j].scatter(x[:,0].cpu(),y.cpu(),color=\"k\")\n        ax[i,j].plot(xticks.cpu(),pmean.cpu(),color=colors[j])\n        ax[i,j].fill_between(xticks.cpu(),ci_low.cpu(),ci_high.cpu(),color=colors[j],alpha=_alpha)\nax[0,0].set_title(\"StandardGP\")\nax[0,1].set_title(\"FastGPDigitalNetB2\")\nax[0,2].set_title(\"FastGPLattice\")\nax[0,0].set_ylabel(r\"n = %d\"%n)\nax[1,0].set_ylabel(r\"n = %d\"%(2*n))\nax[2,0].set_ylabel(r\"n = %d\"%(4*n))\nfig.savefig(\"./gps.pdf\")\n</pre> nrows = 3 ncols = len(data[0]) _alpha = 0.25 pyplot.rcParams.update(tueplots.figsizes.icml2024_full(nrows=nrows,ncols=ncols)) fig,ax = pyplot.subplots(nrows=nrows,ncols=ncols) ax = ax.reshape((nrows,ncols)) for i in range(3):     for j in range(ncols):         x,y,pmean,ci_low,ci_high = data[i][j]         ax[i,j].plot(xticks.cpu(),yticks.cpu(),color=\"k\")         ax[i,j].scatter(x[:,0].cpu(),y.cpu(),color=\"k\")         ax[i,j].plot(xticks.cpu(),pmean.cpu(),color=colors[j])         ax[i,j].fill_between(xticks.cpu(),ci_low.cpu(),ci_high.cpu(),color=colors[j],alpha=_alpha) ax[0,0].set_title(\"StandardGP\") ax[0,1].set_title(\"FastGPDigitalNetB2\") ax[0,2].set_title(\"FastGPLattice\") ax[0,0].set_ylabel(r\"n = %d\"%n) ax[1,0].set_ylabel(r\"n = %d\"%(2*n)) ax[2,0].set_ylabel(r\"n = %d\"%(4*n)) fig.savefig(\"./gps.pdf\") In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/simple/compare_gps_plot/#compare-gps-plot","title":"Compare GPs + Plot\u00b6","text":""},{"location":"examples/simple/compare_gps_plot/#true-function","title":"True Function\u00b6","text":""},{"location":"examples/simple/compare_gps_plot/#parameters","title":"Parameters\u00b6","text":""},{"location":"examples/simple/compare_gps_plot/#standard-gp","title":"Standard GP\u00b6","text":""},{"location":"examples/simple/compare_gps_plot/#lattice","title":"Lattice\u00b6","text":""},{"location":"examples/simple/compare_gps_plot/#digital-net","title":"Digital Net\u00b6","text":""},{"location":"examples/simple/compare_gps_plot/#collect-data-plot","title":"Collect Data + Plot\u00b6","text":""},{"location":"examples/simple/fgp_dnb2/","title":"Fast GP Net","text":"In\u00a0[1]: Copied! <pre>import fastgps\nimport qmcpy as qp\nimport torch\nimport numpy as np\n</pre> import fastgps import qmcpy as qp import torch import numpy as np In\u00a0[2]: Copied! <pre>device = \"cpu\"\nif device!=\"mps\":\n    torch.set_default_dtype(torch.float64)\n</pre> device = \"cpu\" if device!=\"mps\":     torch.set_default_dtype(torch.float64) In\u00a0[3]: Copied! <pre>def f_ackley(x, a=20, b=0.2, c=2*np.pi, scaling=32.768):\n    # https://www.sfu.ca/~ssurjano/ackley.html\n    assert x.ndim==2\n    x = 2*scaling*x-scaling\n    t1 = a*torch.exp(-b*torch.sqrt(torch.mean(x**2,1)))\n    t2 = torch.exp(torch.mean(torch.cos(c*x),1))\n    t3 = a+np.exp(1)\n    y = -t1-t2+t3\n    return y\nd = 1 # dimension\nrng = torch.Generator().manual_seed(17)\nx = torch.rand((2**7,d),generator=rng).to(device) # random testing locations\ny = f_ackley(x) # true values at random testing locations\nz = torch.rand((2**8,d),generator=rng).to(device) # other random locations at which to evaluate covariance\nprint(\"x.shape = %s\"%str(tuple(x.shape)))\nprint(\"y.shape = %s\"%str(tuple(y.shape)))\nprint(\"z.shape = %s\"%str(tuple(z.shape)))\n</pre> def f_ackley(x, a=20, b=0.2, c=2*np.pi, scaling=32.768):     # https://www.sfu.ca/~ssurjano/ackley.html     assert x.ndim==2     x = 2*scaling*x-scaling     t1 = a*torch.exp(-b*torch.sqrt(torch.mean(x**2,1)))     t2 = torch.exp(torch.mean(torch.cos(c*x),1))     t3 = a+np.exp(1)     y = -t1-t2+t3     return y d = 1 # dimension rng = torch.Generator().manual_seed(17) x = torch.rand((2**7,d),generator=rng).to(device) # random testing locations y = f_ackley(x) # true values at random testing locations z = torch.rand((2**8,d),generator=rng).to(device) # other random locations at which to evaluate covariance print(\"x.shape = %s\"%str(tuple(x.shape))) print(\"y.shape = %s\"%str(tuple(y.shape))) print(\"z.shape = %s\"%str(tuple(z.shape))) <pre>x.shape = (128, 1)\ny.shape = (128,)\nz.shape = (256, 1)\n</pre> In\u00a0[4]: Copied! <pre>fgp = fastgps.FastGPDigitalNetB2(qp.KernelDigShiftInvar(d,torchify=True,device=device),seqs=7)\nx_next = fgp.get_x_next(2**10)\ny_next = f_ackley(x_next)\nfgp.add_y_next(y_next)\nprint(\"x_next.shape = %s\"%str(tuple(x_next.shape)))\nprint(\"y_next.shape = %s\"%str(tuple(y_next.shape)))\n</pre> fgp = fastgps.FastGPDigitalNetB2(qp.KernelDigShiftInvar(d,torchify=True,device=device),seqs=7) x_next = fgp.get_x_next(2**10) y_next = f_ackley(x_next) fgp.add_y_next(y_next) print(\"x_next.shape = %s\"%str(tuple(x_next.shape))) print(\"y_next.shape = %s\"%str(tuple(y_next.shape))) <pre>x_next.shape = (1024, 1)\ny_next.shape = (1024,)\n</pre> In\u00a0[5]: Copied! <pre>pmean = fgp.post_mean(x)\nprint(\"pmean.shape = %s\"%str(tuple(pmean.shape)))\nprint(\"l2 relative error = %.2e\"%(torch.linalg.norm(y-pmean)/torch.linalg.norm(y)))\n</pre> pmean = fgp.post_mean(x) print(\"pmean.shape = %s\"%str(tuple(pmean.shape))) print(\"l2 relative error = %.2e\"%(torch.linalg.norm(y-pmean)/torch.linalg.norm(y))) <pre>pmean.shape = (128,)\nl2 relative error = 8.37e-03\n</pre> In\u00a0[6]: Copied! <pre>data = fgp.fit()\nlist(data.keys())\n</pre> data = fgp.fit() list(data.keys()) <pre>     iter of 5.0e+03 | best loss  | loss      \n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n            0.00e+00 | 1.10e+04   | 1.10e+04  \n            5.00e+00 | 2.41e+03   | 2.41e+03  \n            1.00e+01 | 1.07e+03   | 1.08e+03  \n            1.50e+01 | 1.06e+03   | 1.07e+03  \n            2.00e+01 | 1.06e+03   | 1.06e+03  \n            2.50e+01 | 1.06e+03   | 1.06e+03  \n            2.80e+01 | 1.06e+03   | 1.06e+03  \n</pre> Out[6]: <pre>[]</pre> In\u00a0[7]: Copied! <pre>pmean,pvar,q,ci_low,ci_high = fgp.post_ci(x,confidence=0.99)\nprint(\"pmean.shape = %s\"%str(tuple(pmean.shape)))\nprint(\"pvar.shape = %s\"%str(tuple(pvar.shape)))\nprint(\"q = %.2f\"%q)\nprint(\"ci_low.shape = %s\"%str(tuple(ci_low.shape)))\nprint(\"ci_high.shape = %s\"%str(tuple(ci_high.shape)))\nprint(\"l2 relative error = %.2e\"%(torch.linalg.norm(y-pmean)/torch.linalg.norm(y)))\npcov = fgp.post_cov(x,x)\nprint(\"pcov.shape = %s\"%str(tuple(pcov.shape)))\npcov2 = fgp.post_cov(x,z)\nprint(\"pcov2.shape = %s\"%str(tuple(pcov2.shape)))\nprint(\"\\npcov diag matches pvar: %s\"%torch.allclose(pcov.diagonal(),pvar))\nprint(\"non-negative pvar: %s\"%(pvar&gt;=0).all().item())\n</pre> pmean,pvar,q,ci_low,ci_high = fgp.post_ci(x,confidence=0.99) print(\"pmean.shape = %s\"%str(tuple(pmean.shape))) print(\"pvar.shape = %s\"%str(tuple(pvar.shape))) print(\"q = %.2f\"%q) print(\"ci_low.shape = %s\"%str(tuple(ci_low.shape))) print(\"ci_high.shape = %s\"%str(tuple(ci_high.shape))) print(\"l2 relative error = %.2e\"%(torch.linalg.norm(y-pmean)/torch.linalg.norm(y))) pcov = fgp.post_cov(x,x) print(\"pcov.shape = %s\"%str(tuple(pcov.shape))) pcov2 = fgp.post_cov(x,z) print(\"pcov2.shape = %s\"%str(tuple(pcov2.shape))) print(\"\\npcov diag matches pvar: %s\"%torch.allclose(pcov.diagonal(),pvar)) print(\"non-negative pvar: %s\"%(pvar&gt;=0).all().item()) <pre>pmean.shape = (128,)\npvar.shape = (128,)\nq = 2.58\nci_low.shape = (128,)\nci_high.shape = (128,)\nl2 relative error = 8.44e-03\npcov.shape = (128, 128)\npcov2.shape = (128, 256)\n\npcov diag matches pvar: True\nnon-negative pvar: True\n</pre> In\u00a0[8]: Copied! <pre>pcmean,pcvar,q,cci_low,cci_high = fgp.post_cubature_ci(confidence=0.99)\nprint(\"pcmean = %.3e\"%pcmean)\nprint(\"pcvar = %.3e\"%pcvar)\nprint(\"cci_low = %.3e\"%cci_low)\nprint(\"cci_high = %.3e\"%cci_high)\n</pre> pcmean,pcvar,q,cci_low,cci_high = fgp.post_cubature_ci(confidence=0.99) print(\"pcmean = %.3e\"%pcmean) print(\"pcvar = %.3e\"%pcvar) print(\"cci_low = %.3e\"%cci_low) print(\"cci_high = %.3e\"%cci_high) <pre>pcmean = 1.841e+01\npcvar = 2.286e-02\ncci_low = 1.802e+01\ncci_high = 1.880e+01\n</pre> In\u00a0[9]: Copied! <pre>pcov_future = fgp.post_cov(x,z,n=2*fgp.n)\npvar_future = fgp.post_var(x,n=2*fgp.n)\npcvar_future = fgp.post_cubature_var(n=2*fgp.n)\n</pre> pcov_future = fgp.post_cov(x,z,n=2*fgp.n) pvar_future = fgp.post_var(x,n=2*fgp.n) pcvar_future = fgp.post_cubature_var(n=2*fgp.n) In\u00a0[10]: Copied! <pre>x_next = fgp.get_x_next(2*fgp.n)\ny_next = f_ackley(x_next)\nfgp.add_y_next(y_next)\nprint(\"l2 relative error = %.2e\"%(torch.linalg.norm(y-fgp.post_mean(x))/torch.linalg.norm(y)))\nassert torch.allclose(fgp.post_cov(x,z),pcov_future)\nassert torch.allclose(fgp.post_var(x),pvar_future)\nassert torch.allclose(fgp.post_cubature_var(),pcvar_future)\n</pre> x_next = fgp.get_x_next(2*fgp.n) y_next = f_ackley(x_next) fgp.add_y_next(y_next) print(\"l2 relative error = %.2e\"%(torch.linalg.norm(y-fgp.post_mean(x))/torch.linalg.norm(y))) assert torch.allclose(fgp.post_cov(x,z),pcov_future) assert torch.allclose(fgp.post_var(x),pvar_future) assert torch.allclose(fgp.post_cubature_var(),pcvar_future) <pre>l2 relative error = 4.72e-03\n</pre> In\u00a0[11]: Copied! <pre>data = fgp.fit(verbose=False)\nprint(\"l2 relative error = %.2e\"%(torch.linalg.norm(y-fgp.post_mean(x))/torch.linalg.norm(y)))\n</pre> data = fgp.fit(verbose=False) print(\"l2 relative error = %.2e\"%(torch.linalg.norm(y-fgp.post_mean(x))/torch.linalg.norm(y))) <pre>l2 relative error = 4.72e-03\n</pre> In\u00a0[12]: Copied! <pre>pcov_8n = fgp.post_cov(x,z,n=8*fgp.n)\npvar_8n = fgp.post_var(x,n=8*fgp.n)\npcvar_8n = fgp.post_cubature_var(n=8*fgp.n)\nx_next = fgp.get_x_next(8*fgp.n)\ny_next = f_ackley(x_next)\nfgp.add_y_next(y_next)\nassert torch.allclose(fgp.post_cov(x,z),pcov_8n)\nassert torch.allclose(fgp.post_var(x),pvar_8n)\nassert torch.allclose(fgp.post_cubature_var(),pcvar_8n)\n</pre> pcov_8n = fgp.post_cov(x,z,n=8*fgp.n) pvar_8n = fgp.post_var(x,n=8*fgp.n) pcvar_8n = fgp.post_cubature_var(n=8*fgp.n) x_next = fgp.get_x_next(8*fgp.n) y_next = f_ackley(x_next) fgp.add_y_next(y_next) assert torch.allclose(fgp.post_cov(x,z),pcov_8n) assert torch.allclose(fgp.post_var(x),pvar_8n) assert torch.allclose(fgp.post_cubature_var(),pcvar_8n) In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/simple/fgp_dnb2/#fast-net-gp","title":"Fast Net GP\u00b6","text":""},{"location":"examples/simple/fgp_dnb2/#true-function","title":"True Function\u00b6","text":""},{"location":"examples/simple/fgp_dnb2/#construct-fast-gp","title":"Construct Fast GP\u00b6","text":""},{"location":"examples/simple/fgp_dnb2/#project-and-increase-sample-size","title":"Project and Increase Sample Size\u00b6","text":""},{"location":"examples/simple/fgp_lattice/","title":"Fast GP Lattice","text":"In\u00a0[1]: Copied! <pre>import fastgps\nimport qmcpy as qp\nimport torch\nimport numpy as np\n</pre> import fastgps import qmcpy as qp import torch import numpy as np In\u00a0[2]: Copied! <pre>device = \"cpu\"\nif device!=\"mps\":\n    torch.set_default_dtype(torch.float64)\n</pre> device = \"cpu\" if device!=\"mps\":     torch.set_default_dtype(torch.float64) In\u00a0[3]: Copied! <pre>def f_ackley(x, a=20, b=0.2, c=2*np.pi, scaling=32.768):\n    # https://www.sfu.ca/~ssurjano/ackley.html\n    assert x.ndim==2\n    x = 2*scaling*x-scaling\n    t1 = a*torch.exp(-b*torch.sqrt(torch.mean(x**2,1)))\n    t2 = torch.exp(torch.mean(torch.cos(c*x),1))\n    t3 = a+np.exp(1)\n    y = -t1-t2+t3\n    return y\nd = 1 # dimension\nrng = torch.Generator().manual_seed(17)\nx = torch.rand((2**7,d),generator=rng).to(device) # random testing locations\ny = f_ackley(x) # true values at random testing locations\nz = torch.rand((2**8,d),generator=rng).to(device) # other random locations at which to evaluate covariance\nprint(\"x.shape = %s\"%str(tuple(x.shape)))\nprint(\"y.shape = %s\"%str(tuple(y.shape)))\nprint(\"z.shape = %s\"%str(tuple(z.shape)))\n</pre> def f_ackley(x, a=20, b=0.2, c=2*np.pi, scaling=32.768):     # https://www.sfu.ca/~ssurjano/ackley.html     assert x.ndim==2     x = 2*scaling*x-scaling     t1 = a*torch.exp(-b*torch.sqrt(torch.mean(x**2,1)))     t2 = torch.exp(torch.mean(torch.cos(c*x),1))     t3 = a+np.exp(1)     y = -t1-t2+t3     return y d = 1 # dimension rng = torch.Generator().manual_seed(17) x = torch.rand((2**7,d),generator=rng).to(device) # random testing locations y = f_ackley(x) # true values at random testing locations z = torch.rand((2**8,d),generator=rng).to(device) # other random locations at which to evaluate covariance print(\"x.shape = %s\"%str(tuple(x.shape))) print(\"y.shape = %s\"%str(tuple(y.shape))) print(\"z.shape = %s\"%str(tuple(z.shape))) <pre>x.shape = (128, 1)\ny.shape = (128,)\nz.shape = (256, 1)\n</pre> In\u00a0[4]: Copied! <pre>fgp = fastgps.FastGPLattice(qp.KernelShiftInvar(d,torchify=True,device=device),seqs=7)\nx_next = fgp.get_x_next(2**10)\ny_next = f_ackley(x_next)\nfgp.add_y_next(y_next)\nprint(\"x_next.shape = %s\"%str(tuple(x_next.shape)))\nprint(\"y_next.shape = %s\"%str(tuple(y_next.shape)))\n</pre> fgp = fastgps.FastGPLattice(qp.KernelShiftInvar(d,torchify=True,device=device),seqs=7) x_next = fgp.get_x_next(2**10) y_next = f_ackley(x_next) fgp.add_y_next(y_next) print(\"x_next.shape = %s\"%str(tuple(x_next.shape))) print(\"y_next.shape = %s\"%str(tuple(y_next.shape))) <pre>x_next.shape = (1024, 1)\ny_next.shape = (1024,)\n</pre> In\u00a0[5]: Copied! <pre>pmean = fgp.post_mean(x)\nprint(\"pmean.shape = %s\"%str(tuple(pmean.shape)))\nprint(\"l2 relative error = %.2e\"%(torch.linalg.norm(y-pmean)/torch.linalg.norm(y)))\n</pre> pmean = fgp.post_mean(x) print(\"pmean.shape = %s\"%str(tuple(pmean.shape))) print(\"l2 relative error = %.2e\"%(torch.linalg.norm(y-pmean)/torch.linalg.norm(y))) <pre>pmean.shape = (128,)\nl2 relative error = 1.96e-04\n</pre> In\u00a0[6]: Copied! <pre>data = fgp.fit()\nlist(data.keys())\n</pre> data = fgp.fit() list(data.keys()) <pre>     iter of 5.0e+03 | best loss  | loss      \n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n            0.00e+00 | 1.24e+07   | 1.24e+07  \n            5.00e+00 | 2.79e+06   | 2.79e+06  \n            1.00e+01 | 6.53e+04   | 6.53e+04  \n            1.50e+01 | -4.66e+02  | -4.05e+01 \n            2.00e+01 | -5.11e+02  | -5.11e+02 \n            2.50e+01 | -5.11e+02  | -5.10e+02 \n            3.00e+01 | -5.11e+02  | -5.11e+02 \n            3.50e+01 | -5.11e+02  | -5.11e+02 \n            4.00e+01 | -5.11e+02  | -5.11e+02 \n</pre> Out[6]: <pre>[]</pre> In\u00a0[7]: Copied! <pre>pmean,pvar,q,ci_low,ci_high = fgp.post_ci(x,confidence=0.99)\nprint(\"pmean.shape = %s\"%str(tuple(pmean.shape)))\nprint(\"pvar.shape = %s\"%str(tuple(pvar.shape)))\nprint(\"q = %.2f\"%q)\nprint(\"ci_low.shape = %s\"%str(tuple(ci_low.shape)))\nprint(\"ci_high.shape = %s\"%str(tuple(ci_high.shape)))\nprint(\"l2 relative error = %.2e\"%(torch.linalg.norm(y-pmean)/torch.linalg.norm(y)))\npcov = fgp.post_cov(x,x)\nprint(\"pcov.shape = %s\"%str(tuple(pcov.shape)))\npcov2 = fgp.post_cov(x,z)\nprint(\"pcov2.shape = %s\"%str(tuple(pcov2.shape)))\nprint(\"pcov2.shape = %s\"%str(tuple(pcov2.shape)))\nprint(\"\\npcov diag matches pvar: %s\"%torch.allclose(pcov.diagonal(),pvar))\nprint(\"non-negative pvar: %s\"%(pvar&gt;=0).all().item())\n</pre> pmean,pvar,q,ci_low,ci_high = fgp.post_ci(x,confidence=0.99) print(\"pmean.shape = %s\"%str(tuple(pmean.shape))) print(\"pvar.shape = %s\"%str(tuple(pvar.shape))) print(\"q = %.2f\"%q) print(\"ci_low.shape = %s\"%str(tuple(ci_low.shape))) print(\"ci_high.shape = %s\"%str(tuple(ci_high.shape))) print(\"l2 relative error = %.2e\"%(torch.linalg.norm(y-pmean)/torch.linalg.norm(y))) pcov = fgp.post_cov(x,x) print(\"pcov.shape = %s\"%str(tuple(pcov.shape))) pcov2 = fgp.post_cov(x,z) print(\"pcov2.shape = %s\"%str(tuple(pcov2.shape))) print(\"pcov2.shape = %s\"%str(tuple(pcov2.shape))) print(\"\\npcov diag matches pvar: %s\"%torch.allclose(pcov.diagonal(),pvar)) print(\"non-negative pvar: %s\"%(pvar&gt;=0).all().item()) <pre>pmean.shape = (128,)\npvar.shape = (128,)\nq = 2.58\nci_low.shape = (128,)\nci_high.shape = (128,)\nl2 relative error = 1.96e-04\npcov.shape = (128, 128)\npcov2.shape = (128, 256)\npcov2.shape = (128, 256)\n\npcov diag matches pvar: True\nnon-negative pvar: True\n</pre> In\u00a0[8]: Copied! <pre>pcmean,pcvar,q,cci_low,cci_high = fgp.post_cubature_ci(confidence=0.99)\nprint(\"pcmean = %.3e\"%pcmean)\nprint(\"pcvar = %.3e\"%pcvar)\nprint(\"cci_low = %.3e\"%cci_low)\nprint(\"cci_high = %.3e\"%cci_high)\n</pre> pcmean,pcvar,q,cci_low,cci_high = fgp.post_cubature_ci(confidence=0.99) print(\"pcmean = %.3e\"%pcmean) print(\"pcvar = %.3e\"%pcvar) print(\"cci_low = %.3e\"%cci_low) print(\"cci_high = %.3e\"%cci_high) <pre>pcmean = 1.841e+01\npcvar = 4.759e-08\ncci_low = 1.841e+01\ncci_high = 1.841e+01\n</pre> In\u00a0[9]: Copied! <pre>pcov_future = fgp.post_cov(x,z,n=2*fgp.n)\npvar_future = fgp.post_var(x,n=2*fgp.n)\npcvar_future = fgp.post_cubature_var(n=2*fgp.n)\n</pre> pcov_future = fgp.post_cov(x,z,n=2*fgp.n) pvar_future = fgp.post_var(x,n=2*fgp.n) pcvar_future = fgp.post_cubature_var(n=2*fgp.n) In\u00a0[10]: Copied! <pre>x_next = fgp.get_x_next(2*fgp.n)\ny_next = f_ackley(x_next)\nfgp.add_y_next(y_next)\nprint(\"l2 relative error = %.2e\"%(torch.linalg.norm(y-fgp.post_mean(x))/torch.linalg.norm(y)))\nassert torch.allclose(fgp.post_cov(x,z),pcov_future)\nassert torch.allclose(fgp.post_var(x),pvar_future)\nassert torch.allclose(fgp.post_cubature_var(),pcvar_future)\n</pre> x_next = fgp.get_x_next(2*fgp.n) y_next = f_ackley(x_next) fgp.add_y_next(y_next) print(\"l2 relative error = %.2e\"%(torch.linalg.norm(y-fgp.post_mean(x))/torch.linalg.norm(y))) assert torch.allclose(fgp.post_cov(x,z),pcov_future) assert torch.allclose(fgp.post_var(x),pvar_future) assert torch.allclose(fgp.post_cubature_var(),pcvar_future) <pre>l2 relative error = 6.43e-06\n</pre> In\u00a0[11]: Copied! <pre>data = fgp.fit(verbose=False)\nprint(\"l2 relative error = %.2e\"%(torch.linalg.norm(y-fgp.post_mean(x))/torch.linalg.norm(y)))\n</pre> data = fgp.fit(verbose=False) print(\"l2 relative error = %.2e\"%(torch.linalg.norm(y-fgp.post_mean(x))/torch.linalg.norm(y))) <pre>l2 relative error = 6.43e-06\n</pre> In\u00a0[12]: Copied! <pre>pcov_8n = fgp.post_cov(x,z,n=8*fgp.n)\npvar_8n = fgp.post_var(x,n=8*fgp.n)\npcvar_8n = fgp.post_cubature_var(n=8*fgp.n)\nx_next = fgp.get_x_next(8*fgp.n)\ny_next = f_ackley(x_next)\nfgp.add_y_next(y_next)\nassert torch.allclose(fgp.post_cov(x,z),pcov_8n)\nassert torch.allclose(fgp.post_var(x),pvar_8n)\nassert torch.allclose(fgp.post_cubature_var(),pcvar_8n)\n</pre> pcov_8n = fgp.post_cov(x,z,n=8*fgp.n) pvar_8n = fgp.post_var(x,n=8*fgp.n) pcvar_8n = fgp.post_cubature_var(n=8*fgp.n) x_next = fgp.get_x_next(8*fgp.n) y_next = f_ackley(x_next) fgp.add_y_next(y_next) assert torch.allclose(fgp.post_cov(x,z),pcov_8n) assert torch.allclose(fgp.post_var(x),pvar_8n) assert torch.allclose(fgp.post_cubature_var(),pcvar_8n) In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/simple/fgp_lattice/#fast-lattice-gp","title":"Fast Lattice GP\u00b6","text":""},{"location":"examples/simple/fgp_lattice/#true-function","title":"True Function\u00b6","text":""},{"location":"examples/simple/fgp_lattice/#construct-fast-gp","title":"Construct Fast GP\u00b6","text":""},{"location":"examples/simple/fgp_lattice/#project-and-increase-sample-size","title":"Project and Increase Sample Size\u00b6","text":""},{"location":"examples/simple/standard_gp/","title":"Standard GP","text":"In\u00a0[1]: Copied! <pre>import fastgps\nimport qmcpy as qp\nimport torch\nimport numpy as np\n</pre> import fastgps import qmcpy as qp import torch import numpy as np In\u00a0[2]: Copied! <pre>device = \"cpu\"\nif device!=\"mps\":\n    torch.set_default_dtype(torch.float64)\n</pre> device = \"cpu\" if device!=\"mps\":     torch.set_default_dtype(torch.float64) In\u00a0[3]: Copied! <pre>def f_ackley(x, a=20, b=0.2, c=2*np.pi, scaling=32.768):\n    # https://www.sfu.ca/~ssurjano/ackley.html\n    assert x.ndim==2\n    x = 2*scaling*x-scaling\n    t1 = a*torch.exp(-b*torch.sqrt(torch.mean(x**2,1)))\n    t2 = torch.exp(torch.mean(torch.cos(c*x),1))\n    t3 = a+np.exp(1)\n    y = -t1-t2+t3\n    return y\nd = 1 # dimension\nrng = torch.Generator().manual_seed(17)\nx = torch.rand((2**7,d),generator=rng).to(device) # random testing locations\ny = f_ackley(x) # true values at random testing locations\nz = torch.rand((2**8,d),generator=rng).to(device) # other random locations at which to evaluate covariance\nprint(\"x.shape = %s\"%str(tuple(x.shape)))\nprint(\"y.shape = %s\"%str(tuple(y.shape)))\nprint(\"z.shape = %s\"%str(tuple(z.shape)))\n</pre> def f_ackley(x, a=20, b=0.2, c=2*np.pi, scaling=32.768):     # https://www.sfu.ca/~ssurjano/ackley.html     assert x.ndim==2     x = 2*scaling*x-scaling     t1 = a*torch.exp(-b*torch.sqrt(torch.mean(x**2,1)))     t2 = torch.exp(torch.mean(torch.cos(c*x),1))     t3 = a+np.exp(1)     y = -t1-t2+t3     return y d = 1 # dimension rng = torch.Generator().manual_seed(17) x = torch.rand((2**7,d),generator=rng).to(device) # random testing locations y = f_ackley(x) # true values at random testing locations z = torch.rand((2**8,d),generator=rng).to(device) # other random locations at which to evaluate covariance print(\"x.shape = %s\"%str(tuple(x.shape))) print(\"y.shape = %s\"%str(tuple(y.shape))) print(\"z.shape = %s\"%str(tuple(z.shape))) <pre>x.shape = (128, 1)\ny.shape = (128,)\nz.shape = (256, 1)\n</pre> In\u00a0[4]: Copied! <pre>fgp = fastgps.StandardGP(qp.KernelSquaredExponential(d,torchify=True,device=device),seqs=7)\nx_next = fgp.get_x_next(2**6)\ny_next = f_ackley(x_next)\nfgp.add_y_next(y_next)\nprint(\"x_next.shape = %s\"%str(tuple(x_next.shape)))\nprint(\"y_next.shape = %s\"%str(tuple(y_next.shape)))\n</pre> fgp = fastgps.StandardGP(qp.KernelSquaredExponential(d,torchify=True,device=device),seqs=7) x_next = fgp.get_x_next(2**6) y_next = f_ackley(x_next) fgp.add_y_next(y_next) print(\"x_next.shape = %s\"%str(tuple(x_next.shape))) print(\"y_next.shape = %s\"%str(tuple(y_next.shape))) <pre>x_next.shape = (64, 1)\ny_next.shape = (64,)\n</pre> In\u00a0[5]: Copied! <pre>pmean = fgp.post_mean(x)\nprint(\"pmean.shape = %s\"%str(tuple(pmean.shape)))\nprint(\"l2 relative error = %.2e\"%(torch.linalg.norm(y-pmean)/torch.linalg.norm(y)))\n</pre> pmean = fgp.post_mean(x) print(\"pmean.shape = %s\"%str(tuple(pmean.shape))) print(\"l2 relative error = %.2e\"%(torch.linalg.norm(y-pmean)/torch.linalg.norm(y))) <pre>pmean.shape = (128,)\nl2 relative error = 1.67e-01\n</pre> In\u00a0[6]: Copied! <pre>data = fgp.fit()\nlist(data.keys())\n</pre> data = fgp.fit() list(data.keys()) <pre>     iter of 5.0e+03 | best loss  | loss      \n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n            0.00e+00 | 4.07e+06   | 4.07e+06  \n            5.00e+00 | 2.39e+06   | 2.39e+06  \n            1.00e+01 | 1.94e+05   | 1.94e+05  \n            1.50e+01 | 1.70e+02   | 1.89e+02  \n            2.00e+01 | 1.64e+02   | 1.64e+02  \n            2.50e+01 | 1.52e+02   | 1.52e+02  \n            3.00e+01 | 1.50e+02   | 1.50e+02  \n            3.50e+01 | 1.50e+02   | 1.50e+02  \n            4.00e+01 | 1.50e+02   | 1.50e+02  \n            4.50e+01 | 1.50e+02   | 1.50e+02  \n            4.90e+01 | 1.50e+02   | 1.50e+02  \n</pre> Out[6]: <pre>[]</pre> In\u00a0[7]: Copied! <pre>pmean,pvar,q,ci_low,ci_high = fgp.post_ci(x,confidence=0.99)\nprint(\"pmean.shape = %s\"%str(tuple(pmean.shape)))\nprint(\"pvar.shape = %s\"%str(tuple(pvar.shape)))\nprint(\"q = %.2f\"%q)\nprint(\"ci_low.shape = %s\"%str(tuple(ci_low.shape)))\nprint(\"ci_high.shape = %s\"%str(tuple(ci_high.shape)))\nprint(\"l2 relative error = %.2e\"%(torch.linalg.norm(y-pmean)/torch.linalg.norm(y)))\npcov = fgp.post_cov(x,x)\nprint(\"pcov.shape = %s\"%str(tuple(pcov.shape)))\npcov2 = fgp.post_cov(x,z)\nprint(\"pcov2.shape = %s\"%str(tuple(pcov2.shape)))\nprint(\"\\npcov diag matches pvar: %s\"%torch.allclose(pcov.diagonal(),pvar))\nprint(\"non-negative pvar: %s\"%(pvar&gt;=0).all().item())\n</pre> pmean,pvar,q,ci_low,ci_high = fgp.post_ci(x,confidence=0.99) print(\"pmean.shape = %s\"%str(tuple(pmean.shape))) print(\"pvar.shape = %s\"%str(tuple(pvar.shape))) print(\"q = %.2f\"%q) print(\"ci_low.shape = %s\"%str(tuple(ci_low.shape))) print(\"ci_high.shape = %s\"%str(tuple(ci_high.shape))) print(\"l2 relative error = %.2e\"%(torch.linalg.norm(y-pmean)/torch.linalg.norm(y))) pcov = fgp.post_cov(x,x) print(\"pcov.shape = %s\"%str(tuple(pcov.shape))) pcov2 = fgp.post_cov(x,z) print(\"pcov2.shape = %s\"%str(tuple(pcov2.shape))) print(\"\\npcov diag matches pvar: %s\"%torch.allclose(pcov.diagonal(),pvar)) print(\"non-negative pvar: %s\"%(pvar&gt;=0).all().item()) <pre>pmean.shape = (128,)\npvar.shape = (128,)\nq = 2.58\nci_low.shape = (128,)\nci_high.shape = (128,)\nl2 relative error = 7.02e-02\npcov.shape = (128, 128)\npcov2.shape = (128, 256)\n\npcov diag matches pvar: True\nnon-negative pvar: True\n</pre> In\u00a0[8]: Copied! <pre>pcmean,pcvar,q,cci_low,cci_high = fgp.post_cubature_ci(confidence=0.99)\nprint(\"pcmean = %.3e\"%pcmean)\nprint(\"pcvar = %.3e\"%pcvar)\nprint(\"cci_low = %.3e\"%cci_low)\nprint(\"cci_high = %.3e\"%cci_high)\n</pre> pcmean,pcvar,q,cci_low,cci_high = fgp.post_cubature_ci(confidence=0.99) print(\"pcmean = %.3e\"%pcmean) print(\"pcvar = %.3e\"%pcvar) print(\"cci_low = %.3e\"%cci_low) print(\"cci_high = %.3e\"%cci_high) <pre>pcmean = 1.832e+01\npcvar = 1.503e-03\ncci_low = 1.822e+01\ncci_high = 1.842e+01\n</pre> In\u00a0[9]: Copied! <pre>pcov_future = fgp.post_cov(x,z,n=2*fgp.n)\npvar_future = fgp.post_var(x,n=2*fgp.n)\npcvar_future = fgp.post_cubature_var(n=2*fgp.n)\n</pre> pcov_future = fgp.post_cov(x,z,n=2*fgp.n) pvar_future = fgp.post_var(x,n=2*fgp.n) pcvar_future = fgp.post_cubature_var(n=2*fgp.n) In\u00a0[10]: Copied! <pre>x_next = fgp.get_x_next(2*fgp.n)\ny_next = f_ackley(x_next)\nfgp.add_y_next(y_next)\nprint(\"l2 relative error = %.2e\"%(torch.linalg.norm(y-fgp.post_mean(x))/torch.linalg.norm(y)))\nassert torch.allclose(fgp.post_cov(x,z),pcov_future)\nassert torch.allclose(fgp.post_var(x),pvar_future)\nassert torch.allclose(fgp.post_cubature_var(),pcvar_future)\n</pre> x_next = fgp.get_x_next(2*fgp.n) y_next = f_ackley(x_next) fgp.add_y_next(y_next) print(\"l2 relative error = %.2e\"%(torch.linalg.norm(y-fgp.post_mean(x))/torch.linalg.norm(y))) assert torch.allclose(fgp.post_cov(x,z),pcov_future) assert torch.allclose(fgp.post_var(x),pvar_future) assert torch.allclose(fgp.post_cubature_var(),pcvar_future) <pre>l2 relative error = 6.47e-02\n</pre> In\u00a0[11]: Copied! <pre>data = fgp.fit(verbose=False)\nprint(\"l2 relative error = %.2e\"%(torch.linalg.norm(y-fgp.post_mean(x))/torch.linalg.norm(y)))\n</pre> data = fgp.fit(verbose=False) print(\"l2 relative error = %.2e\"%(torch.linalg.norm(y-fgp.post_mean(x))/torch.linalg.norm(y))) <pre>l2 relative error = 5.81e-02\n</pre> In\u00a0[12]: Copied! <pre>pcov_16n = fgp.post_cov(x,z,n=16*fgp.n)\npvar_16n = fgp.post_var(x,n=16*fgp.n)\npcvar_16n = fgp.post_cubature_var(n=16*fgp.n)\nx_next = fgp.get_x_next(16*fgp.n)\ny_next = f_ackley(x_next)\nfgp.add_y_next(y_next)\nassert torch.allclose(fgp.post_cov(x,z),pcov_16n)\nassert torch.allclose(fgp.post_var(x),pvar_16n)\nassert torch.allclose(fgp.post_cubature_var(),pcvar_16n)\n</pre> pcov_16n = fgp.post_cov(x,z,n=16*fgp.n) pvar_16n = fgp.post_var(x,n=16*fgp.n) pcvar_16n = fgp.post_cubature_var(n=16*fgp.n) x_next = fgp.get_x_next(16*fgp.n) y_next = f_ackley(x_next) fgp.add_y_next(y_next) assert torch.allclose(fgp.post_cov(x,z),pcov_16n) assert torch.allclose(fgp.post_var(x),pvar_16n) assert torch.allclose(fgp.post_cubature_var(),pcvar_16n) In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/simple/standard_gp/#standard-gp","title":"Standard GP\u00b6","text":""},{"location":"examples/simple/standard_gp/#true-function","title":"True Function\u00b6","text":""},{"location":"examples/simple/standard_gp/#construct-gp","title":"Construct GP\u00b6","text":""},{"location":"examples/simple/standard_gp/#project-and-increase-sample-size","title":"Project and Increase Sample Size\u00b6","text":""}]}